{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VuO84cFldVgD"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1svhuuf6dbpY"},"outputs":[],"source":["!pip install -q pytorch-lightning wandb sentencepiece\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install -q --upgrade --force-reinstall --no-deps kaggle\n","\n","# install apex for AMP\n","!git clone https://github.com/NVIDIA/apex\n","!cd apex\n","!pip install -v --disable-pip-version-check --no-cache-dir ./apex"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2RLP9JmdejV"},"outputs":[],"source":["!mkdir /root/.kaggle\n","!cp /content/drive/MyDrive/Colab/kaggle/kaggle.json /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5159,"status":"ok","timestamp":1654419366887,"user":{"displayName":"永友遥","userId":"11743586908271963047"},"user_tz":-540},"id":"OSFFpBcvdymq","outputId":"6c72caa9-92ce-4c2a-91ad-0199c11a72a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["import os\n","import gc\n","import sys\n","import json\n","import itertools\n","from tqdm.auto import tqdm\n","import logging\n","import datetime\n","import ast\n","import numpy as np\n","import pandas as pd\n","import math\n","import re\n","from sklearn import model_selection as sms\n","from sklearn.preprocessing import LabelEncoder\n","import scipy as sp\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer, seed_everything\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n","from pytorch_lightning.loggers import WandbLogger\n","\n","from transformers import AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n","\n","import wandb\n","\n","%env TOKENIZERS_PARALLELISM=true"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1654419366888,"user":{"displayName":"永友遥","userId":"11743586908271963047"},"user_tz":-540},"id":"JUf7MOUFem6z"},"outputs":[],"source":["class Config:\n","    # ==============================\n","    # Globals #\n","    # ==============================\n","    competition_name = \"us-patent-phrase-to-phrase-matching\"\n","    group = \"RoBERTa-large\"\n","    exp_id = \"030\"\n","    debug = False\n","    inference_only = False\n","    upload_from_colab = True\n","    colab_dir = \"/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching\"\n","    kaggle_json_path = \"/root/.kaggle/kaggle.json\"\n","    kaggle_dataset_path = None\n","    gpus = 1\n","    seed = 2434\n","    max_epochs = 5\n","    accumulate_grad_batches = 2\n","    precision = 16\n","    num_fold = 5\n","    train_fold = [0,1,2,3,4] # 実行するfold\n","    pretrained = True\n","    mlm_pretrained = False\n","    mlm_model_dir = \"/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/mlm_exp001/model\"\n","    gradient_clip_val = 1\n","    # ==============================\n","    # Dataloader #\n","    # ==============================\n","    train_batch_size = 8\n","    valid_batch_size = 32\n","    test_batch_size = 32\n","    num_workers = 8\n","    # ==============================\n","    # Split #\n","    # ==============================\n","    split_name = \"StratifiedGroupKFold\"\n","    split_params = {\n","        \"n_splits\": num_fold,\n","        \"shuffle\": True,\n","        \"random_state\": seed,\n","    }\n","    # ==============================\n","    # Model #\n","    # ==============================\n","    model_name = \"roberta-large\"\n","    max_length = 175\n","    hidden_size = 1024\n","    use_backbone_dropout = True\n","    dropout = 0.2\n","    initializer_range = 0.02\n","    # ==============================\n","    # Loss #\n","    # ==============================\n","    loss_name = \"BCEWithLogitsLoss\"\n","    loss_params = {\n","        \"reduction\": \"mean\",\n","    }\n","    # ==============================\n","    # Optimizer #\n","    # ==============================\n","    optimizer_name = \"AdamW\"\n","    optimizer_params = {\n","        \"lr\": 2e-5,\n","        \"eps\": 1e-6,\n","        \"betas\": (0.9, 0.999)\n","    }\n","    encoder_lr = 2e-5\n","    decoder_lr = 2e-5\n","    weight_decay = 0.01\n","    # ==============================\n","    # Scheduler #\n","    # ==============================\n","    scheduler_name = \"cosine-warmup\"\n","    scheduler_warmup_ratio = 0.1\n","    scheduler_params = {}\n","    scheduler_interval = \"step\"\n","    scheduler_cycle = \"one-cycle\" # epoch or one-cycle\n","    # ==============================\n","    # Callbacks #\n","    # ==============================\n","    checkpoint_params = {\n","        \"monitor\": \"val/pearson_corr\",\n","        \"save_top_k\": 1,\n","        \"save_weights_only\": True,\n","        \"mode\": \"max\",\n","        \"verbose\": True,\n","    }\n","    early_stopping = False\n","    early_stopping_params = {\n","        \"monitor\": \"val/pearson_corr\",\n","        \"min_delta\": 0.0,\n","        \"patience\": 8,\n","        \"verbose\": False,\n","        \"mode\": \"min\",\n","    }"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4088,"status":"ok","timestamp":1654419464704,"user":{"displayName":"永友遥","userId":"11743586908271963047"},"user_tz":-540},"id":"kq2PHavyzv4C"},"outputs":[],"source":["# ====================================\n","# Setup #\n","# ====================================\n","class Logger:\n","    \"\"\" ref) https://github.com/ghmagazine/kagglebook/blob/master/ch04-model-interface/code/util.py\"\"\"\n","    def __init__(self, path):\n","        self.general_logger = logging.getLogger(path)\n","        stream_handler = logging.StreamHandler()\n","        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n","        if len(self.general_logger.handlers) == 0:\n","            self.general_logger.addHandler(stream_handler)\n","            self.general_logger.addHandler(file_general_handler)\n","            self.general_logger.setLevel(logging.INFO)\n","\n","    def info(self, message):\n","        # display time\n","        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n","\n","    @staticmethod\n","    def now_string():\n","        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n","\n","\n","def setup(cfg):\n","    cfg.on_colab = \"google.colab\" in sys.modules\n","    if cfg.on_colab:\n","        # kaggle api\n","        f = open(Config.kaggle_json_path, 'r')\n","        json_data = json.load(f)\n","        os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","        # set input/output dir\n","        cfg.input_dir = os.path.join(cfg.colab_dir, \"input\")\n","        cfg.train_csv = os.path.join(cfg.input_dir, \"train.csv\")\n","        cfg.test_csv = os.path.join(cfg.input_dir, \"test.csv\")\n","        cfg.cpc_data = os.path.join(cfg.input_dir, \"cpc-data\")\n","        cfg.cpc_codes_csv = os.path.join(cfg.input_dir, \"cpc-codes/cpc_codes.csv\")\n","        cfg.sample_submission = os.path.join(cfg.input_dir, \"sample_submission.csv\")\n","        cfg.output_dir = os.path.join(cfg.colab_dir, \"output\")\n","        cfg.exp_output_dir = os.path.join(cfg.output_dir, f\"exp{cfg.exp_id}\")\n","        cfg.model_dir = os.path.join(cfg.exp_output_dir, \"model\")\n","\n","        for d in [cfg.output_dir, cfg.exp_output_dir, cfg.model_dir]:\n","            os.makedirs(d, exist_ok=True)\n","            \n","        # wandb\n","        wandb.login()\n","    else:\n","        cfg.input_dir = f\"../input/{cfg.competition_name}\"\n","        cfg.train_csv = os.path.join(cfg.input_dir, \"train.csv\")\n","        cfg.test_csv = os.path.join(cfg.input_dir, \"test.csv\")\n","        cfg.cpc_data = \"../input/cpc-data\"\n","        cfg.cpc_codes_csv = \"../input/cpc-codes/cpc_codes.csv\"\n","        cfg.sample_submission = os.path.join(cfg.input_dir, \"sample_submission.csv\")\n","        cfg.submission = \"./\"\n","        cfg.exp_output_dir = f\"exp{cfg.exp_id}\"\n","        cfg.model_dir = os.path.join(cfg.exp_output_dir, \"model\")\n","\n","        if cfg.kaggle_dataset_path is not None:\n","            cfg.model_dir = os.path.join(cfg.kaggle_dataset_path, \"model\")\n","\n","        for d in [cfg.exp_output_dir, cfg.model_dir]:\n","            os.makedirs(d, exist_ok=True)\n","\n","    return cfg\n","\n","\n","# ====================================\n","# Preprocess #\n","# ====================================\n","def get_tokenizer(cfg):\n","    if cfg.kaggle_dataset_path is None:\n","        pretrained_dir = os.path.join(cfg.exp_output_dir, \"pretrain_tokenizer\")\n","    else:\n","        pretrained_dir = os.path.join(cfg.kaggle_dataset_path, \"pretrain_tokenizer\")\n","\n","    if not os.path.isdir(pretrained_dir):\n","        # except for (\"roberta\", \"deberta-v2\", \"deberta-v3\")\n","        if \"roberta\" not in cfg.model_name:\n","            tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n","        # roberta\n","        else:\n","            tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, trim_offsets=False)\n","\n","        tokenizer.save_pretrained(pretrained_dir)\n","\n","    else:\n","        # deberta-v2 or deberta-v3\n","        if (\"deberta-v2\" in cfg.model_name) or (\"deberta-v3\" in cfg.model_name):\n","            tokenizer = DebertaV2TokenizerFast.from_pretrained(pretrained_dir)\n","        # except for (\"roberta\", \"deberta-v2\", \"deberta-v3\")\n","        elif \"roberta\" not in cfg.model_name:\n","            tokenizer = AutoTokenizer.from_pretrained(pretrained_dir)\n","        # roberta\n","        else:\n","            tokenizer = AutoTokenizer.from_pretrained(pretrained_dir, trim_offsets=False)\n","\n","    return tokenizer\n","\n","\n","def get_backbone_config(cfg):\n","    filename = \"model_config\"\n","    filelist = get_filname_listdir(cfg.exp_output_dir if cfg.on_colab else cfg.kaggle_dataset_path)\n","\n","    if not filename in filelist:\n","        model_config = AutoConfig.from_pretrained(cfg.model_name, output_hidden_states=True)\n","        torch.save(model_config, os.path.join(cfg.exp_output_dir if cfg.on_colab else cfg.kaggle_dataset_path, f\"{filename}.pth\"))\n","    else:\n","        cfg_path = os.path.join(cfg.exp_output_dir if cfg.on_colab else cfg.kaggle_dataset_path, f\"{filename}.pth\")\n","        model_config = torch.load(cfg_path)\n","\n","    return model_config\n","\n","\n","def get_cpc_texts(cfg):\n","    contexts = []\n","    pattern = '[A-Z]\\d+'\n","    for file_name in os.listdir(os.path.join(cfg.cpc_data, \"CPCSchemeXML202105\")):\n","        result = re.findall(pattern, file_name)\n","        if result:\n","            contexts.append(result)\n","    contexts = sorted(set(sum(contexts, [])))\n","    results = {}\n","    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n","        with open(os.path.join(cfg.cpc_data, f\"CPCTitleList202202/cpc-section-{cpc}_20220201.txt\")) as f:\n","            s = f.read()\n","        pattern = f'{cpc}\\t\\t.+'\n","        result = re.findall(pattern, s)\n","        cpc_result = result[0].lstrip(pattern)\n","        for context in [c for c in contexts if c[0] == cpc]:\n","            pattern = f'{context}\\t\\t.+'\n","            result = re.findall(pattern, s)\n","            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n","\n","    return results\n","\n","\n","def get_input_data(cfg, input_type: str = \"train\"):\n","    input_df = pd.read_csv(cfg.train_csv if input_type == \"train\" else cfg.test_csv, nrows=2000 if cfg.debug else None)\n","\n","    cpc_texts = get_cpc_texts(cfg)\n","    input_df[\"context_text\"] = input_df[\"context\"].map(cpc_texts)\n","    # input_df[\"context_text\"] = input_df[\"context_text\"].str.lower()\n","\n","    # cpc_codes_df = pd.read_csv(cfg.cpc_codes_csv)\n","    # cpc_codes_df[\"subclass_title\"] = cpc_codes_df[\"subclass_title\"].str.lower()\n","    # cpc_codes_df[\"group_title\"] = cpc_codes_df[\"group_title\"].str.lower()\n","    # input_df = input_df.merge(cpc_codes_df, on=[\"context\"], how=\"left\")\n","\n","    # input_df[\"text\"] = input_df[\"anchor\"] + \"[SEP]\" + input_df[\"target\"] + \"[SEP]\" + input_df[\"context_text\"] + \"[SEP]\" + input_df[\"subclass_title\"]\n","    input_df[\"text\"] = input_df[\"anchor\"] + \"[SEP]\" + input_df[\"target\"] + \"[SEP]\" + input_df[\"context_text\"]\n","\n","    if input_type == \"train\":\n","        input_df[\"score_map\"] = input_df[\"score\"].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n"," \n","    return input_df\n","\n","\n","def get_split(cfg, train_df):\n","    split_name = cfg.split_name\n","    split_params = cfg.split_params\n","    splitter = sms.__getattribute__(split_name)(**split_params)\n","\n","    groups = train_df[\"anchor\"].to_numpy()\n","    train_df[\"fold\"] = -1\n","\n","    for fold_id, (train_idx, valid_idx) in enumerate(splitter.split(\n","        train_df,\n","        train_df[\"score_map\"],\n","        groups\n","        )):\n","        train_df.loc[valid_idx, \"fold\"] = int(fold_id)\n","\n","    return train_df\n","\n","\n","def get_filname_listdir(directory):\n","    listdir = os.listdir(directory)\n","    out_lst = [os.path.splitext(d)[0] for d in listdir]\n","    \n","    return out_lst\n","\n","\n","# ====================================\n","# Dataset #\n","# ====================================\n","def get_inputs(cfg, tokenizer, text: str):\n","    encoding = tokenizer(\n","        text,\n","        max_length=cfg.max_length,\n","        padding=\"max_length\",\n","        return_offsets_mapping=False,\n","        add_special_tokens=True,\n","        truncation=True\n","    )\n","\n","    for k, v in encoding.items():\n","        encoding[k] = torch.tensor(v, dtype=torch.long)\n","\n","    return encoding\n","\n","\n","class PPPMDataset(Dataset):\n","    def __init__(self, cfg, tokenizer, input_df: pd.DataFrame, phase: str = \"train\"):\n","        self.cfg = cfg\n","        self.tokenizer = tokenizer\n","        self.text = input_df[\"text\"].to_numpy()\n","        self.label = input_df[\"score\"].to_numpy() if phase == \"train\" else None\n","        self.phase = phase\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, idx):\n","        if self.phase == \"train\":\n","            inputs = get_inputs(\n","                self.cfg,\n","                self.tokenizer,\n","                self.text[idx]\n","            )\n","\n","            return {\n","                \"input_ids\": inputs[\"input_ids\"],\n","                \"attention_mask\": inputs[\"attention_mask\"],\n","                \"label\": torch.tensor(self.label[idx], dtype=torch.float),\n","            }\n","        \n","        elif self.phase == \"test\":\n","            inputs = get_inputs(\n","                self.cfg,\n","                self.tokenizer,\n","                self.text[idx]\n","            )\n","\n","            return {\n","                \"input_ids\": inputs[\"input_ids\"],\n","                \"attention_mask\": inputs[\"attention_mask\"],\n","            }\n","        else:\n","            raise NotImplementedError\n","\n","\n","class PPPMDataModule(pl.LightningDataModule):\n","    def __init__(self, cfg, tokenizer, train_df: pd.DataFrame = None, valid_df: pd.DataFrame = None, test_df: pd.DataFrame = None):\n","        super(PPPMDataModule, self).__init__()\n","\n","        self.cfg = cfg\n","        self.tokenizer = tokenizer\n","        self.train_df = train_df\n","        self.valid_df = valid_df\n","        self.test_df = test_df\n","\n","    def prepare_data(self):\n","        if self.test_df is None:\n","            self.train_dataset = PPPMDataset(\n","                cfg=self.cfg,\n","                tokenizer=self.tokenizer,\n","                input_df=self.train_df,\n","                phase=\"train\"\n","            )\n","            self.val_dataset = PPPMDataset(\n","                cfg=self.cfg,\n","                tokenizer=self.tokenizer,\n","                input_df=self.valid_df,\n","                phase=\"train\"\n","            )\n","        else:\n","            self.test_dataset = PPPMDataset(\n","                cfg=self.cfg,\n","                tokenizer=self.tokenizer,\n","                input_df=self.test_df,\n","                phase=\"test\"\n","            )\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.cfg.train_batch_size,\n","            num_workers=self.cfg.num_workers,\n","            shuffle=True,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","    \n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.val_dataset,\n","            batch_size=self.cfg.valid_batch_size,\n","            num_workers=self.cfg.num_workers,\n","            shuffle=False,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","\n","    def predict_dataloader(self):\n","        return DataLoader(\n","            self.test_dataset,\n","            batch_size=self.cfg.test_batch_size,\n","            num_workers=self.cfg.num_workers,\n","            shuffle=False,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","\n","\n","# ====================================\n","# Model #\n","# ====================================\n","class PPPMModel(nn.Module):\n","    def __init__(self, cfg):\n","        super(PPPMModel, self).__init__()\n","        \n","        self.cfg = cfg\n","        self.model_config = get_backbone_config(self.cfg)\n","        if self.cfg.mlm_pretrained:\n","            self.backbone = AutoModel.from_pretrained(self.cfg.mlm_model_dir, config=self.model_config)\n","        elif self.cfg.pretrained:\n","            self.backbone = AutoModel.from_pretrained(self.cfg.model_name, config=self.model_config)\n","        else:\n","            self.backbone = AutoModel.from_config(self.model_config)\n","        self.dropout = nn.Dropout(self.cfg.dropout)\n","        self.fc = nn.Linear(self.cfg.hidden_size, 1)\n","        self._init_weights(self.fc)\n","        self.attention = nn.Sequential(\n","            nn.Linear(self.cfg.hidden_size, 512),\n","            nn.Tanh(),\n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","        self._init_weights(self.attention)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.cfg.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.cfg.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n","        x = outputs[0] # (batch_size, seq_len, hidden_size)\n","        \n","        weights = self.attention(x)\n","        x = torch.sum(weights * x, dim=1) # (batch_size, hidden_size)\n","\n","        x = self.dropout(x)\n","        x = self.fc(x) # (batch_size, 1)\n","\n","        return x.squeeze(-1)\n","\n","\n","class PPPMLightningModule(pl.LightningModule):\n","    def __init__(self, cfg):\n","        super(PPPMLightningModule, self).__init__()\n","\n","        self.cfg = cfg\n","        self.model = PPPMModel(self.cfg)\n","        self.criterion = get_criterion(self.cfg)\n","\n","    def setup(self, stage=None):\n","        # calculate training total steps\n","        if stage == \"fit\":\n","            if self.cfg.scheduler_cycle == \"one-cycle\":\n","                self.training_steps = math.ceil(len(self.trainer.datamodule.train_dataloader()) / self.cfg.accumulate_grad_batches) * self.trainer.max_epochs\n","            elif self.cfg.scheduler_cycle == \"epoch\":\n","                self.training_steps = math.ceil(len(self.trainer.datamodule.train_dataloader()) / self.cfg.accumulate_grad_batches)\n","            else:\n","                raise NotImplementedError\n","            self.warmup_steps = int(self.training_steps * self.cfg.scheduler_warmup_ratio)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        return self.model(input_ids, attention_mask)\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids, attention_mask, label = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"label\"]\n","        output = self.forward(input_ids, attention_mask)\n","        loss = self.criterion(output.view(-1, 1), label.view(-1, 1))\n","        self.log(\"train/loss\", loss, on_epoch=True, logger=True, prog_bar=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_ids, attention_mask, label = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"label\"]\n","        output = self.forward(input_ids, attention_mask)\n","        loss = self.criterion(output.view(-1, 1), label.view(-1, 1))\n","        self.log(\"val/loss\", loss, on_epoch=True, logger=True, prog_bar=True)\n","\n","        return {\n","            \"preds\": output.detach(),\n","            \"labels\": label.detach(),\n","            \"loss\": loss,\n","        }\n","\n","    def validation_epoch_end(self, outputs):\n","        preds = torch.cat([output[\"preds\"] for output in outputs]).sigmoid().cpu().numpy()\n","        labels = torch.cat([output[\"labels\"] for output in outputs]).cpu().numpy()\n","        score = get_score(labels, preds)\n","        self.log(\"val/pearson_corr\", score, logger=True, prog_bar=True)\n","\n","    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n","        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n","        output = self.forward(input_ids, attention_mask)\n","        output = output.sigmoid()\n","\n","        return output.squeeze()\n","\n","    def configure_optimizers(self):\n","        optimizer_params = get_optimizer_params(self.model, self.cfg.encoder_lr, self.cfg.decoder_lr, self.cfg.weight_decay)\n","        optimizer = get_optimizer(self.cfg, optimizer_params)\n","\n","        if self.cfg.scheduler_name is None:\n","            return [optimizer]\n","        else:\n","            scheduler = get_scheduler(self.cfg, optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.training_steps)\n","            scheduler = {\"scheduler\": scheduler, \"interval\": self.cfg.scheduler_interval}\n","\n","            return [optimizer], [scheduler]\n","\n","\n","# ====================================\n","# Criterion, Optimizer, Scheduler #\n","# ====================================\n","def get_criterion(cfg):\n","    loss_name = cfg.loss_name\n","    loss_params = cfg.loss_params\n","\n","    if loss_name != \"SmoothFocalLoss\":\n","        return nn.__getattribute__(loss_name)(**loss_params)\n","    else:\n","        return SmoothFocalLoss(**loss_params)\n","\n","\n","def get_optimizer(cfg, parameters):\n","    optimizer_name = cfg.optimizer_name\n","    optimizer_params = cfg.optimizer_params\n","\n","    return optim.__getattribute__(optimizer_name)(parameters, **optimizer_params)\n","\n","\n","def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","    # param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)],\n","            'lr': encoder_lr, 'weight_decay': weight_decay},\n","        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)],\n","            'lr': encoder_lr, 'weight_decay': 0.0},\n","        {'params': [p for n, p in model.named_parameters() if \"backbone\" not in n],\n","            'lr': decoder_lr, 'weight_decay': 0.0}\n","    ]\n","\n","    return optimizer_parameters\n","\n","\n","def get_scheduler(cfg, optimizer, num_warmup_steps=None, num_training_steps=None):\n","    scheduler_name = cfg.scheduler_name\n","    scheduler_params = cfg.scheduler_params\n","\n","    if scheduler_name == \"cosine-warmup\":\n","        return get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","            **scheduler_params\n","        )\n","    elif scheduler_name == \"linear-warmup\":\n","        return get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","            **scheduler_params\n","        )\n","    else:\n","        return optim.lr_scheduler.__getattribute__(scheduler_name)(optimizer, **scheduler_params)\n","\n","\n","class FocalLoss(nn.Module):\n","    \"\"\"\n","    reference: https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/322799\n","    \"\"\"\n","    def __init__(self, reduction='none', alpha=1, gamma=2):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n","        pt = torch.exp(-bce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * bce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","\n","class SmoothFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.focal_loss = FocalLoss(reduction='none', alpha=alpha, gamma=gamma)\n","        self.smoothing = smoothing\n","\n","    @staticmethod\n","    def _smooth(targets:torch.Tensor, smoothing=0.0):\n","        assert 0 \u003c= smoothing \u003c 1\n","        with torch.no_grad():\n","            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n","        return targets\n","\n","    def forward(self, inputs, targets):\n","        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n","        loss = self.focal_loss(inputs, targets)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","\n","class AWP:\n","    def __init__(self, model, optimizer, adv_param=\"weight\", adv_lr=1e-3, adv_eps=1e-3, start_epoch=0, adv_step=1):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.adv_param = adv_param\n","        self.adv_lr = adv_lr\n","        self.adv_eps = adv_eps\n","        self.start_epoch = start_epoch\n","        self.adv_step = adv_step\n","        self.backup = {}\n","        self.backup_eps = {}\n","        self.criterion = nn.MSELoss(reduction=\"mean\")\n","\n","    def attack_backward(self, tokens, attention_mask, label, epoch):\n","        if (self.adv_lr == 0) or (epoch \u003c self.start_epoch):\n","            return None\n","\n","        self._save()\n","        for _ in range(self.adv_step):\n","            self._attack_step()\n","            with torch.cuda.amp.autocast():\n","                out = self.model(tokens, attention_mask=attention_mask).view(-1, 1)\n","                adv_loss = self.criterion(out, label.view(-1, 1))\n","\n","        return adv_loss\n","\n","    def _attack_step(self):\n","        e = 1e-6\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                norm1 = torch.norm(param.grad)\n","                norm2 = torch.norm(param.data.detach())\n","                if norm1 != 0 and not torch.isnan(norm1):\n","                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n","                    param.data.add_(r_at)\n","                    param.data = torch.min(\n","                        torch.max(param.data, self.backup_eps[name][0]), self.backup_eps[name][1]\n","                    )\n","                # param.data.clamp_(*self.backup_eps[name])\n","\n","    def _save(self):\n","        for name, param in self.model.named_parameters():\n","            if param.requires_grad and param.grad is not None and self.adv_param in name:\n","                if name not in self.backup:\n","                    self.backup[name] = param.data.clone()\n","                    grad_eps = self.adv_eps * param.abs().detach()\n","                    self.backup_eps[name] = (\n","                        self.backup[name] - grad_eps,\n","                        self.backup[name] + grad_eps,\n","                    )\n","\n","    def _restore(self):\n","        for name, param in self.model.named_parameters():\n","            if name in self.backup:\n","                param.data = self.backup[name]\n","        self.backup = {}\n","        self.backup_eps = {}\n","\n","\n","# ====================================\n","# Train \u0026 Predict #\n","# ====================================\n","def train_fold(cfg, tokenizer, train_df, valid_df, fold):\n","    # Seed\n","    seed_everything(cfg.seed)\n","\n","    # Wandb\n","    wandb_logger = WandbLogger(\n","        project=cfg.competition_name,\n","        group=cfg.group,\n","        name=f\"exp{cfg.exp_id}-fold-{fold}\",\n","        job_type=f\"exp{cfg.exp_id}\",\n","        reinit=True,\n","        anonymous=\"must\",\n","    )\n","\n","    # Model Checkpoint\n","    checkpoint = ModelCheckpoint(\n","        dirpath=cfg.model_dir,\n","        # filename=f\"exp{cfg.exp_id}-fold-{fold}\" + \"-{epoch}\",\n","        filename=f\"exp{cfg.exp_id}-fold-{fold}\",\n","        **cfg.checkpoint_params,\n","    )\n","\n","    # Learning Rate\n","    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n","    callbacks = [checkpoint, lr_monitor]\n","\n","    # Early Stopping\n","    if cfg.early_stopping:\n","        early_stopping = EarlyStopping(**cfg.early_stopping_params)\n","        callbacks += [early_stopping]\n","    \n","    # DataModule\n","    lightning_datamodule = PPPMDataModule(\n","        cfg=cfg,\n","        tokenizer=tokenizer,\n","        train_df=train_df,\n","        valid_df=valid_df,\n","    )\n","\n","    # Model\n","    lightning_model = PPPMLightningModule(\n","        cfg=cfg,\n","    )\n","\n","    # Trainer\n","    trainer = Trainer(\n","        gpus=cfg.gpus,\n","        max_epochs=cfg.max_epochs,\n","        callbacks=callbacks,\n","        logger=[wandb_logger],\n","        accumulate_grad_batches=cfg.accumulate_grad_batches,\n","        precision=cfg.precision,\n","        # deterministic=True,\n","        benchmark=False,\n","        gradient_clip_val=cfg.gradient_clip_val,\n","        amp_backend=\"apex\",\n","    )\n","\n","    trainer.fit(lightning_model, datamodule=lightning_datamodule)\n","    wandb.finish(quiet=True)\n","\n","    del lightning_datamodule, lightning_model, trainer\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","\n","def train_cv(cfg, tokenizer, input_df: pd.DataFrame):\n","    oof_df = []\n","\n","    for fold_id in range(cfg.num_fold):\n","        if fold_id in cfg.train_fold:\n","            filename = f\"exp{cfg.exp_id}-fold-{fold_id}\"\n","            filelist = get_filname_listdir(cfg.model_dir)\n","\n","            train_df = input_df[input_df[\"fold\"] != fold_id].reset_index(drop=True)\n","            valid_df = input_df[input_df[\"fold\"] == fold_id].reset_index(drop=True)\n","\n","            # training\n","            if not filename in filelist:\n","                train_fold(\n","                    cfg=cfg,\n","                    tokenizer=tokenizer,\n","                    train_df=train_df,\n","                    valid_df=valid_df,\n","                    fold=fold_id,\n","                )\n","\n","            # oof\n","            oof_pred = predict(\n","                cfg=cfg,\n","                tokenizer=tokenizer,\n","                input_df=valid_df,\n","                filename=filename,\n","            )\n","            valid_df[\"oof\"] = oof_pred\n","            oof_df.append(valid_df)\n","\n","            oof_score = get_score(valid_df[\"score\"].to_numpy(), oof_pred)\n","            cfg.logger.info(f\"fold{fold_id}-score: {oof_score}\")\n","\n","    oof_df = pd.concat(oof_df, axis=0).reset_index(drop=True)        \n","    oof_score = get_score(oof_df[\"score\"].to_numpy(), oof_df[\"oof\"].to_numpy())\n","    cfg.logger.info(f\"cv-score: {oof_score}\")\n","\n","    return oof_df\n","\n","\n","def predict_raw_prediction(cfg, tokenizer, input_df: pd.DataFrame, filename: str):\n","    checkpoint_path = os.path.join(cfg.model_dir, filename + \".ckpt\")\n","\n","    lightning_model = PPPMLightningModule(\n","        cfg=cfg,\n","    )\n","\n","    lightning_model = lightning_model.load_from_checkpoint(\n","        checkpoint_path=checkpoint_path,\n","        cfg=cfg,\n","    )\n","\n","    lightning_datamodule = PPPMDataModule(\n","        cfg,\n","        tokenizer=tokenizer,\n","        test_df=input_df\n","    )\n","\n","    trainer = Trainer(\n","        gpus=cfg.gpus,\n","    )\n","\n","    preds = trainer.predict(\n","        lightning_model,\n","        datamodule=lightning_datamodule,\n","        return_predictions=True\n","    )\n","\n","    preds = torch.cat(preds).cpu().numpy() # (samples, 1)\n","\n","    del lightning_datamodule, lightning_model, trainer\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    \n","    return preds\n","\n","\n","def predict(cfg, tokenizer, input_df: pd.DataFrame, filename: str):\n","    file_path = os.path.join(cfg.exp_output_dir, f\"{filename}.npy\")\n","    \n","    if os.path.isfile(file_path):\n","        preds = np.load(file_path)\n","    else:\n","        preds = predict_raw_prediction(\n","            cfg=cfg,\n","            tokenizer=tokenizer,\n","            input_df=input_df,\n","            filename=filename\n","        )\n","        np.save(os.path.join(cfg.exp_output_dir, filename), preds)\n","\n","    return preds\n","\n","\n","def predict_cv(cfg, tokenizer, input_df: pd.DataFrame):\n","    \"\"\"\n","    CVモデルで予測\n","    \"\"\"\n","    output_df = input_df.copy()\n","    fold_preds = []\n","\n","    for fold_id in range(cfg.num_fold):\n","        if fold_id in cfg.train_fold:\n","            filename = f\"exp{cfg.exp_id}-fold-{fold_id}\"\n","            preds = predict_raw_prediction(\n","                cfg=cfg,\n","                tokenizer=tokenizer,\n","                input_df=input_df,\n","                filename=filename\n","            )\n","            fold_preds.append(preds)\n","\n","    fold_preds = np.mean(fold_preds, axis=0)\n","    output_df[\"score\"] = fold_preds\n","    \n","    return output_df\n","\n","\n","# ====================================\n","# Metrics #\n","# ====================================\n","def get_score(y_true, y_pred):\n","    score = sp.stats.pearsonr(y_true, y_pred)[0]\n","\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"CV7a8JxMeU_o"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","Global seed set to 2434\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loggers/wandb.py:348: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n","  \"There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse\"\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Using 16bit apex Automatic Mixed Precision (AMP)\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type              | Params\n","------------------------------------------------\n","0 | model     | PPPMModel         | 355 M \n","1 | criterion | BCEWithLogitsLoss | 0     \n","------------------------------------------------\n","355 M     Trainable params\n","0         Non-trainable params\n","355 M     Total params\n","1,423.544 Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"091e8973159c43ddb6d92d650e51af9a","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n","                not been set for this class (_ResultMetric). The property determines if `update` by\n","                default needs access to the full metric state. If this is not the case, significant speedups can be\n","                achieved and we recommend setting this to `False`.\n","                We provide an checking function\n","                `from torchmetrics.utilities import check_forward_no_full_state`\n","                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n","                default for now) or if `full_state_update=False` can be used safely.\n","                \n","  warnings.warn(*args, **kwargs)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86f572d6ff674d0f8355cddd6c5b78b4","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8bba3841eade454a8ac821f5257f0b18","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 1795: 'val/pearson_corr' reached 0.75785 (best 0.75785), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-0.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc9799c632c747379e52d03de8989fa6","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 3590: 'val/pearson_corr' was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fdd9cb03b32c40b3b408e0a0edd96035","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 5385: 'val/pearson_corr' reached 0.76682 (best 0.76682), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-0.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88d7262805084af6a0f30bcc136d2d5a","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 7180: 'val/pearson_corr' reached 0.77259 (best 0.77259), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-0.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb3b4ec338764a3381607036122c19a0","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 8975: 'val/pearson_corr' reached 0.77383 (best 0.77383), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-0.ckpt' as top 1\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a649cfaff04c4b6eb421c3d9263fcbf8","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003eexp030-fold-0\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/3ig7s3bi\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/3ig7s3bi\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","  category=PossibleUserWarning,\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","Missing logger folder: /content/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55732494250d43258ccb8d2501a280c2","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[2022-06-05 09:48:34] - fold0-score: 0.7738387526175705\n","Global seed set to 2434\n"]},{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u003ca href=\"https://wandb.me/wandb-init\" target=\"_blank\"\u003ethe W\u0026B docs\u003c/a\u003e."],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.17"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20220605_094835-3prv8l35\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/3prv8l35\" target=\"_blank\"\u003eexp030-fold-1\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://wandb.me/run\" target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Using 16bit apex Automatic Mixed Precision (AMP)\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type              | Params\n","------------------------------------------------\n","0 | model     | PPPMModel         | 355 M \n","1 | criterion | BCEWithLogitsLoss | 0     \n","------------------------------------------------\n","355 M     Trainable params\n","0         Non-trainable params\n","355 M     Total params\n","1,423.544 Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14ecdb673be44128b0ac980bfacc23f8","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b493348d809342e4962be7371d40027d","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02d92e0138fb48ca9588dc9c1162483b","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 1838: 'val/pearson_corr' reached 0.76076 (best 0.76076), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-1.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19c568c3fb3c4104a392f28e8bb6e03b","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 3676: 'val/pearson_corr' reached 0.77589 (best 0.77589), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-1.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79336d46722244cca2df3ec5ce8742a1","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 5514: 'val/pearson_corr' reached 0.79092 (best 0.79092), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-1.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9161ff29783d42709133301ec08679bc","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 7352: 'val/pearson_corr' reached 0.79404 (best 0.79404), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-1.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd874bcd45e84264850c7dc48de42427","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 9190: 'val/pearson_corr' was not in top 1\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec334de3cab54d1d833827a0f7598482","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003eexp030-fold-1\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/3prv8l35\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/3prv8l35\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9778fa7bf30f46bab0aea45d86985c0c","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[2022-06-05 10:40:17] - fold1-score: 0.7940408798838282\n","Global seed set to 2434\n"]},{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u003ca href=\"https://wandb.me/wandb-init\" target=\"_blank\"\u003ethe W\u0026B docs\u003c/a\u003e."],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.17"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20220605_104017-123e5ikr\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/123e5ikr\" target=\"_blank\"\u003eexp030-fold-2\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://wandb.me/run\" target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Using 16bit apex Automatic Mixed Precision (AMP)\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type              | Params\n","------------------------------------------------\n","0 | model     | PPPMModel         | 355 M \n","1 | criterion | BCEWithLogitsLoss | 0     \n","------------------------------------------------\n","355 M     Trainable params\n","0         Non-trainable params\n","355 M     Total params\n","1,423.544 Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb4725f8cbc14a7cbb5d0f66b5171baa","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"133189e0fc4c49b6b0167604578c622f","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"538b9b67c651400295da7f9c78f740c3","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 1822: 'val/pearson_corr' reached 0.72873 (best 0.72873), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-2.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8de835fd9b0c4c6d8667687eba2c7842","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 3644: 'val/pearson_corr' reached 0.77929 (best 0.77929), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-2.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"811f8ed1d1e84785b3488474d12a1c41","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 5466: 'val/pearson_corr' reached 0.79511 (best 0.79511), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-2.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3dfc5057621d447883a87e395ec28e4e","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 7288: 'val/pearson_corr' was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51c3ce51a4464421bd3e2e2c895205b8","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 9110: 'val/pearson_corr' was not in top 1\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a4dbfb6660ac4422b1ca590877dfdbb2","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003eexp030-fold-2\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/123e5ikr\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/123e5ikr\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54f371a7706f47d1afaf362744dfbb0a","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[2022-06-05 11:31:49] - fold2-score: 0.7951248819154241\n","Global seed set to 2434\n"]},{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u003ca href=\"https://wandb.me/wandb-init\" target=\"_blank\"\u003ethe W\u0026B docs\u003c/a\u003e."],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.17"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20220605_113149-128zczxd\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/128zczxd\" target=\"_blank\"\u003eexp030-fold-3\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://wandb.me/run\" target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Using 16bit apex Automatic Mixed Precision (AMP)\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type              | Params\n","------------------------------------------------\n","0 | model     | PPPMModel         | 355 M \n","1 | criterion | BCEWithLogitsLoss | 0     \n","------------------------------------------------\n","355 M     Trainable params\n","0         Non-trainable params\n","355 M     Total params\n","1,423.544 Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8af24a761ab84a7bb024f52d9a6d01c4","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34bf742680aa44629ab8d0e3c201481c","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85df41f61c254798b1e720d3cc7f5d4d","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 1822: 'val/pearson_corr' reached 0.75692 (best 0.75692), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-3.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"679931f43a7845f69625f112b03d01b5","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 3644: 'val/pearson_corr' reached 0.76602 (best 0.76602), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-3.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7528374971b147bbb0b1f14b514af54d","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 5466: 'val/pearson_corr' reached 0.77460 (best 0.77460), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-3.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9607899b171440fbba1a63a9ed7abaaf","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 7288: 'val/pearson_corr' reached 0.78413 (best 0.78413), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-3.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd15088e2023470298da7c3704dc931d","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 9110: 'val/pearson_corr' was not in top 1\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31b08ee4efc14aa89afb59eebeb04ede","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003eexp030-fold-3\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/128zczxd\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/128zczxd\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7b2aa904e954437b7edb834b0d66c24","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[2022-06-05 12:23:29] - fold3-score: 0.7841305129886345\n","Global seed set to 2434\n"]},{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u003ca href=\"https://wandb.me/wandb-init\" target=\"_blank\"\u003ethe W\u0026B docs\u003c/a\u003e."],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.17"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20220605_122329-3kb1e7xw\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/3kb1e7xw\" target=\"_blank\"\u003eexp030-fold-4\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://wandb.me/run\" target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Using 16bit apex Automatic Mixed Precision (AMP)\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type              | Params\n","------------------------------------------------\n","0 | model     | PPPMModel         | 355 M \n","1 | criterion | BCEWithLogitsLoss | 0     \n","------------------------------------------------\n","355 M     Trainable params\n","0         Non-trainable params\n","355 M     Total params\n","1,423.544 Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n","\n","Defaults for this optimization level are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n","Processing user overrides (additional kwargs that are not None)...\n","After processing overrides, optimization options are:\n","enabled                : True\n","opt_level              : O2\n","cast_model_type        : torch.float16\n","patch_torch_functions  : False\n","keep_batchnorm_fp32    : True\n","master_weights         : True\n","loss_scale             : dynamic\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2a1c2c783ba43958f2771b65b0d4e30","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b79fef9b8e54262bcccbbc0291a58a2","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0590fe21243478db51bf2507df8130a","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 1844: 'val/pearson_corr' reached 0.76439 (best 0.76439), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-4.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e36a97ae13de465598b107718a159341","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 3688: 'val/pearson_corr' reached 0.76852 (best 0.76852), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-4.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"201f66d5ef0944119b86a0987c2ac1d6","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 5532: 'val/pearson_corr' reached 0.78249 (best 0.78249), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-4.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc90ed30d59e46d6ad3fd348c6a711b4","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 7376: 'val/pearson_corr' reached 0.79099 (best 0.79099), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-4.ckpt' as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n","Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a1907114d88416189777be0175970cc","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 9220: 'val/pearson_corr' reached 0.79425 (best 0.79425), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp030/model/exp030-fold-4.ckpt' as top 1\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93eadb7ff9b44f23829d55fb8229b857","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003eexp030-fold-4\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/3kb1e7xw\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/3kb1e7xw\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65b71b2ceec44207912647716e523a6c","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[2022-06-05 13:15:49] - fold4-score: 0.7942673035221962\n","[2022-06-05 13:15:49] - cv-score: 0.7878017367093308\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6dff2a401c344847b1237feaf947f308","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5cd21502133d4f79821e5cc9f902cdaa","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5658955ec66a49aeb1f6d40ca3c8ec94","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98a7d284e4ba4d0aac3a46fafcd3f8b9","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f609be8bc42647079a5ab2d6527c1fee","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Starting upload for file model.tar\n"]},{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0.00/6.63G [00:00\u003c?, ?B/s]\u001b[A\n","  0%|          | 664k/6.63G [00:00\u003c2:17:16, 864kB/s]\u001b[A\n","  0%|          | 10.3M/6.63G [00:00\u003c07:22, 16.1MB/s]\u001b[A\n","  0%|          | 23.0M/6.63G [00:01\u003c03:14, 36.6MB/s]\u001b[A\n","  1%|          | 35.2M/6.63G [00:01\u003c02:10, 54.1MB/s]\u001b[A\n","  1%|          | 44.8M/6.63G [00:01\u003c02:45, 42.8MB/s]\u001b[A\n","  1%|          | 51.9M/6.63G [00:01\u003c02:33, 46.0MB/s]\u001b[A\n","  1%|          | 58.5M/6.63G [00:01\u003c03:07, 37.6MB/s]\u001b[A\n","  1%|          | 63.7M/6.63G [00:02\u003c03:12, 36.7MB/s]\u001b[A\n","  1%|          | 70.2M/6.63G [00:02\u003c02:47, 42.0MB/s]\u001b[A\n","  1%|          | 75.3M/6.63G [00:02\u003c03:05, 37.9MB/s]\u001b[A\n","  1%|          | 79.6M/6.63G [00:02\u003c03:28, 33.8MB/s]\u001b[A\n","  1%|          | 83.4M/6.63G [00:02\u003c03:27, 33.9MB/s]\u001b[A\n","  1%|▏         | 89.6M/6.63G [00:02\u003c03:01, 38.8MB/s]\u001b[A\n","  1%|▏         | 93.7M/6.63G [00:02\u003c03:11, 36.7MB/s]\u001b[A\n","  1%|▏         | 97.7M/6.63G [00:03\u003c03:54, 29.9MB/s]\u001b[A\n","  2%|▏         | 102M/6.63G [00:03\u003c03:35, 32.6MB/s] \u001b[A\n","  2%|▏         | 108M/6.63G [00:03\u003c03:01, 38.6MB/s]\u001b[A\n","  2%|▏         | 114M/6.63G [00:03\u003c02:43, 42.7MB/s]\u001b[A\n","  2%|▏         | 118M/6.63G [00:03\u003c02:55, 39.9MB/s]\u001b[A\n","  2%|▏         | 122M/6.63G [00:03\u003c02:58, 39.1MB/s]\u001b[A\n","  2%|▏         | 128M/6.63G [00:03\u003c02:42, 43.0MB/s]\u001b[A\n","  2%|▏         | 134M/6.63G [00:03\u003c03:09, 36.8MB/s]\u001b[A\n","  2%|▏         | 138M/6.63G [00:04\u003c03:05, 37.6MB/s]\u001b[A\n","  2%|▏         | 142M/6.63G [00:04\u003c03:03, 37.9MB/s]\u001b[A\n","  2%|▏         | 150M/6.63G [00:04\u003c02:29, 46.5MB/s]\u001b[A\n","  2%|▏         | 154M/6.63G [00:04\u003c02:37, 44.2MB/s]\u001b[A\n","  2%|▏         | 159M/6.63G [00:04\u003c02:39, 43.7MB/s]\u001b[A\n","  2%|▏         | 163M/6.63G [00:04\u003c02:50, 40.7MB/s]\u001b[A\n","  2%|▏         | 167M/6.63G [00:04\u003c03:01, 38.2MB/s]\u001b[A\n","  3%|▎         | 172M/6.63G [00:04\u003c02:47, 41.4MB/s]\u001b[A\n","  3%|▎         | 178M/6.63G [00:05\u003c03:13, 35.8MB/s]\u001b[A\n","  3%|▎         | 182M/6.63G [00:05\u003c03:09, 36.7MB/s]\u001b[A\n","  3%|▎         | 188M/6.63G [00:05\u003c02:46, 41.5MB/s]\u001b[A\n","  3%|▎         | 195M/6.63G [00:05\u003c02:26, 47.0MB/s]\u001b[A\n","  3%|▎         | 199M/6.63G [00:05\u003c02:36, 44.0MB/s]\u001b[A\n","  3%|▎         | 204M/6.63G [00:05\u003c02:44, 42.1MB/s]\u001b[A\n","  3%|▎         | 208M/6.63G [00:05\u003c02:42, 42.5MB/s]\u001b[A\n","  3%|▎         | 213M/6.63G [00:05\u003c02:33, 44.8MB/s]\u001b[A\n","  3%|▎         | 217M/6.63G [00:06\u003c02:44, 41.8MB/s]\u001b[A\n","  3%|▎         | 221M/6.63G [00:06\u003c02:50, 40.4MB/s]\u001b[A\n","  3%|▎         | 228M/6.63G [00:06\u003c02:27, 46.5MB/s]\u001b[A\n","  3%|▎         | 232M/6.63G [00:06\u003c03:14, 35.4MB/s]\u001b[A\n","  4%|▎         | 238M/6.63G [00:06\u003c02:50, 40.2MB/s]\u001b[A\n","  4%|▎         | 245M/6.63G [00:06\u003c02:21, 48.5MB/s]\u001b[A\n","  4%|▎         | 250M/6.63G [00:06\u003c02:32, 45.0MB/s]\u001b[A\n","  4%|▍         | 255M/6.63G [00:06\u003c02:33, 44.5MB/s]\u001b[A\n","  4%|▍         | 259M/6.63G [00:07\u003c02:49, 40.3MB/s]\u001b[A\n","  4%|▍         | 265M/6.63G [00:07\u003c02:35, 43.9MB/s]\u001b[A\n","  4%|▍         | 270M/6.63G [00:07\u003c02:40, 42.6MB/s]\u001b[A\n","  4%|▍         | 274M/6.63G [00:07\u003c02:40, 42.7MB/s]\u001b[A\n","  4%|▍         | 278M/6.63G [00:07\u003c02:43, 41.8MB/s]\u001b[A\n","  4%|▍         | 284M/6.63G [00:07\u003c02:31, 45.2MB/s]\u001b[A\n","  4%|▍         | 288M/6.63G [00:07\u003c02:34, 44.1MB/s]\u001b[A\n","  4%|▍         | 294M/6.63G [00:07\u003c02:23, 47.4MB/s]\u001b[A\n","  4%|▍         | 299M/6.63G [00:08\u003c02:30, 45.4MB/s]\u001b[A\n","  4%|▍         | 305M/6.63G [00:08\u003c02:25, 46.8MB/s]\u001b[A\n","  5%|▍         | 309M/6.63G [00:08\u003c02:31, 45.0MB/s]\u001b[A\n","  5%|▍         | 315M/6.63G [00:08\u003c02:19, 48.7MB/s]\u001b[A\n","  5%|▍         | 319M/6.63G [00:08\u003c02:24, 46.8MB/s]\u001b[A\n","  5%|▍         | 324M/6.63G [00:08\u003c02:28, 45.5MB/s]\u001b[A\n","  5%|▍         | 328M/6.63G [00:08\u003c02:45, 40.8MB/s]\u001b[A\n","  5%|▍         | 334M/6.63G [00:08\u003c02:50, 39.6MB/s]\u001b[A\n","  5%|▍         | 337M/6.63G [00:08\u003c02:55, 38.5MB/s]\u001b[A\n","  5%|▌         | 342M/6.63G [00:09\u003c02:51, 39.4MB/s]\u001b[A\n","  5%|▌         | 346M/6.63G [00:09\u003c02:52, 39.2MB/s]\u001b[A\n","  5%|▌         | 350M/6.63G [00:09\u003c02:56, 38.3MB/s]\u001b[A\n","  5%|▌         | 356M/6.63G [00:09\u003c02:33, 44.0MB/s]\u001b[A\n","  5%|▌         | 362M/6.63G [00:09\u003c02:22, 47.2MB/s]\u001b[A\n","  5%|▌         | 367M/6.63G [00:09\u003c02:30, 44.9MB/s]\u001b[A\n","  5%|▌         | 371M/6.63G [00:09\u003c02:36, 43.0MB/s]\u001b[A\n","  6%|▌         | 375M/6.63G [00:09\u003c02:36, 42.9MB/s]\u001b[A\n","  6%|▌         | 383M/6.63G [00:10\u003c02:17, 49.0MB/s]\u001b[A\n","  6%|▌         | 387M/6.63G [00:10\u003c02:21, 47.5MB/s]\u001b[A\n","  6%|▌         | 392M/6.63G [00:10\u003c02:26, 45.7MB/s]\u001b[A\n","  6%|▌         | 396M/6.63G [00:10\u003c02:31, 44.4MB/s]\u001b[A\n","  6%|▌         | 402M/6.63G [00:10\u003c02:18, 48.3MB/s]\u001b[A\n","  6%|▌         | 407M/6.63G [00:10\u003c02:23, 46.5MB/s]\u001b[A\n","  6%|▌         | 411M/6.63G [00:10\u003c02:30, 44.3MB/s]\u001b[A\n","  6%|▌         | 416M/6.63G [00:10\u003c02:28, 45.1MB/s]\u001b[A\n","  6%|▌         | 422M/6.63G [00:10\u003c02:17, 48.5MB/s]\u001b[A\n","  6%|▋         | 427M/6.63G [00:11\u003c02:24, 46.3MB/s]\u001b[A\n","  6%|▋         | 431M/6.63G [00:11\u003c02:30, 44.3MB/s]\u001b[A\n","  6%|▋         | 437M/6.63G [00:11\u003c02:27, 45.3MB/s]\u001b[A\n","  7%|▋         | 443M/6.63G [00:11\u003c02:17, 48.2MB/s]\u001b[A\n","  7%|▋         | 447M/6.63G [00:11\u003c02:21, 46.9MB/s]\u001b[A\n","  7%|▋         | 452M/6.63G [00:11\u003c02:28, 44.9MB/s]\u001b[A\n","  7%|▋         | 457M/6.63G [00:11\u003c02:28, 44.7MB/s]\u001b[A\n","  7%|▋         | 461M/6.63G [00:11\u003c02:31, 43.8MB/s]\u001b[A\n","  7%|▋         | 465M/6.63G [00:11\u003c02:42, 40.9MB/s]\u001b[A\n","  7%|▋         | 469M/6.63G [00:12\u003c02:48, 39.4MB/s]\u001b[A\n","  7%|▋         | 476M/6.63G [00:12\u003c02:19, 47.5MB/s]\u001b[A\n","  7%|▋         | 480M/6.63G [00:12\u003c03:04, 35.8MB/s]\u001b[A\n","  7%|▋         | 486M/6.63G [00:12\u003c02:47, 39.6MB/s]\u001b[A\n","  7%|▋         | 492M/6.63G [00:12\u003c02:29, 44.2MB/s]\u001b[A\n","  7%|▋         | 496M/6.63G [00:12\u003c02:31, 43.5MB/s]\u001b[A\n","  7%|▋         | 501M/6.63G [00:12\u003c02:36, 42.3MB/s]\u001b[A\n","  7%|▋         | 506M/6.63G [00:12\u003c02:25, 45.2MB/s]\u001b[A\n","  8%|▊         | 512M/6.63G [00:13\u003c02:19, 47.1MB/s]\u001b[A\n","  8%|▊         | 517M/6.63G [00:13\u003c02:24, 45.7MB/s]\u001b[A\n","  8%|▊         | 521M/6.63G [00:13\u003c02:28, 44.1MB/s]\u001b[A\n","  8%|▊         | 527M/6.63G [00:13\u003c02:21, 46.5MB/s]\u001b[A\n","  8%|▊         | 533M/6.63G [00:13\u003c02:13, 49.0MB/s]\u001b[A\n","  8%|▊         | 537M/6.63G [00:13\u003c02:18, 47.4MB/s]\u001b[A\n","  8%|▊         | 542M/6.63G [00:13\u003c02:25, 45.0MB/s]\u001b[A\n","  8%|▊         | 546M/6.63G [00:13\u003c02:26, 44.8MB/s]\u001b[A\n","  8%|▊         | 552M/6.63G [00:13\u003c02:17, 47.4MB/s]\u001b[A\n","  8%|▊         | 557M/6.63G [00:14\u003c02:24, 45.4MB/s]\u001b[A\n","  8%|▊         | 561M/6.63G [00:14\u003c02:28, 44.1MB/s]\u001b[A\n","  8%|▊         | 566M/6.63G [00:14\u003c02:21, 46.1MB/s]\u001b[A\n","  8%|▊         | 572M/6.63G [00:14\u003c02:16, 47.9MB/s]\u001b[A\n","  8%|▊         | 577M/6.63G [00:14\u003c02:23, 45.5MB/s]\u001b[A\n","  9%|▊         | 581M/6.63G [00:14\u003c02:28, 43.9MB/s]\u001b[A\n","  9%|▊         | 586M/6.63G [00:14\u003c02:24, 45.1MB/s]\u001b[A\n","  9%|▊         | 592M/6.63G [00:14\u003c02:16, 47.7MB/s]\u001b[A\n","  9%|▉         | 596M/6.63G [00:15\u003c02:22, 45.5MB/s]\u001b[A\n","  9%|▉         | 601M/6.63G [00:15\u003c02:28, 43.7MB/s]\u001b[A\n","  9%|▉         | 606M/6.63G [00:15\u003c02:21, 45.9MB/s]\u001b[A\n","  9%|▉         | 612M/6.63G [00:15\u003c02:15, 47.9MB/s]\u001b[A\n","  9%|▉         | 616M/6.63G [00:15\u003c02:21, 45.7MB/s]\u001b[A\n","  9%|▉         | 621M/6.63G [00:15\u003c02:27, 43.8MB/s]\u001b[A\n","  9%|▉         | 626M/6.63G [00:15\u003c02:21, 45.6MB/s]\u001b[A\n","  9%|▉         | 632M/6.63G [00:16\u003c04:51, 22.1MB/s]\u001b[A\n","  9%|▉         | 644M/6.63G [00:16\u003c02:52, 37.4MB/s]\u001b[A\n"," 10%|▉         | 650M/6.63G [00:16\u003c03:18, 32.4MB/s]\u001b[A\n"," 10%|▉         | 655M/6.63G [00:16\u003c03:14, 33.1MB/s]\u001b[A\n"," 10%|▉         | 659M/6.63G [00:16\u003c03:05, 34.7MB/s]\u001b[A\n"," 10%|▉         | 664M/6.63G [00:16\u003c02:47, 38.4MB/s]\u001b[A\n"," 10%|▉         | 668M/6.63G [00:17\u003c02:43, 39.3MB/s]\u001b[A\n"," 10%|▉         | 673M/6.63G [00:17\u003c03:27, 30.9MB/s]\u001b[A\n"," 10%|▉         | 676M/6.63G [00:17\u003c03:22, 31.7MB/s]\u001b[A\n"," 10%|█         | 682M/6.63G [00:17\u003c02:54, 36.8MB/s]\u001b[A\n"," 10%|█         | 688M/6.63G [00:17\u003c02:37, 40.7MB/s]\u001b[A\n"," 10%|█         | 692M/6.63G [00:17\u003c02:37, 40.5MB/s]\u001b[A\n"," 10%|█         | 697M/6.63G [00:17\u003c02:37, 40.5MB/s]\u001b[A\n"," 10%|█         | 703M/6.63G [00:17\u003c02:25, 43.9MB/s]\u001b[A\n"," 10%|█         | 707M/6.63G [00:18\u003c02:27, 43.2MB/s]\u001b[A\n"," 10%|█         | 711M/6.63G [00:18\u003c02:30, 42.3MB/s]\u001b[A\n"," 11%|█         | 716M/6.63G [00:18\u003c02:30, 42.2MB/s]\u001b[A\n"," 11%|█         | 720M/6.63G [00:18\u003c02:32, 41.6MB/s]\u001b[A\n"," 11%|█         | 726M/6.63G [00:18\u003c02:21, 44.9MB/s]\u001b[A\n"," 11%|█         | 730M/6.63G [00:18\u003c02:26, 43.3MB/s]\u001b[A\n"," 11%|█         | 734M/6.63G [00:18\u003c02:33, 41.3MB/s]\u001b[A\n"," 11%|█         | 738M/6.63G [00:18\u003c02:36, 40.6MB/s]\u001b[A\n"," 11%|█         | 744M/6.63G [00:19\u003c02:20, 45.0MB/s]\u001b[A\n"," 11%|█         | 750M/6.63G [00:19\u003c02:10, 48.7MB/s]\u001b[A\n"," 11%|█         | 755M/6.63G [00:19\u003c02:16, 46.3MB/s]\u001b[A\n"," 11%|█         | 759M/6.63G [00:19\u003c02:24, 43.9MB/s]\u001b[A\n"," 11%|█▏        | 765M/6.63G [00:19\u003c02:15, 46.6MB/s]\u001b[A\n"," 11%|█▏        | 770M/6.63G [00:19\u003c02:10, 48.5MB/s]\u001b[A\n"," 11%|█▏        | 775M/6.63G [00:19\u003c02:18, 45.4MB/s]\u001b[A\n"," 11%|█▏        | 779M/6.63G [00:19\u003c02:20, 44.8MB/s]\u001b[A\n"," 12%|█▏        | 783M/6.63G [00:19\u003c02:28, 42.4MB/s]\u001b[A\n"," 12%|█▏        | 788M/6.63G [00:20\u003c02:25, 43.1MB/s]\u001b[A\n"," 12%|█▏        | 794M/6.63G [00:20\u003c02:14, 46.7MB/s]\u001b[A\n"," 12%|█▏        | 798M/6.63G [00:20\u003c02:22, 44.1MB/s]\u001b[A\n"," 12%|█▏        | 803M/6.63G [00:20\u003c02:26, 42.9MB/s]\u001b[A\n"," 12%|█▏        | 808M/6.63G [00:20\u003c02:16, 46.1MB/s]\u001b[A\n"," 12%|█▏        | 814M/6.63G [00:20\u003c02:09, 48.3MB/s]\u001b[A\n"," 12%|█▏        | 819M/6.63G [00:20\u003c02:14, 46.4MB/s]\u001b[A\n"," 12%|█▏        | 823M/6.63G [00:20\u003c02:19, 44.9MB/s]\u001b[A\n"," 12%|█▏        | 828M/6.63G [00:20\u003c02:30, 41.6MB/s]\u001b[A\n"," 12%|█▏        | 834M/6.63G [00:21\u003c02:18, 45.2MB/s]\u001b[A\n"," 12%|█▏        | 839M/6.63G [00:21\u003c02:20, 44.3MB/s]\u001b[A\n"," 12%|█▏        | 844M/6.63G [00:21\u003c02:14, 46.3MB/s]\u001b[A\n"," 13%|█▎        | 849M/6.63G [00:21\u003c02:18, 45.1MB/s]\u001b[A\n"," 13%|█▎        | 854M/6.63G [00:21\u003c02:15, 46.1MB/s]\u001b[A\n"," 13%|█▎        | 859M/6.63G [00:21\u003c02:19, 44.4MB/s]\u001b[A\n"," 13%|█▎        | 864M/6.63G [00:21\u003c02:11, 47.2MB/s]\u001b[A\n"," 13%|█▎        | 869M/6.63G [00:22\u003c06:17, 16.5MB/s]\u001b[A\n"," 13%|█▎        | 874M/6.63G [00:22\u003c04:50, 21.4MB/s]\u001b[A\n"," 13%|█▎        | 883M/6.63G [00:22\u003c03:32, 29.1MB/s]\u001b[A\n"," 13%|█▎        | 888M/6.63G [00:22\u003c03:25, 30.2MB/s]\u001b[A\n"," 13%|█▎        | 892M/6.63G [00:23\u003c03:17, 31.3MB/s]\u001b[A\n"," 13%|█▎        | 898M/6.63G [00:23\u003c02:44, 37.6MB/s]\u001b[A\n"," 13%|█▎        | 902M/6.63G [00:23\u003c02:41, 38.3MB/s]\u001b[A\n"," 13%|█▎        | 907M/6.63G [00:23\u003c02:41, 38.1MB/s]\u001b[A\n"," 13%|█▎        | 911M/6.63G [00:23\u003c02:41, 38.1MB/s]\u001b[A\n"," 13%|█▎        | 916M/6.63G [00:23\u003c02:23, 42.8MB/s]\u001b[A\n"," 14%|█▎        | 922M/6.63G [00:23\u003c02:15, 45.4MB/s]\u001b[A\n"," 14%|█▎        | 927M/6.63G [00:23\u003c02:19, 44.1MB/s]\u001b[A\n"," 14%|█▎        | 931M/6.63G [00:23\u003c02:23, 42.8MB/s]\u001b[A\n"," 14%|█▍        | 937M/6.63G [00:24\u003c02:14, 45.7MB/s]\u001b[A\n"," 14%|█▍        | 942M/6.63G [00:24\u003c02:19, 44.0MB/s]\u001b[A\n"," 14%|█▍        | 947M/6.63G [00:24\u003c02:22, 42.9MB/s]\u001b[A\n"," 14%|█▍        | 952M/6.63G [00:24\u003c02:14, 45.6MB/s]\u001b[A\n"," 14%|█▍        | 958M/6.63G [00:24\u003c02:08, 47.6MB/s]\u001b[A\n"," 14%|█▍        | 963M/6.63G [00:24\u003c02:12, 46.2MB/s]\u001b[A\n"," 14%|█▍        | 967M/6.63G [00:24\u003c02:28, 41.0MB/s]\u001b[A\n"," 14%|█▍        | 971M/6.63G [00:24\u003c02:28, 41.2MB/s]\u001b[A\n"," 14%|█▍        | 975M/6.63G [00:25\u003c02:40, 37.9MB/s]\u001b[A\n"," 14%|█▍        | 979M/6.63G [00:25\u003c02:40, 38.0MB/s]\u001b[A\n"," 14%|█▍        | 984M/6.63G [00:25\u003c02:25, 41.9MB/s]\u001b[A\n"," 15%|█▍        | 988M/6.63G [00:25\u003c02:31, 40.2MB/s]\u001b[A\n"," 15%|█▍        | 992M/6.63G [00:25\u003c02:37, 38.6MB/s]\u001b[A\n"," 15%|█▍        | 996M/6.63G [00:25\u003c02:41, 37.5MB/s]\u001b[A\n"," 15%|█▍        | 0.98G/6.63G [00:25\u003c02:15, 44.7MB/s]\u001b[A\n"," 15%|█▍        | 0.98G/6.63G [00:25\u003c02:35, 39.1MB/s]\u001b[A\n"," 15%|█▍        | 0.99G/6.63G [00:25\u003c02:34, 39.1MB/s]\u001b[A\n"," 15%|█▍        | 0.99G/6.63G [00:26\u003c02:27, 41.1MB/s]\u001b[A\n"," 15%|█▌        | 1.00G/6.63G [00:26\u003c02:11, 45.9MB/s]\u001b[A\n"," 15%|█▌        | 1.00G/6.63G [00:26\u003c02:18, 43.6MB/s]\u001b[A\n"," 15%|█▌        | 1.01G/6.63G [00:26\u003c02:23, 42.0MB/s]\u001b[A\n"," 15%|█▌        | 1.01G/6.63G [00:26\u003c01:58, 50.9MB/s]\u001b[A\n"," 15%|█▌        | 1.02G/6.63G [00:26\u003c02:03, 48.7MB/s]\u001b[A\n"," 15%|█▌        | 1.02G/6.63G [00:26\u003c02:07, 47.2MB/s]\u001b[A\n"," 15%|█▌        | 1.03G/6.63G [00:26\u003c02:13, 45.1MB/s]\u001b[A\n"," 16%|█▌        | 1.03G/6.63G [00:27\u003c02:17, 43.6MB/s]\u001b[A\n"," 16%|█▌        | 1.04G/6.63G [00:27\u003c02:06, 47.5MB/s]\u001b[A\n"," 16%|█▌        | 1.04G/6.63G [00:27\u003c02:10, 45.9MB/s]\u001b[A\n"," 16%|█▌        | 1.05G/6.63G [00:27\u003c02:15, 44.4MB/s]\u001b[A\n"," 16%|█▌        | 1.05G/6.63G [00:27\u003c02:14, 44.4MB/s]\u001b[A\n"," 16%|█▌        | 1.06G/6.63G [00:27\u003c02:16, 43.7MB/s]\u001b[A\n"," 16%|█▌        | 1.06G/6.63G [00:27\u003c02:19, 42.9MB/s]\u001b[A\n"," 16%|█▌        | 1.07G/6.63G [00:27\u003c02:09, 46.3MB/s]\u001b[A\n"," 16%|█▌        | 1.07G/6.63G [00:27\u003c02:02, 48.5MB/s]\u001b[A\n"," 16%|█▌        | 1.08G/6.63G [00:28\u003c02:08, 46.5MB/s]\u001b[A\n"," 16%|█▋        | 1.08G/6.63G [00:28\u003c02:13, 44.6MB/s]\u001b[A\n"," 16%|█▋        | 1.09G/6.63G [00:28\u003c02:10, 45.6MB/s]\u001b[A\n"," 16%|█▋        | 1.09G/6.63G [00:28\u003c02:02, 48.4MB/s]\u001b[A\n"," 17%|█▋        | 1.10G/6.63G [00:28\u003c02:08, 46.1MB/s]\u001b[A\n"," 17%|█▋        | 1.10G/6.63G [00:28\u003c02:12, 44.8MB/s]\u001b[A\n"," 17%|█▋        | 1.11G/6.63G [00:28\u003c02:10, 45.6MB/s]\u001b[A\n"," 17%|█▋        | 1.11G/6.63G [00:28\u003c02:01, 48.8MB/s]\u001b[A\n"," 17%|█▋        | 1.12G/6.63G [00:28\u003c02:07, 46.5MB/s]\u001b[A\n"," 17%|█▋        | 1.12G/6.63G [00:29\u003c02:12, 44.7MB/s]\u001b[A\n"," 17%|█▋        | 1.13G/6.63G [00:29\u003c02:05, 47.1MB/s]\u001b[A\n"," 17%|█▋        | 1.13G/6.63G [00:29\u003c02:05, 47.0MB/s]\u001b[A\n"," 17%|█▋        | 1.14G/6.63G [00:29\u003c02:12, 44.6MB/s]\u001b[A\n"," 17%|█▋        | 1.14G/6.63G [00:29\u003c02:17, 43.0MB/s]\u001b[A\n"," 17%|█▋        | 1.14G/6.63G [00:29\u003c02:07, 46.2MB/s]\u001b[A\n"," 17%|█▋        | 1.15G/6.63G [00:29\u003c02:11, 44.7MB/s]\u001b[A\n"," 17%|█▋        | 1.16G/6.63G [00:29\u003c02:11, 44.7MB/s]\u001b[A\n"," 17%|█▋        | 1.16G/6.63G [00:30\u003c02:14, 43.7MB/s]\u001b[A\n"," 18%|█▊        | 1.17G/6.63G [00:30\u003c01:58, 49.6MB/s]\u001b[A\n"," 18%|█▊        | 1.17G/6.63G [00:30\u003c02:05, 46.8MB/s]\u001b[A\n"," 18%|█▊        | 1.18G/6.63G [00:30\u003c02:12, 44.3MB/s]\u001b[A\n"," 18%|█▊        | 1.18G/6.63G [00:30\u003c02:17, 42.7MB/s]\u001b[A\n"," 18%|█▊        | 1.19G/6.63G [00:30\u003c02:02, 47.8MB/s]\u001b[A\n"," 18%|█▊        | 1.19G/6.63G [00:30\u003c02:06, 46.2MB/s]\u001b[A\n"," 18%|█▊        | 1.19G/6.63G [00:31\u003c04:44, 20.5MB/s]\u001b[A\n"," 18%|█▊        | 1.21G/6.63G [00:31\u003c02:42, 35.9MB/s]\u001b[A\n"," 18%|█▊        | 1.21G/6.63G [00:31\u003c02:43, 35.6MB/s]\u001b[A\n"," 18%|█▊        | 1.22G/6.63G [00:31\u003c02:43, 35.5MB/s]\u001b[A\n"," 18%|█▊        | 1.22G/6.63G [00:31\u003c02:28, 39.0MB/s]\u001b[A\n"," 19%|█▊        | 1.23G/6.63G [00:32\u003c02:37, 36.8MB/s]\u001b[A\n"," 19%|█▊        | 1.23G/6.63G [00:32\u003c02:35, 37.2MB/s]\u001b[A\n"," 19%|█▊        | 1.24G/6.63G [00:32\u003c02:21, 41.0MB/s]\u001b[A\n"," 19%|█▉        | 1.24G/6.63G [00:32\u003c02:05, 46.1MB/s]\u001b[A\n"," 19%|█▉        | 1.25G/6.63G [00:32\u003c02:07, 45.3MB/s]\u001b[A\n"," 19%|█▉        | 1.25G/6.63G [00:32\u003c02:09, 44.5MB/s]\u001b[A\n"," 19%|█▉        | 1.26G/6.63G [00:32\u003c02:10, 44.1MB/s]\u001b[A\n"," 19%|█▉        | 1.26G/6.63G [00:32\u003c02:01, 47.5MB/s]\u001b[A\n"," 19%|█▉        | 1.27G/6.63G [00:32\u003c02:18, 41.5MB/s]\u001b[A\n"," 19%|█▉        | 1.27G/6.63G [00:33\u003c02:19, 41.1MB/s]\u001b[A\n"," 19%|█▉        | 1.28G/6.63G [00:33\u003c01:59, 48.0MB/s]\u001b[A\n"," 19%|█▉        | 1.28G/6.63G [00:33\u003c02:03, 46.4MB/s]\u001b[A\n"," 19%|█▉        | 1.29G/6.63G [00:33\u003c02:13, 43.0MB/s]\u001b[A\n"," 19%|█▉        | 1.29G/6.63G [00:33\u003c02:13, 43.0MB/s]\u001b[A\n"," 20%|█▉        | 1.30G/6.63G [00:33\u003c02:08, 44.4MB/s]\u001b[A\n"," 20%|█▉        | 1.30G/6.63G [00:33\u003c02:09, 44.0MB/s]\u001b[A\n"," 20%|█▉        | 1.31G/6.63G [00:33\u003c02:20, 40.7MB/s]\u001b[A\n"," 20%|█▉        | 1.31G/6.63G [00:34\u003c02:12, 43.2MB/s]\u001b[A\n"," 20%|█▉        | 1.32G/6.63G [00:34\u003c02:01, 46.9MB/s]\u001b[A\n"," 20%|█▉        | 1.32G/6.63G [00:34\u003c02:05, 45.4MB/s]\u001b[A\n"," 20%|█▉        | 1.33G/6.63G [00:34\u003c02:11, 43.4MB/s]\u001b[A\n"," 20%|██        | 1.33G/6.63G [00:34\u003c02:04, 45.6MB/s]\u001b[A\n"," 20%|██        | 1.34G/6.63G [00:34\u003c01:59, 47.8MB/s]\u001b[A\n"," 20%|██        | 1.34G/6.63G [00:34\u003c02:04, 45.6MB/s]\u001b[A\n"," 20%|██        | 1.35G/6.63G [00:34\u003c02:08, 44.2MB/s]\u001b[A\n"," 20%|██        | 1.35G/6.63G [00:34\u003c02:06, 44.7MB/s]\u001b[A\n"," 20%|██        | 1.35G/6.63G [00:35\u003c02:00, 47.0MB/s]\u001b[A\n"," 21%|██        | 1.36G/6.63G [00:35\u003c02:07, 44.5MB/s]\u001b[A\n"," 21%|██        | 1.36G/6.63G [00:35\u003c02:11, 43.1MB/s]\u001b[A\n"," 21%|██        | 1.37G/6.63G [00:35\u003c01:59, 47.2MB/s]\u001b[A\n"," 21%|██        | 1.38G/6.63G [00:35\u003c01:54, 49.1MB/s]\u001b[A\n"," 21%|██        | 1.38G/6.63G [00:35\u003c02:06, 44.4MB/s]\u001b[A\n"," 21%|██        | 1.38G/6.63G [00:35\u003c02:41, 34.8MB/s]\u001b[A\n"," 21%|██        | 1.39G/6.63G [00:35\u003c02:16, 41.2MB/s]\u001b[A\n"," 21%|██        | 1.40G/6.63G [00:36\u003c01:59, 46.8MB/s]\u001b[A\n"," 21%|██        | 1.40G/6.63G [00:36\u003c02:06, 44.4MB/s]\u001b[A\n"," 21%|██        | 1.41G/6.63G [00:36\u003c02:10, 43.1MB/s]\u001b[A\n"," 21%|██▏       | 1.41G/6.63G [00:36\u003c02:12, 42.4MB/s]\u001b[A\n"," 21%|██▏       | 1.42G/6.63G [00:36\u003c02:32, 36.8MB/s]\u001b[A\n"," 21%|██▏       | 1.42G/6.63G [00:36\u003c02:29, 37.4MB/s]\u001b[A\n"," 22%|██▏       | 1.43G/6.63G [00:36\u003c02:00, 46.3MB/s]\u001b[A\n"," 22%|██▏       | 1.43G/6.63G [00:36\u003c02:02, 45.6MB/s]\u001b[A\n"," 22%|██▏       | 1.44G/6.63G [00:37\u003c02:51, 32.6MB/s]\u001b[A\n"," 22%|██▏       | 1.44G/6.63G [00:37\u003c02:08, 43.3MB/s]\u001b[A\n"," 22%|██▏       | 1.45G/6.63G [00:37\u003c02:07, 43.6MB/s]\u001b[A\n"," 22%|██▏       | 1.45G/6.63G [00:37\u003c02:51, 32.5MB/s]\u001b[A\n"," 22%|██▏       | 1.46G/6.63G [00:37\u003c02:48, 33.0MB/s]\u001b[A\n"," 22%|██▏       | 1.46G/6.63G [00:37\u003c02:16, 40.8MB/s]\u001b[A\n"," 22%|██▏       | 1.47G/6.63G [00:38\u003c02:16, 40.6MB/s]\u001b[A\n"," 22%|██▏       | 1.47G/6.63G [00:38\u003c02:20, 39.5MB/s]\u001b[A\n"," 22%|██▏       | 1.48G/6.63G [00:38\u003c02:27, 37.4MB/s]\u001b[A\n"," 22%|██▏       | 1.48G/6.63G [00:38\u003c02:30, 36.7MB/s]\u001b[A\n"," 22%|██▏       | 1.49G/6.63G [00:38\u003c02:02, 45.0MB/s]\u001b[A\n"," 23%|██▎       | 1.49G/6.63G [00:38\u003c02:38, 34.8MB/s]\u001b[A\n"," 23%|██▎       | 1.50G/6.63G [00:38\u003c02:38, 34.8MB/s]\u001b[A\n"," 23%|██▎       | 1.50G/6.63G [00:38\u003c02:15, 40.7MB/s]\u001b[A\n"," 23%|██▎       | 1.51G/6.63G [00:39\u003c02:02, 44.9MB/s]\u001b[A\n"," 23%|██▎       | 1.51G/6.63G [00:39\u003c02:08, 42.8MB/s]\u001b[A\n"," 23%|██▎       | 1.52G/6.63G [00:39\u003c02:10, 42.0MB/s]\u001b[A\n"," 23%|██▎       | 1.52G/6.63G [00:39\u003c02:07, 43.0MB/s]\u001b[A\n"," 23%|██▎       | 1.53G/6.63G [00:39\u003c02:07, 43.0MB/s]\u001b[A\n"," 23%|██▎       | 1.53G/6.63G [00:39\u003c02:11, 41.6MB/s]\u001b[A\n"," 23%|██▎       | 1.53G/6.63G [00:40\u003c05:41, 16.0MB/s]\u001b[A\n"," 23%|██▎       | 1.54G/6.63G [00:40\u003c04:09, 21.9MB/s]\u001b[A\n"," 23%|██▎       | 1.55G/6.63G [00:40\u003c03:08, 28.9MB/s]\u001b[A\n"," 23%|██▎       | 1.55G/6.63G [00:40\u003c03:01, 30.0MB/s]\u001b[A\n"," 23%|██▎       | 1.56G/6.63G [00:40\u003c02:57, 30.7MB/s]\u001b[A\n"," 24%|██▎       | 1.56G/6.63G [00:41\u003c02:20, 38.7MB/s]\u001b[A\n"," 24%|██▎       | 1.57G/6.63G [00:41\u003c02:18, 39.2MB/s]\u001b[A\n"," 24%|██▎       | 1.57G/6.63G [00:41\u003c02:20, 38.8MB/s]\u001b[A\n"," 24%|██▍       | 1.58G/6.63G [00:41\u003c02:17, 39.3MB/s]\u001b[A\n"," 24%|██▍       | 1.58G/6.63G [00:41\u003c02:16, 39.8MB/s]\u001b[A\n"," 24%|██▍       | 1.59G/6.63G [00:41\u003c01:59, 45.4MB/s]\u001b[A\n"," 24%|██▍       | 1.59G/6.63G [00:41\u003c02:02, 44.0MB/s]\u001b[A\n"," 24%|██▍       | 1.60G/6.63G [00:41\u003c02:07, 42.6MB/s]\u001b[A\n"," 24%|██▍       | 1.60G/6.63G [00:41\u003c02:06, 42.7MB/s]\u001b[A\n"," 24%|██▍       | 1.60G/6.63G [00:42\u003c01:55, 46.8MB/s]\u001b[A\n"," 24%|██▍       | 1.61G/6.63G [00:42\u003c01:58, 45.5MB/s]\u001b[A\n"," 24%|██▍       | 1.61G/6.63G [00:42\u003c02:04, 43.3MB/s]\u001b[A\n"," 24%|██▍       | 1.62G/6.63G [00:42\u003c01:54, 47.0MB/s]\u001b[A\n"," 25%|██▍       | 1.62G/6.63G [00:42\u003c01:54, 47.1MB/s]\u001b[A\n"," 25%|██▍       | 1.63G/6.63G [00:42\u003c02:05, 42.9MB/s]\u001b[A\n"," 25%|██▍       | 1.63G/6.63G [00:42\u003c02:01, 44.2MB/s]\u001b[A\n"," 25%|██▍       | 1.64G/6.63G [00:42\u003c02:03, 43.3MB/s]\u001b[A\n"," 25%|██▍       | 1.64G/6.63G [00:42\u003c02:13, 40.1MB/s]\u001b[A\n"," 25%|██▍       | 1.65G/6.63G [00:43\u003c02:18, 38.7MB/s]\u001b[A\n"," 25%|██▍       | 1.65G/6.63G [00:43\u003c01:57, 45.4MB/s]\u001b[A\n"," 25%|██▌       | 1.66G/6.63G [00:43\u003c01:51, 47.8MB/s]\u001b[A\n"," 25%|██▌       | 1.66G/6.63G [00:43\u003c01:56, 45.7MB/s]\u001b[A\n"," 25%|██▌       | 1.67G/6.63G [00:43\u003c01:59, 44.7MB/s]\u001b[A\n"," 25%|██▌       | 1.67G/6.63G [00:43\u003c01:58, 45.0MB/s]\u001b[A\n"," 25%|██▌       | 1.68G/6.63G [00:43\u003c01:50, 47.9MB/s]\u001b[A\n"," 25%|██▌       | 1.68G/6.63G [00:43\u003c01:54, 46.2MB/s]\u001b[A\n"," 25%|██▌       | 1.69G/6.63G [00:44\u003c02:00, 44.1MB/s]\u001b[A\n"," 26%|██▌       | 1.69G/6.63G [00:44\u003c01:55, 45.8MB/s]\u001b[A\n"," 26%|██▌       | 1.70G/6.63G [00:44\u003c01:49, 48.4MB/s]\u001b[A\n"," 26%|██▌       | 1.70G/6.63G [00:44\u003c01:54, 46.3MB/s]\u001b[A\n"," 26%|██▌       | 1.71G/6.63G [00:44\u003c01:59, 44.3MB/s]\u001b[A\n"," 26%|██▌       | 1.71G/6.63G [00:44\u003c01:59, 44.2MB/s]\u001b[A\n"," 26%|██▌       | 1.72G/6.63G [00:44\u003c01:49, 48.2MB/s]\u001b[A\n"," 26%|██▌       | 1.72G/6.63G [00:44\u003c01:53, 46.2MB/s]\u001b[A\n"," 26%|██▌       | 1.73G/6.63G [00:44\u003c01:54, 45.9MB/s]\u001b[A\n"," 26%|██▌       | 1.73G/6.63G [00:45\u003c01:57, 44.8MB/s]\u001b[A\n"," 26%|██▌       | 1.74G/6.63G [00:45\u003c01:51, 47.3MB/s]\u001b[A\n"," 26%|██▋       | 1.74G/6.63G [00:45\u003c01:55, 45.6MB/s]\u001b[A\n"," 26%|██▋       | 1.74G/6.63G [00:45\u003c01:57, 44.6MB/s]\u001b[A\n"," 26%|██▋       | 1.75G/6.63G [00:45\u003c01:54, 45.9MB/s]\u001b[A\n"," 26%|██▋       | 1.76G/6.63G [00:45\u003c01:47, 48.5MB/s]\u001b[A\n"," 27%|██▋       | 1.76G/6.63G [00:45\u003c01:51, 46.9MB/s]\u001b[A\n"," 27%|██▋       | 1.77G/6.63G [00:45\u003c01:57, 44.4MB/s]\u001b[A\n"," 27%|██▋       | 1.77G/6.63G [00:46\u003c03:51, 22.6MB/s]\u001b[A\n"," 27%|██▋       | 1.78G/6.63G [00:46\u003c02:16, 38.2MB/s]\u001b[A\n"," 27%|██▋       | 1.79G/6.63G [00:46\u003c02:18, 37.5MB/s]\u001b[A\n"," 27%|██▋       | 1.79G/6.63G [00:46\u003c02:21, 36.6MB/s]\u001b[A\n"," 27%|██▋       | 1.80G/6.63G [00:46\u003c02:14, 38.5MB/s]\u001b[A\n"," 27%|██▋       | 1.80G/6.63G [00:46\u003c02:04, 41.6MB/s]\u001b[A\n"," 27%|██▋       | 1.80G/6.63G [00:47\u003c02:31, 34.2MB/s]\u001b[A\n"," 27%|██▋       | 1.81G/6.63G [00:47\u003c02:15, 38.1MB/s]\u001b[A\n"," 27%|██▋       | 1.82G/6.63G [00:47\u003c02:02, 42.4MB/s]\u001b[A\n"," 27%|██▋       | 1.82G/6.63G [00:47\u003c02:11, 39.4MB/s]\u001b[A\n"," 28%|██▊       | 1.82G/6.63G [00:47\u003c02:10, 39.6MB/s]\u001b[A\n"," 28%|██▊       | 1.83G/6.63G [00:47\u003c02:12, 38.8MB/s]\u001b[A\n"," 28%|██▊       | 1.84G/6.63G [00:47\u003c01:50, 46.6MB/s]\u001b[A\n"," 28%|██▊       | 1.84G/6.63G [00:47\u003c01:54, 45.0MB/s]\u001b[A\n"," 28%|██▊       | 1.84G/6.63G [00:48\u003c01:57, 43.9MB/s]\u001b[A\n"," 28%|██▊       | 1.85G/6.63G [00:48\u003c02:00, 42.6MB/s]\u001b[A\n"," 28%|██▊       | 1.85G/6.63G [00:48\u003c01:44, 49.2MB/s]\u001b[A\n"," 28%|██▊       | 1.86G/6.63G [00:48\u003c02:17, 37.4MB/s]\u001b[A\n"," 28%|██▊       | 1.86G/6.63G [00:48\u003c02:06, 40.6MB/s]\u001b[A\n"," 28%|██▊       | 1.87G/6.63G [00:48\u003c01:51, 46.0MB/s]\u001b[A\n"," 28%|██▊       | 1.88G/6.63G [00:48\u003c01:52, 45.4MB/s]\u001b[A\n"," 28%|██▊       | 1.88G/6.63G [00:48\u003c02:01, 42.0MB/s]\u001b[A\n"," 28%|██▊       | 1.89G/6.63G [00:49\u003c01:48, 46.8MB/s]\u001b[A\n"," 29%|██▊       | 1.89G/6.63G [00:49\u003c02:07, 39.9MB/s]\u001b[A\n"," 29%|██▊       | 1.90G/6.63G [00:49\u003c02:10, 38.9MB/s]\u001b[A\n"," 29%|██▊       | 1.90G/6.63G [00:49\u003c01:57, 43.0MB/s]\u001b[A\n"," 29%|██▉       | 1.91G/6.63G [00:49\u003c01:49, 46.2MB/s]\u001b[A\n"," 29%|██▉       | 1.91G/6.63G [00:49\u003c01:54, 44.3MB/s]\u001b[A\n"," 29%|██▉       | 1.92G/6.63G [00:49\u003c01:55, 43.8MB/s]\u001b[A\n"," 29%|██▉       | 1.92G/6.63G [00:49\u003c01:53, 44.7MB/s]\u001b[A\n"," 29%|██▉       | 1.93G/6.63G [00:50\u003c01:56, 43.3MB/s]\u001b[A\n"," 29%|██▉       | 1.93G/6.63G [00:50\u003c01:49, 46.3MB/s]\u001b[A\n"," 29%|██▉       | 1.94G/6.63G [00:50\u003c01:59, 42.3MB/s]\u001b[A\n"," 29%|██▉       | 1.94G/6.63G [00:50\u003c01:42, 48.9MB/s]\u001b[A\n"," 29%|██▉       | 1.95G/6.63G [00:50\u003c01:50, 45.5MB/s]\u001b[A\n"," 29%|██▉       | 1.95G/6.63G [00:50\u003c01:53, 44.2MB/s]\u001b[A\n"," 29%|██▉       | 1.96G/6.63G [00:50\u003c01:57, 42.7MB/s]\u001b[A\n"," 30%|██▉       | 1.96G/6.63G [00:50\u003c01:57, 42.7MB/s]\u001b[A\n"," 30%|██▉       | 1.96G/6.63G [00:51\u003c01:50, 45.2MB/s]\u001b[A\n"," 30%|██▉       | 1.97G/6.63G [00:51\u003c01:54, 43.6MB/s]\u001b[A\n"," 30%|██▉       | 1.97G/6.63G [00:51\u003c01:59, 41.9MB/s]\u001b[A\n"," 30%|██▉       | 1.98G/6.63G [00:51\u003c01:49, 45.4MB/s]\u001b[A\n"," 30%|██▉       | 1.98G/6.63G [00:51\u003c01:43, 48.0MB/s]\u001b[A\n"," 30%|██▉       | 1.99G/6.63G [00:51\u003c01:47, 46.3MB/s]\u001b[A\n"," 30%|███       | 1.99G/6.63G [00:51\u003c01:53, 44.1MB/s]\u001b[A\n"," 30%|███       | 2.00G/6.63G [00:51\u003c01:48, 45.7MB/s]\u001b[A\n"," 30%|███       | 2.00G/6.63G [00:51\u003c01:41, 48.7MB/s]\u001b[A\n"," 30%|███       | 2.01G/6.63G [00:52\u003c01:47, 46.0MB/s]\u001b[A\n"," 30%|███       | 2.01G/6.63G [00:52\u003c01:51, 44.5MB/s]\u001b[A\n"," 30%|███       | 2.02G/6.63G [00:52\u003c01:50, 44.7MB/s]\u001b[A\n"," 31%|███       | 2.02G/6.63G [00:52\u003c01:43, 47.8MB/s]\u001b[A\n"," 31%|███       | 2.03G/6.63G [00:52\u003c01:47, 45.9MB/s]\u001b[A\n"," 31%|███       | 2.03G/6.63G [00:52\u003c01:51, 44.4MB/s]\u001b[A\n"," 31%|███       | 2.04G/6.63G [00:53\u003c04:17, 19.1MB/s]\u001b[A\n"," 31%|███       | 2.04G/6.63G [00:53\u003c03:19, 24.7MB/s]\u001b[A\n"," 31%|███       | 2.05G/6.63G [00:53\u003c02:33, 32.0MB/s]\u001b[A\n"," 31%|███       | 2.06G/6.63G [00:53\u003c02:31, 32.4MB/s]\u001b[A\n"," 31%|███       | 2.06G/6.63G [00:53\u003c02:27, 33.2MB/s]\u001b[A\n"," 31%|███       | 2.07G/6.63G [00:53\u003c02:03, 39.5MB/s]\u001b[A\n"," 31%|███       | 2.07G/6.63G [00:54\u003c02:03, 39.7MB/s]\u001b[A\n"," 31%|███▏      | 2.07G/6.63G [00:54\u003c02:04, 39.3MB/s]\u001b[A\n"," 31%|███▏      | 2.08G/6.63G [00:54\u003c02:03, 39.4MB/s]\u001b[A\n"," 31%|███▏      | 2.08G/6.63G [00:54\u003c02:00, 40.6MB/s]\u001b[A\n"," 32%|███▏      | 2.09G/6.63G [00:54\u003c01:44, 46.8MB/s]\u001b[A\n"," 32%|███▏      | 2.09G/6.63G [00:54\u003c01:47, 45.2MB/s]\u001b[A\n"," 32%|███▏      | 2.10G/6.63G [00:54\u003c01:53, 42.9MB/s]\u001b[A\n"," 32%|███▏      | 2.10G/6.63G [00:54\u003c01:56, 41.8MB/s]\u001b[A\n"," 32%|███▏      | 2.11G/6.63G [00:54\u003c01:46, 45.6MB/s]\u001b[A\n"," 32%|███▏      | 2.11G/6.63G [00:55\u003c01:50, 43.7MB/s]\u001b[A\n"," 32%|███▏      | 2.12G/6.63G [00:55\u003c01:42, 47.2MB/s]\u001b[A\n"," 32%|███▏      | 2.12G/6.63G [00:55\u003c01:45, 45.7MB/s]\u001b[A\n"," 32%|███▏      | 2.13G/6.63G [00:55\u003c01:44, 46.1MB/s]\u001b[A\n"," 32%|███▏      | 2.13G/6.63G [00:55\u003c01:47, 44.8MB/s]\u001b[A\n"," 32%|███▏      | 2.14G/6.63G [00:55\u003c01:41, 47.4MB/s]\u001b[A\n"," 32%|███▏      | 2.14G/6.63G [00:55\u003c01:47, 44.9MB/s]\u001b[A\n"," 32%|███▏      | 2.15G/6.63G [00:55\u003c01:41, 47.2MB/s]\u001b[A\n"," 32%|███▏      | 2.15G/6.63G [00:56\u003c02:16, 35.1MB/s]\u001b[A\n"," 33%|███▎      | 2.16G/6.63G [00:56\u003c01:55, 41.7MB/s]\u001b[A\n"," 33%|███▎      | 2.17G/6.63G [00:56\u003c01:44, 45.8MB/s]\u001b[A\n"," 33%|███▎      | 2.17G/6.63G [00:56\u003c01:52, 42.7MB/s]\u001b[A\n"," 33%|███▎      | 2.17G/6.63G [00:56\u003c01:53, 42.0MB/s]\u001b[A\n"," 33%|███▎      | 2.18G/6.63G [00:56\u003c01:47, 44.4MB/s]\u001b[A\n"," 33%|███▎      | 2.18G/6.63G [00:56\u003c01:50, 43.1MB/s]\u001b[A\n"," 33%|███▎      | 2.19G/6.63G [00:56\u003c01:52, 42.3MB/s]\u001b[A\n"," 33%|███▎      | 2.19G/6.63G [00:57\u003c01:56, 40.8MB/s]\u001b[A\n"," 33%|███▎      | 2.20G/6.63G [00:57\u003c01:45, 45.1MB/s]\u001b[A\n"," 33%|███▎      | 2.20G/6.63G [00:57\u003c01:39, 47.6MB/s]\u001b[A\n"," 33%|███▎      | 2.21G/6.63G [00:57\u003c01:43, 46.0MB/s]\u001b[A\n"," 33%|███▎      | 2.21G/6.63G [00:57\u003c01:47, 44.0MB/s]\u001b[A\n"," 33%|███▎      | 2.22G/6.63G [00:57\u003c01:45, 45.1MB/s]\u001b[A\n"," 34%|███▎      | 2.22G/6.63G [00:57\u003c01:38, 48.2MB/s]\u001b[A\n"," 34%|███▎      | 2.23G/6.63G [00:57\u003c01:42, 46.1MB/s]\u001b[A\n"," 34%|███▎      | 2.23G/6.63G [00:57\u003c01:47, 44.1MB/s]\u001b[A\n"," 34%|███▎      | 2.24G/6.63G [00:58\u003c01:47, 44.1MB/s]\u001b[A\n"," 34%|███▍      | 2.24G/6.63G [00:58\u003c01:40, 47.0MB/s]\u001b[A\n"," 34%|███▍      | 2.25G/6.63G [00:58\u003c01:49, 42.8MB/s]\u001b[A\n"," 34%|███▍      | 2.25G/6.63G [00:58\u003c01:49, 42.9MB/s]\u001b[A\n"," 34%|███▍      | 2.26G/6.63G [00:58\u003c01:44, 44.9MB/s]\u001b[A\n"," 34%|███▍      | 2.26G/6.63G [00:58\u003c01:46, 44.0MB/s]\u001b[A\n"," 34%|███▍      | 2.26G/6.63G [00:58\u003c01:55, 40.5MB/s]\u001b[A\n"," 34%|███▍      | 2.27G/6.63G [00:58\u003c01:59, 39.2MB/s]\u001b[A\n"," 34%|███▍      | 2.27G/6.63G [00:58\u003c01:51, 41.9MB/s]\u001b[A\n"," 34%|███▍      | 2.28G/6.63G [00:59\u003c01:46, 44.0MB/s]\u001b[A\n"," 34%|███▍      | 2.28G/6.63G [00:59\u003c01:56, 40.2MB/s]\u001b[A\n"," 34%|███▍      | 2.29G/6.63G [00:59\u003c01:59, 39.0MB/s]\u001b[A\n"," 35%|███▍      | 2.29G/6.63G [00:59\u003c02:03, 37.8MB/s]\u001b[A\n"," 35%|███▍      | 2.30G/6.63G [00:59\u003c01:40, 46.5MB/s]\u001b[A\n"," 35%|███▍      | 2.30G/6.63G [00:59\u003c01:44, 44.5MB/s]\u001b[A\n"," 35%|███▍      | 2.30G/6.63G [00:59\u003c01:49, 42.5MB/s]\u001b[A\n"," 35%|███▍      | 2.31G/6.63G [00:59\u003c01:52, 41.3MB/s]\u001b[A\n"," 35%|███▍      | 2.31G/6.63G [00:59\u003c01:44, 44.5MB/s]\u001b[A\n"," 35%|███▍      | 2.32G/6.63G [01:00\u003c01:44, 44.2MB/s]\u001b[A\n"," 35%|███▌      | 2.32G/6.63G [01:00\u003c01:48, 42.5MB/s]\u001b[A\n"," 35%|███▌      | 2.33G/6.63G [01:00\u003c01:54, 40.5MB/s]\u001b[A\n"," 35%|███▌      | 2.33G/6.63G [01:00\u003c01:42, 45.1MB/s]\u001b[A\n"," 35%|███▌      | 2.34G/6.63G [01:00\u003c01:35, 48.1MB/s]\u001b[A\n"," 35%|███▌      | 2.34G/6.63G [01:00\u003c01:40, 45.9MB/s]\u001b[A\n"," 35%|███▌      | 2.35G/6.63G [01:00\u003c01:44, 43.8MB/s]\u001b[A\n"," 35%|███▌      | 2.35G/6.63G [01:01\u003c04:08, 18.5MB/s]\u001b[A\n"," 36%|███▌      | 2.36G/6.63G [01:01\u003c02:20, 32.5MB/s]\u001b[A\n"," 36%|███▌      | 2.37G/6.63G [01:01\u003c02:17, 33.2MB/s]\u001b[A\n"," 36%|███▌      | 2.37G/6.63G [01:01\u003c02:14, 33.9MB/s]\u001b[A\n"," 36%|███▌      | 2.38G/6.63G [01:01\u003c02:06, 36.2MB/s]\u001b[A\n"," 36%|███▌      | 2.38G/6.63G [01:02\u003c01:50, 41.1MB/s]\u001b[A\n"," 36%|███▌      | 2.39G/6.63G [01:02\u003c01:49, 41.4MB/s]\u001b[A\n"," 36%|███▌      | 2.39G/6.63G [01:02\u003c01:49, 41.7MB/s]\u001b[A\n"," 36%|███▌      | 2.40G/6.63G [01:02\u003c01:47, 42.4MB/s]\u001b[A\n"," 36%|███▌      | 2.40G/6.63G [01:02\u003c01:44, 43.3MB/s]\u001b[A\n"," 36%|███▋      | 2.41G/6.63G [01:02\u003c01:46, 42.6MB/s]\u001b[A\n"," 36%|███▋      | 2.41G/6.63G [01:02\u003c01:52, 40.1MB/s]\u001b[A\n"," 36%|███▋      | 2.42G/6.63G [01:02\u003c01:41, 44.4MB/s]\u001b[A\n"," 37%|███▋      | 2.42G/6.63G [01:03\u003c01:33, 48.1MB/s]\u001b[A\n"," 37%|███▋      | 2.43G/6.63G [01:03\u003c02:02, 36.9MB/s]\u001b[A\n"," 37%|███▋      | 2.44G/6.63G [01:03\u003c01:37, 46.3MB/s]\u001b[A\n"," 37%|███▋      | 2.44G/6.63G [01:03\u003c01:32, 48.8MB/s]\u001b[A\n"," 37%|███▋      | 2.45G/6.63G [01:03\u003c01:38, 45.4MB/s]\u001b[A\n"," 37%|███▋      | 2.45G/6.63G [01:03\u003c01:39, 45.0MB/s]\u001b[A\n"," 37%|███▋      | 2.45G/6.63G [01:03\u003c01:41, 44.0MB/s]\u001b[A\n"," 37%|███▋      | 2.46G/6.63G [01:03\u003c01:41, 44.2MB/s]\u001b[A\n"," 37%|███▋      | 2.46G/6.63G [01:04\u003c01:46, 42.0MB/s]\u001b[A\n"," 37%|███▋      | 2.47G/6.63G [01:04\u003c01:45, 42.2MB/s]\u001b[A\n"," 37%|███▋      | 2.47G/6.63G [01:04\u003c01:40, 44.5MB/s]\u001b[A\n"," 37%|███▋      | 2.48G/6.63G [01:04\u003c01:35, 46.8MB/s]\u001b[A\n"," 37%|███▋      | 2.48G/6.63G [01:04\u003c01:38, 45.4MB/s]\u001b[A\n"," 38%|███▊      | 2.49G/6.63G [01:04\u003c01:42, 43.4MB/s]\u001b[A\n"," 38%|███▊      | 2.49G/6.63G [01:04\u003c01:39, 44.7MB/s]\u001b[A\n"," 38%|███▊      | 2.50G/6.63G [01:04\u003c01:32, 47.7MB/s]\u001b[A\n"," 38%|███▊      | 2.50G/6.63G [01:04\u003c01:37, 45.6MB/s]\u001b[A\n"," 38%|███▊      | 2.51G/6.63G [01:05\u003c01:42, 43.3MB/s]\u001b[A\n"," 38%|███▊      | 2.51G/6.63G [01:05\u003c01:34, 46.9MB/s]\u001b[A\n"," 38%|███▊      | 2.52G/6.63G [01:05\u003c01:32, 47.8MB/s]\u001b[A\n"," 38%|███▊      | 2.52G/6.63G [01:05\u003c01:35, 46.4MB/s]\u001b[A\n"," 38%|███▊      | 2.53G/6.63G [01:05\u003c01:38, 44.6MB/s]\u001b[A\n"," 38%|███▊      | 2.53G/6.63G [01:05\u003c01:38, 44.7MB/s]\u001b[A\n"," 38%|███▊      | 2.54G/6.63G [01:06\u003c03:35, 20.4MB/s]\u001b[A\n"," 38%|███▊      | 2.54G/6.63G [01:06\u003c03:09, 23.2MB/s]\u001b[A\n"," 39%|███▊      | 2.55G/6.63G [01:06\u003c02:05, 34.8MB/s]\u001b[A\n"," 39%|███▊      | 2.56G/6.63G [01:06\u003c02:06, 34.5MB/s]\u001b[A\n"," 39%|███▊      | 2.56G/6.63G [01:06\u003c02:04, 35.1MB/s]\u001b[A\n"," 39%|███▊      | 2.57G/6.63G [01:06\u003c01:53, 38.5MB/s]\u001b[A\n"," 39%|███▉      | 2.57G/6.63G [01:07\u003c01:49, 39.8MB/s]\u001b[A\n"," 39%|███▉      | 2.57G/6.63G [01:07\u003c01:50, 39.3MB/s]\u001b[A\n"," 39%|███▉      | 2.58G/6.63G [01:07\u003c01:49, 39.8MB/s]\u001b[A\n"," 39%|███▉      | 2.58G/6.63G [01:07\u003c01:41, 42.7MB/s]\u001b[A\n"," 39%|███▉      | 2.59G/6.63G [01:07\u003c01:35, 45.5MB/s]\u001b[A\n"," 39%|███▉      | 2.59G/6.63G [01:07\u003c01:38, 44.0MB/s]\u001b[A\n"," 39%|███▉      | 2.60G/6.63G [01:07\u003c01:40, 42.9MB/s]\u001b[A\n"," 39%|███▉      | 2.60G/6.63G [01:07\u003c01:36, 44.7MB/s]\u001b[A\n"," 39%|███▉      | 2.61G/6.63G [01:07\u003c01:29, 48.0MB/s]\u001b[A\n"," 39%|███▉      | 2.61G/6.63G [01:08\u003c01:34, 45.6MB/s]\u001b[A\n"," 39%|███▉      | 2.62G/6.63G [01:08\u003c01:39, 43.4MB/s]\u001b[A\n"," 40%|███▉      | 2.62G/6.63G [01:08\u003c01:35, 44.9MB/s]\u001b[A\n"," 40%|███▉      | 2.63G/6.63G [01:08\u003c01:29, 47.8MB/s]\u001b[A\n"," 40%|███▉      | 2.63G/6.63G [01:08\u003c01:33, 45.7MB/s]\u001b[A\n"," 40%|███▉      | 2.64G/6.63G [01:08\u003c01:37, 44.1MB/s]\u001b[A\n"," 40%|███▉      | 2.64G/6.63G [01:08\u003c01:35, 44.6MB/s]\u001b[A\n"," 40%|███▉      | 2.65G/6.63G [01:08\u003c01:28, 48.2MB/s]\u001b[A\n"," 40%|████      | 2.65G/6.63G [01:08\u003c01:32, 45.9MB/s]\u001b[A\n"," 40%|████      | 2.66G/6.63G [01:09\u003c01:36, 44.2MB/s]\u001b[A\n"," 40%|████      | 2.66G/6.63G [01:09\u003c01:35, 44.5MB/s]\u001b[A\n"," 40%|████      | 2.67G/6.63G [01:09\u003c01:31, 46.7MB/s]\u001b[A\n"," 40%|████      | 2.67G/6.63G [01:09\u003c01:36, 44.2MB/s]\u001b[A\n"," 40%|████      | 2.68G/6.63G [01:09\u003c01:40, 42.3MB/s]\u001b[A\n"," 40%|████      | 2.68G/6.63G [01:09\u003c01:33, 45.3MB/s]\u001b[A\n"," 41%|████      | 2.69G/6.63G [01:09\u003c01:28, 47.7MB/s]\u001b[A\n"," 41%|████      | 2.69G/6.63G [01:09\u003c01:32, 45.6MB/s]\u001b[A\n"," 41%|████      | 2.70G/6.63G [01:10\u003c01:36, 43.7MB/s]\u001b[A\n"," 41%|████      | 2.70G/6.63G [01:10\u003c01:36, 43.8MB/s]\u001b[A\n"," 41%|████      | 2.71G/6.63G [01:10\u003c01:24, 49.6MB/s]\u001b[A\n"," 41%|████      | 2.71G/6.63G [01:10\u003c01:27, 48.0MB/s]\u001b[A\n"," 41%|████      | 2.72G/6.63G [01:10\u003c01:31, 45.7MB/s]\u001b[A\n"," 41%|████      | 2.72G/6.63G [01:10\u003c01:33, 45.0MB/s]\u001b[A\n"," 41%|████      | 2.73G/6.63G [01:10\u003c01:27, 47.9MB/s]\u001b[A\n"," 41%|████      | 2.73G/6.63G [01:10\u003c01:30, 46.4MB/s]\u001b[A\n"," 41%|████▏     | 2.74G/6.63G [01:10\u003c01:38, 42.3MB/s]\u001b[A\n"," 41%|████▏     | 2.74G/6.63G [01:11\u003c01:38, 42.3MB/s]\u001b[A\n"," 41%|████▏     | 2.74G/6.63G [01:11\u003c01:37, 42.7MB/s]\u001b[A\n"," 41%|████▏     | 2.75G/6.63G [01:11\u003c01:41, 41.1MB/s]\u001b[A\n"," 42%|████▏     | 2.75G/6.63G [01:11\u003c01:51, 37.4MB/s]\u001b[A\n"," 42%|████▏     | 2.76G/6.63G [01:11\u003c01:36, 43.1MB/s]\u001b[A\n"," 42%|████▏     | 2.76G/6.63G [01:11\u003c01:28, 47.2MB/s]\u001b[A\n"," 42%|████▏     | 2.77G/6.63G [01:11\u003c01:59, 34.8MB/s]\u001b[A\n"," 42%|████▏     | 2.77G/6.63G [01:11\u003c01:56, 35.6MB/s]\u001b[A\n"," 42%|████▏     | 2.78G/6.63G [01:12\u003c01:32, 44.9MB/s]\u001b[A\n"," 42%|████▏     | 2.78G/6.63G [01:12\u003c01:33, 44.3MB/s]\u001b[A\n"," 42%|████▏     | 2.79G/6.63G [01:12\u003c01:39, 41.6MB/s]\u001b[A\n"," 42%|████▏     | 2.79G/6.63G [01:12\u003c01:41, 40.8MB/s]\u001b[A\n"," 42%|████▏     | 2.80G/6.63G [01:12\u003c01:29, 45.9MB/s]\u001b[A\n"," 42%|████▏     | 2.80G/6.63G [01:12\u003c01:30, 45.3MB/s]\u001b[A\n"," 42%|████▏     | 2.81G/6.63G [01:12\u003c02:00, 34.1MB/s]\u001b[A\n"," 42%|████▏     | 2.81G/6.63G [01:12\u003c01:47, 38.0MB/s]\u001b[A\n"," 42%|████▏     | 2.82G/6.63G [01:13\u003c01:37, 42.1MB/s]\u001b[A\n"," 43%|████▎     | 2.82G/6.63G [01:13\u003c01:39, 41.2MB/s]\u001b[A\n"," 43%|████▎     | 2.83G/6.63G [01:13\u003c01:40, 40.7MB/s]\u001b[A\n"," 43%|████▎     | 2.83G/6.63G [01:13\u003c01:33, 43.9MB/s]\u001b[A\n"," 43%|████▎     | 2.84G/6.63G [01:13\u003c01:26, 47.3MB/s]\u001b[A\n"," 43%|████▎     | 2.84G/6.63G [01:13\u003c01:30, 44.7MB/s]\u001b[A\n"," 43%|████▎     | 2.85G/6.63G [01:13\u003c01:33, 43.6MB/s]\u001b[A\n"," 43%|████▎     | 2.85G/6.63G [01:13\u003c01:28, 45.8MB/s]\u001b[A\n"," 43%|████▎     | 2.86G/6.63G [01:14\u003c01:24, 48.0MB/s]\u001b[A\n"," 43%|████▎     | 2.86G/6.63G [01:14\u003c01:27, 46.1MB/s]\u001b[A\n"," 43%|████▎     | 2.87G/6.63G [01:14\u003c01:30, 44.8MB/s]\u001b[A\n"," 43%|████▎     | 2.87G/6.63G [01:14\u003c01:30, 44.5MB/s]\u001b[A\n"," 43%|████▎     | 2.88G/6.63G [01:14\u003c01:25, 47.4MB/s]\u001b[A\n"," 43%|████▎     | 2.88G/6.63G [01:14\u003c01:29, 44.8MB/s]\u001b[A\n"," 44%|████▎     | 2.89G/6.63G [01:14\u003c01:32, 43.6MB/s]\u001b[A\n"," 44%|████▎     | 2.89G/6.63G [01:14\u003c01:27, 46.0MB/s]\u001b[A\n"," 44%|████▎     | 2.90G/6.63G [01:14\u003c01:23, 48.2MB/s]\u001b[A\n"," 44%|████▍     | 2.90G/6.63G [01:15\u003c01:25, 46.7MB/s]\u001b[A\n"," 44%|████▍     | 2.91G/6.63G [01:15\u003c01:28, 45.2MB/s]\u001b[A\n"," 44%|████▍     | 2.91G/6.63G [01:15\u003c01:29, 44.9MB/s]\u001b[A\n"," 44%|████▍     | 2.92G/6.63G [01:15\u003c01:23, 47.9MB/s]\u001b[A\n"," 44%|████▍     | 2.92G/6.63G [01:15\u003c01:26, 46.3MB/s]\u001b[A\n"," 44%|████▍     | 2.92G/6.63G [01:15\u003c01:29, 44.7MB/s]\u001b[A\n"," 44%|████▍     | 2.93G/6.63G [01:15\u003c01:28, 45.0MB/s]\u001b[A\n"," 44%|████▍     | 2.93G/6.63G [01:15\u003c01:23, 47.3MB/s]\u001b[A\n"," 44%|████▍     | 2.94G/6.63G [01:16\u003c03:09, 20.9MB/s]\u001b[A\n"," 45%|████▍     | 2.95G/6.63G [01:16\u003c01:49, 36.1MB/s]\u001b[A\n"," 45%|████▍     | 2.96G/6.63G [01:16\u003c01:52, 35.0MB/s]\u001b[A\n"," 45%|████▍     | 2.96G/6.63G [01:16\u003c01:50, 35.7MB/s]\u001b[A\n"," 45%|████▍     | 2.97G/6.63G [01:16\u003c01:41, 38.6MB/s]\u001b[A\n"," 45%|████▍     | 2.97G/6.63G [01:17\u003c01:36, 40.6MB/s]\u001b[A\n"," 45%|████▍     | 2.98G/6.63G [01:17\u003c01:34, 41.3MB/s]\u001b[A\n"," 45%|████▍     | 2.98G/6.63G [01:17\u003c01:41, 38.4MB/s]\u001b[A\n"," 45%|████▌     | 2.98G/6.63G [01:17\u003c01:39, 39.4MB/s]\u001b[A\n"," 45%|████▌     | 2.99G/6.63G [01:17\u003c01:39, 39.2MB/s]\u001b[A\n"," 45%|████▌     | 2.99G/6.63G [01:17\u003c01:52, 34.7MB/s]\u001b[A\n"," 45%|████▌     | 3.00G/6.63G [01:17\u003c01:38, 39.6MB/s]\u001b[A\n"," 45%|████▌     | 3.01G/6.63G [01:17\u003c01:27, 44.6MB/s]\u001b[A\n"," 45%|████▌     | 3.01G/6.63G [01:18\u003c01:28, 44.0MB/s]\u001b[A\n"," 45%|████▌     | 3.01G/6.63G [01:18\u003c01:33, 41.3MB/s]\u001b[A\n"," 46%|████▌     | 3.02G/6.63G [01:18\u003c01:33, 41.4MB/s]\u001b[A\n"," 46%|████▌     | 3.03G/6.63G [01:18\u003c01:17, 49.7MB/s]\u001b[A\n"," 46%|████▌     | 3.03G/6.63G [01:18\u003c01:24, 45.5MB/s]\u001b[A\n"," 46%|████▌     | 3.03G/6.63G [01:18\u003c01:28, 43.4MB/s]\u001b[A\n"," 46%|████▌     | 3.04G/6.63G [01:19\u003c03:26, 18.6MB/s]\u001b[A\n"," 46%|████▌     | 3.04G/6.63G [01:19\u003c02:52, 22.3MB/s]\u001b[A\n"," 46%|████▌     | 3.05G/6.63G [01:19\u003c02:01, 31.7MB/s]\u001b[A\n"," 46%|████▌     | 3.06G/6.63G [01:19\u003c02:00, 31.8MB/s]\u001b[A\n"," 46%|████▌     | 3.06G/6.63G [01:19\u003c01:56, 32.8MB/s]\u001b[A\n"," 46%|████▌     | 3.06G/6.63G [01:19\u003c01:47, 35.5MB/s]\u001b[A\n"," 46%|████▋     | 3.07G/6.63G [01:20\u003c01:36, 39.4MB/s]\u001b[A\n"," 46%|████▋     | 3.07G/6.63G [01:20\u003c01:38, 38.7MB/s]\u001b[A\n"," 46%|████▋     | 3.08G/6.63G [01:20\u003c01:39, 38.3MB/s]\u001b[A\n"," 47%|████▋     | 3.08G/6.63G [01:20\u003c01:27, 43.3MB/s]\u001b[A\n"," 47%|████▋     | 3.09G/6.63G [01:20\u003c01:21, 46.6MB/s]\u001b[A\n"," 47%|████▋     | 3.09G/6.63G [01:20\u003c01:25, 44.6MB/s]\u001b[A\n"," 47%|████▋     | 3.10G/6.63G [01:20\u003c01:27, 43.4MB/s]\u001b[A\n"," 47%|████▋     | 3.10G/6.63G [01:20\u003c01:26, 43.8MB/s]\u001b[A\n"," 47%|████▋     | 3.11G/6.63G [01:20\u003c01:20, 47.2MB/s]\u001b[A\n"," 47%|████▋     | 3.11G/6.63G [01:21\u003c01:23, 45.0MB/s]\u001b[A\n"," 47%|████▋     | 3.12G/6.63G [01:21\u003c01:26, 43.4MB/s]\u001b[A\n"," 47%|████▋     | 3.12G/6.63G [01:21\u003c01:23, 45.1MB/s]\u001b[A\n"," 47%|████▋     | 3.13G/6.63G [01:21\u003c01:16, 49.0MB/s]\u001b[A\n"," 47%|████▋     | 3.13G/6.63G [01:21\u003c01:19, 47.2MB/s]\u001b[A\n"," 47%|████▋     | 3.14G/6.63G [01:21\u003c01:23, 44.9MB/s]\u001b[A\n"," 47%|████▋     | 3.14G/6.63G [01:21\u003c01:23, 44.7MB/s]\u001b[A\n"," 47%|████▋     | 3.15G/6.63G [01:21\u003c01:18, 47.4MB/s]\u001b[A\n"," 48%|████▊     | 3.15G/6.63G [01:21\u003c01:22, 45.3MB/s]\u001b[A\n"," 48%|████▊     | 3.16G/6.63G [01:22\u003c01:26, 43.0MB/s]\u001b[A\n"," 48%|████▊     | 3.16G/6.63G [01:22\u003c01:23, 44.8MB/s]\u001b[A\n"," 48%|████▊     | 3.17G/6.63G [01:22\u003c01:17, 48.2MB/s]\u001b[A\n"," 48%|████▊     | 3.17G/6.63G [01:22\u003c01:20, 46.1MB/s]\u001b[A\n"," 48%|████▊     | 3.18G/6.63G [01:22\u003c01:48, 34.3MB/s]\u001b[A\n"," 48%|████▊     | 3.18G/6.63G [01:22\u003c01:39, 37.2MB/s]\u001b[A\n"," 48%|████▊     | 3.19G/6.63G [01:22\u003c01:17, 47.8MB/s]\u001b[A\n"," 48%|████▊     | 3.19G/6.63G [01:23\u003c01:22, 44.9MB/s]\u001b[A\n"," 48%|████▊     | 3.20G/6.63G [01:23\u003c01:25, 43.3MB/s]\u001b[A\n"," 48%|████▊     | 3.20G/6.63G [01:23\u003c01:20, 45.5MB/s]\u001b[A\n"," 48%|████▊     | 3.21G/6.63G [01:23\u003c01:20, 45.4MB/s]\u001b[A\n"," 48%|████▊     | 3.21G/6.63G [01:23\u003c01:24, 43.5MB/s]\u001b[A\n"," 49%|████▊     | 3.22G/6.63G [01:23\u003c01:26, 42.2MB/s]\u001b[A\n"," 49%|████▊     | 3.22G/6.63G [01:23\u003c01:25, 42.8MB/s]\u001b[A\n"," 49%|████▊     | 3.23G/6.63G [01:23\u003c01:20, 45.5MB/s]\u001b[A\n"," 49%|████▊     | 3.23G/6.63G [01:23\u003c01:24, 43.0MB/s]\u001b[A\n"," 49%|████▉     | 3.23G/6.63G [01:24\u003c01:27, 41.9MB/s]\u001b[A\n"," 49%|████▉     | 3.24G/6.63G [01:24\u003c01:21, 44.9MB/s]\u001b[A\n"," 49%|████▉     | 3.25G/6.63G [01:24\u003c01:16, 47.7MB/s]\u001b[A\n"," 49%|████▉     | 3.25G/6.63G [01:24\u003c01:19, 45.7MB/s]\u001b[A\n"," 49%|████▉     | 3.25G/6.63G [01:24\u003c01:23, 43.3MB/s]\u001b[A\n"," 49%|████▉     | 3.26G/6.63G [01:24\u003c01:20, 44.9MB/s]\u001b[A\n"," 49%|████▉     | 3.26G/6.63G [01:24\u003c01:15, 47.8MB/s]\u001b[A\n"," 49%|████▉     | 3.27G/6.63G [01:24\u003c01:20, 45.0MB/s]\u001b[A\n"," 49%|████▉     | 3.27G/6.63G [01:24\u003c01:22, 43.5MB/s]\u001b[A\n"," 49%|████▉     | 3.28G/6.63G [01:25\u003c01:25, 41.9MB/s]\u001b[A\n"," 50%|████▉     | 3.28G/6.63G [01:25\u003c01:12, 49.3MB/s]\u001b[A\n"," 50%|████▉     | 3.29G/6.63G [01:25\u003c01:15, 47.5MB/s]\u001b[A\n"," 50%|████▉     | 3.29G/6.63G [01:25\u003c01:18, 45.6MB/s]\u001b[A\n"," 50%|████▉     | 3.30G/6.63G [01:25\u003c01:21, 43.8MB/s]\u001b[A\n"," 50%|████▉     | 3.30G/6.63G [01:25\u003c01:13, 48.8MB/s]\u001b[A\n"," 50%|████▉     | 3.31G/6.63G [01:25\u003c01:16, 46.3MB/s]\u001b[A\n"," 50%|████▉     | 3.31G/6.63G [01:25\u003c01:19, 44.8MB/s]\u001b[A\n"," 50%|█████     | 3.32G/6.63G [01:25\u003c01:21, 43.9MB/s]\u001b[A\n"," 50%|█████     | 3.32G/6.63G [01:26\u003c01:14, 47.4MB/s]\u001b[A\n"," 50%|█████     | 3.33G/6.63G [01:26\u003c01:17, 45.6MB/s]\u001b[A\n"," 50%|█████     | 3.33G/6.63G [01:26\u003c01:21, 43.6MB/s]\u001b[A\n"," 50%|█████     | 3.34G/6.63G [01:26\u003c01:24, 42.1MB/s]\u001b[A\n"," 50%|█████     | 3.34G/6.63G [01:26\u003c01:18, 44.8MB/s]\u001b[A\n"," 50%|█████     | 3.35G/6.63G [01:26\u003c01:12, 48.8MB/s]\u001b[A\n"," 51%|█████     | 3.35G/6.63G [01:26\u003c01:15, 46.8MB/s]\u001b[A\n"," 51%|█████     | 3.36G/6.63G [01:26\u003c01:22, 42.6MB/s]\u001b[A\n"," 51%|█████     | 3.36G/6.63G [01:26\u003c01:22, 42.7MB/s]\u001b[A\n"," 51%|█████     | 3.37G/6.63G [01:27\u003c01:19, 44.2MB/s]\u001b[A\n"," 51%|█████     | 3.37G/6.63G [01:27\u003c01:22, 42.5MB/s]\u001b[A\n"," 51%|█████     | 3.37G/6.63G [01:27\u003c01:27, 39.7MB/s]\u001b[A\n"," 51%|█████     | 3.38G/6.63G [01:27\u003c01:30, 38.6MB/s]\u001b[A\n"," 51%|█████     | 3.38G/6.63G [01:27\u003c01:13, 47.2MB/s]\u001b[A\n"," 51%|█████     | 3.39G/6.63G [01:27\u003c01:16, 45.5MB/s]\u001b[A\n"," 51%|█████     | 3.39G/6.63G [01:27\u003c01:20, 43.3MB/s]\u001b[A\n"," 51%|█████     | 3.40G/6.63G [01:27\u003c01:22, 42.0MB/s]\u001b[A\n"," 51%|█████▏    | 3.40G/6.63G [01:28\u003c01:20, 43.2MB/s]\u001b[A\n"," 51%|█████▏    | 3.41G/6.63G [01:28\u003c01:18, 44.4MB/s]\u001b[A\n"," 51%|█████▏    | 3.41G/6.63G [01:28\u003c01:20, 42.9MB/s]\u001b[A\n"," 51%|█████▏    | 3.41G/6.63G [01:28\u003c01:21, 42.5MB/s]\u001b[A\n"," 52%|█████▏    | 3.42G/6.63G [01:28\u003c01:16, 44.8MB/s]\u001b[A\n"," 52%|█████▏    | 3.43G/6.63G [01:28\u003c01:12, 47.6MB/s]\u001b[A\n"," 52%|█████▏    | 3.43G/6.63G [01:28\u003c01:15, 45.4MB/s]\u001b[A\n"," 52%|█████▏    | 3.43G/6.63G [01:28\u003c01:19, 43.3MB/s]\u001b[A\n"," 52%|█████▏    | 3.44G/6.63G [01:28\u003c01:15, 45.3MB/s]\u001b[A\n"," 52%|█████▏    | 3.45G/6.63G [01:29\u003c01:10, 48.4MB/s]\u001b[A\n"," 52%|█████▏    | 3.45G/6.63G [01:29\u003c01:13, 46.3MB/s]\u001b[A\n"," 52%|█████▏    | 3.45G/6.63G [01:29\u003c01:16, 44.8MB/s]\u001b[A\n"," 52%|█████▏    | 3.46G/6.63G [01:29\u003c01:16, 44.4MB/s]\u001b[A\n"," 52%|█████▏    | 3.46G/6.63G [01:29\u003c01:10, 48.3MB/s]\u001b[A\n"," 52%|█████▏    | 3.47G/6.63G [01:29\u003c01:14, 45.6MB/s]\u001b[A\n"," 52%|█████▏    | 3.47G/6.63G [01:29\u003c01:16, 44.3MB/s]\u001b[A\n"," 52%|█████▏    | 3.48G/6.63G [01:29\u003c01:14, 45.3MB/s]\u001b[A\n"," 53%|█████▎    | 3.48G/6.63G [01:29\u003c01:10, 47.6MB/s]\u001b[A\n"," 53%|█████▎    | 3.49G/6.63G [01:30\u003c01:12, 46.3MB/s]\u001b[A\n"," 53%|█████▎    | 3.49G/6.63G [01:30\u003c01:17, 43.6MB/s]\u001b[A\n"," 53%|█████▎    | 3.50G/6.63G [01:30\u003c01:13, 45.6MB/s]\u001b[A\n"," 53%|█████▎    | 3.50G/6.63G [01:30\u003c01:09, 48.2MB/s]\u001b[A\n"," 53%|█████▎    | 3.51G/6.63G [01:30\u003c01:12, 46.1MB/s]\u001b[A\n"," 53%|█████▎    | 3.51G/6.63G [01:30\u003c01:14, 45.0MB/s]\u001b[A\n"," 53%|█████▎    | 3.52G/6.63G [01:30\u003c01:14, 45.1MB/s]\u001b[A\n"," 53%|█████▎    | 3.52G/6.63G [01:30\u003c01:09, 47.7MB/s]\u001b[A\n"," 53%|█████▎    | 3.53G/6.63G [01:31\u003c04:20, 12.8MB/s]\u001b[A\n"," 53%|█████▎    | 3.54G/6.63G [01:32\u003c02:27, 22.5MB/s]\u001b[A\n"," 53%|█████▎    | 3.54G/6.63G [01:32\u003c02:16, 24.3MB/s]\u001b[A\n"," 54%|█████▎    | 3.55G/6.63G [01:32\u003c02:04, 26.5MB/s]\u001b[A\n"," 54%|█████▎    | 3.55G/6.63G [01:32\u003c01:53, 29.0MB/s]\u001b[A\n"," 54%|█████▎    | 3.56G/6.63G [01:32\u003c01:46, 31.0MB/s]\u001b[A\n"," 54%|█████▎    | 3.56G/6.63G [01:32\u003c02:00, 27.4MB/s]\u001b[A\n"," 54%|█████▍    | 3.57G/6.63G [01:32\u003c01:47, 30.7MB/s]\u001b[A\n"," 54%|█████▍    | 3.57G/6.63G [01:33\u003c01:40, 32.5MB/s]\u001b[A\n"," 54%|█████▍    | 3.58G/6.63G [01:33\u003c01:25, 38.3MB/s]\u001b[A\n"," 54%|█████▍    | 3.58G/6.63G [01:33\u003c01:25, 38.1MB/s]\u001b[A\n"," 54%|█████▍    | 3.59G/6.63G [01:33\u003c01:17, 42.4MB/s]\u001b[A\n"," 54%|█████▍    | 3.59G/6.63G [01:33\u003c01:17, 42.1MB/s]\u001b[A\n"," 54%|█████▍    | 3.60G/6.63G [01:33\u003c01:12, 44.9MB/s]\u001b[A\n"," 54%|█████▍    | 3.60G/6.63G [01:33\u003c01:15, 43.3MB/s]\u001b[A\n"," 54%|█████▍    | 3.60G/6.63G [01:33\u003c01:16, 42.5MB/s]\u001b[A\n"," 54%|█████▍    | 3.61G/6.63G [01:33\u003c01:22, 39.2MB/s]\u001b[A\n"," 54%|█████▍    | 3.61G/6.63G [01:34\u003c01:19, 40.6MB/s]\u001b[A\n"," 55%|█████▍    | 3.62G/6.63G [01:34\u003c01:16, 42.2MB/s]\u001b[A\n"," 55%|█████▍    | 3.62G/6.63G [01:34\u003c01:19, 40.9MB/s]\u001b[A\n"," 55%|█████▍    | 3.63G/6.63G [01:34\u003c01:22, 39.3MB/s]\u001b[A\n"," 55%|█████▍    | 3.63G/6.63G [01:34\u003c01:13, 43.8MB/s]\u001b[A\n"," 55%|█████▍    | 3.64G/6.63G [01:34\u003c01:07, 47.5MB/s]\u001b[A\n"," 55%|█████▍    | 3.64G/6.63G [01:34\u003c01:10, 45.3MB/s]\u001b[A\n"," 55%|█████▍    | 3.65G/6.63G [01:34\u003c01:13, 43.5MB/s]\u001b[A\n"," 55%|█████▌    | 3.65G/6.63G [01:35\u003c01:15, 42.2MB/s]\u001b[A\n"," 55%|█████▌    | 3.66G/6.63G [01:35\u003c01:05, 49.0MB/s]\u001b[A\n"," 55%|█████▌    | 3.66G/6.63G [01:35\u003c01:08, 46.8MB/s]\u001b[A\n"," 55%|█████▌    | 3.67G/6.63G [01:35\u003c01:09, 45.7MB/s]\u001b[A\n"," 55%|█████▌    | 3.67G/6.63G [01:35\u003c01:13, 43.1MB/s]\u001b[A\n"," 55%|█████▌    | 3.67G/6.63G [01:35\u003c01:13, 43.0MB/s]\u001b[A\n"," 55%|█████▌    | 3.68G/6.63G [01:35\u003c01:07, 46.7MB/s]\u001b[A\n"," 56%|█████▌    | 3.68G/6.63G [01:35\u003c01:11, 44.0MB/s]\u001b[A\n"," 56%|█████▌    | 3.69G/6.63G [01:35\u003c01:13, 42.8MB/s]\u001b[A\n"," 56%|█████▌    | 3.69G/6.63G [01:36\u003c01:08, 46.0MB/s]\u001b[A\n"," 56%|█████▌    | 3.70G/6.63G [01:36\u003c01:04, 49.0MB/s]\u001b[A\n"," 56%|█████▌    | 3.70G/6.63G [01:36\u003c01:07, 46.9MB/s]\u001b[A\n"," 56%|█████▌    | 3.71G/6.63G [01:36\u003c01:10, 44.5MB/s]\u001b[A\n"," 56%|█████▌    | 3.71G/6.63G [01:36\u003c01:10, 44.7MB/s]\u001b[A\n"," 56%|█████▌    | 3.72G/6.63G [01:36\u003c01:05, 47.6MB/s]\u001b[A\n"," 56%|█████▌    | 3.72G/6.63G [01:36\u003c01:07, 45.9MB/s]\u001b[A\n"," 56%|█████▌    | 3.73G/6.63G [01:36\u003c01:11, 43.6MB/s]\u001b[A\n"," 56%|█████▋    | 3.73G/6.63G [01:36\u003c01:09, 44.5MB/s]\u001b[A\n"," 56%|█████▋    | 3.74G/6.63G [01:37\u003c01:04, 48.1MB/s]\u001b[A\n"," 56%|█████▋    | 3.74G/6.63G [01:37\u003c01:08, 45.3MB/s]\u001b[A\n"," 57%|█████▋    | 3.75G/6.63G [01:37\u003c01:10, 43.9MB/s]\u001b[A\n"," 57%|█████▋    | 3.75G/6.63G [01:37\u003c01:07, 45.8MB/s]\u001b[A\n"," 57%|█████▋    | 3.76G/6.63G [01:37\u003c01:03, 48.3MB/s]\u001b[A\n"," 57%|█████▋    | 3.76G/6.63G [01:37\u003c01:06, 46.4MB/s]\u001b[A\n"," 57%|█████▋    | 3.77G/6.63G [01:37\u003c01:09, 44.3MB/s]\u001b[A\n"," 57%|█████▋    | 3.77G/6.63G [01:37\u003c01:06, 46.3MB/s]\u001b[A\n"," 57%|█████▋    | 3.78G/6.63G [01:37\u003c01:05, 46.6MB/s]\u001b[A\n"," 57%|█████▋    | 3.78G/6.63G [01:38\u003c01:07, 45.2MB/s]\u001b[A\n"," 57%|█████▋    | 3.79G/6.63G [01:38\u003c01:09, 43.9MB/s]\u001b[A\n"," 57%|█████▋    | 3.79G/6.63G [01:38\u003c01:05, 46.4MB/s]\u001b[A\n"," 57%|█████▋    | 3.80G/6.63G [01:38\u003c01:02, 48.4MB/s]\u001b[A\n"," 57%|█████▋    | 3.80G/6.63G [01:38\u003c01:05, 46.7MB/s]\u001b[A\n"," 57%|█████▋    | 3.81G/6.63G [01:38\u003c01:07, 44.6MB/s]\u001b[A\n"," 57%|█████▋    | 3.81G/6.63G [01:38\u003c01:08, 44.4MB/s]\u001b[A\n"," 58%|█████▊    | 3.82G/6.63G [01:38\u003c01:02, 48.1MB/s]\u001b[A\n"," 58%|█████▊    | 3.82G/6.63G [01:39\u003c01:05, 46.0MB/s]\u001b[A\n"," 58%|█████▊    | 3.82G/6.63G [01:39\u003c01:07, 44.5MB/s]\u001b[A\n"," 58%|█████▊    | 3.83G/6.63G [01:39\u003c01:05, 45.8MB/s]\u001b[A\n"," 58%|█████▊    | 3.84G/6.63G [01:39\u003c01:02, 48.1MB/s]\u001b[A\n"," 58%|█████▊    | 3.84G/6.63G [01:39\u003c01:05, 45.8MB/s]\u001b[A\n"," 58%|█████▊    | 3.84G/6.63G [01:39\u003c01:07, 44.1MB/s]\u001b[A\n"," 58%|█████▊    | 3.85G/6.63G [01:39\u003c01:05, 45.3MB/s]\u001b[A\n"," 58%|█████▊    | 3.86G/6.63G [01:39\u003c01:01, 48.1MB/s]\u001b[A\n"," 58%|█████▊    | 3.86G/6.63G [01:39\u003c01:04, 46.4MB/s]\u001b[A\n"," 58%|█████▊    | 3.86G/6.63G [01:40\u003c01:06, 44.6MB/s]\u001b[A\n"," 58%|█████▊    | 3.87G/6.63G [01:40\u003c01:05, 45.2MB/s]\u001b[A\n"," 58%|█████▊    | 3.87G/6.63G [01:40\u003c01:02, 47.5MB/s]\u001b[A\n"," 59%|█████▊    | 3.88G/6.63G [01:40\u003c01:04, 45.6MB/s]\u001b[A\n"," 59%|█████▊    | 3.88G/6.63G [01:40\u003c01:06, 44.2MB/s]\u001b[A\n"," 59%|█████▊    | 3.89G/6.63G [01:40\u003c01:05, 45.0MB/s]\u001b[A\n"," 59%|█████▊    | 3.89G/6.63G [01:40\u003c01:01, 47.7MB/s]\u001b[A\n"," 59%|█████▉    | 3.90G/6.63G [01:40\u003c01:04, 45.6MB/s]\u001b[A\n"," 59%|█████▉    | 3.90G/6.63G [01:40\u003c01:07, 43.7MB/s]\u001b[A\n"," 59%|█████▉    | 3.91G/6.63G [01:41\u003c01:03, 45.7MB/s]\u001b[A\n"," 59%|█████▉    | 3.91G/6.63G [01:41\u003c01:00, 48.5MB/s]\u001b[A\n"," 59%|█████▉    | 3.92G/6.63G [01:41\u003c01:01, 47.3MB/s]\u001b[A\n"," 59%|█████▉    | 3.92G/6.63G [01:41\u003c01:05, 44.6MB/s]\u001b[A\n"," 59%|█████▉    | 3.93G/6.63G [01:41\u003c01:04, 44.7MB/s]\u001b[A\n"," 59%|█████▉    | 3.93G/6.63G [01:41\u003c01:00, 48.2MB/s]\u001b[A\n"," 59%|█████▉    | 3.94G/6.63G [01:41\u003c01:02, 46.4MB/s]\u001b[A\n"," 59%|█████▉    | 3.94G/6.63G [01:41\u003c01:04, 44.7MB/s]\u001b[A\n"," 60%|█████▉    | 3.95G/6.63G [01:41\u003c01:04, 45.0MB/s]\u001b[A\n"," 60%|█████▉    | 3.95G/6.63G [01:42\u003c01:01, 46.9MB/s]\u001b[A\n"," 60%|█████▉    | 3.96G/6.63G [01:42\u003c01:05, 44.0MB/s]\u001b[A\n"," 60%|█████▉    | 3.96G/6.63G [01:42\u003c01:08, 42.1MB/s]\u001b[A\n"," 60%|█████▉    | 3.97G/6.63G [01:42\u003c01:01, 46.8MB/s]\u001b[A\n"," 60%|█████▉    | 3.97G/6.63G [01:42\u003c00:58, 48.8MB/s]\u001b[A\n"," 60%|█████▉    | 3.98G/6.63G [01:42\u003c01:00, 46.8MB/s]\u001b[A\n"," 60%|██████    | 3.98G/6.63G [01:42\u003c01:02, 45.7MB/s]\u001b[A\n"," 60%|██████    | 3.99G/6.63G [01:42\u003c01:02, 45.5MB/s]\u001b[A\n"," 60%|██████    | 3.99G/6.63G [01:42\u003c00:58, 48.3MB/s]\u001b[A\n"," 60%|██████    | 4.00G/6.63G [01:43\u003c01:01, 46.2MB/s]\u001b[A\n"," 60%|██████    | 4.00G/6.63G [01:43\u003c01:03, 44.2MB/s]\u001b[A\n"," 60%|██████    | 4.01G/6.63G [01:43\u003c00:58, 48.5MB/s]\u001b[A\n"," 60%|██████    | 4.01G/6.63G [01:43\u003c01:00, 46.4MB/s]\u001b[A\n"," 61%|██████    | 4.02G/6.63G [01:43\u003c01:03, 44.2MB/s]\u001b[A\n"," 61%|██████    | 4.02G/6.63G [01:43\u003c01:05, 43.0MB/s]\u001b[A\n"," 61%|██████    | 4.03G/6.63G [01:43\u003c00:59, 46.7MB/s]\u001b[A\n"," 61%|██████    | 4.03G/6.63G [01:43\u003c00:56, 49.4MB/s]\u001b[A\n"," 61%|██████    | 4.04G/6.63G [01:44\u003c00:59, 47.1MB/s]\u001b[A\n"," 61%|██████    | 4.04G/6.63G [01:44\u003c02:20, 19.8MB/s]\u001b[A\n"," 61%|██████    | 4.05G/6.63G [01:44\u003c01:52, 24.7MB/s]\u001b[A\n"," 61%|██████    | 4.05G/6.63G [01:44\u003c01:27, 31.5MB/s]\u001b[A\n"," 61%|██████    | 4.06G/6.63G [01:45\u003c01:26, 31.8MB/s]\u001b[A\n"," 61%|██████▏   | 4.06G/6.63G [01:45\u003c01:25, 32.4MB/s]\u001b[A\n"," 61%|██████▏   | 4.07G/6.63G [01:45\u003c01:13, 37.3MB/s]\u001b[A\n"," 61%|██████▏   | 4.07G/6.63G [01:45\u003c01:09, 39.3MB/s]\u001b[A\n"," 61%|██████▏   | 4.08G/6.63G [01:45\u003c01:09, 39.2MB/s]\u001b[A\n"," 62%|██████▏   | 4.08G/6.63G [01:45\u003c01:14, 36.9MB/s]\u001b[A\n"," 62%|██████▏   | 4.08G/6.63G [01:45\u003c01:15, 36.2MB/s]\u001b[A\n"," 62%|██████▏   | 4.09G/6.63G [01:45\u003c01:12, 37.4MB/s]\u001b[A\n"," 62%|██████▏   | 4.09G/6.63G [01:45\u003c01:03, 43.0MB/s]\u001b[A\n"," 62%|██████▏   | 4.10G/6.63G [01:46\u003c02:22, 19.1MB/s]\u001b[A\n"," 62%|██████▏   | 4.11G/6.63G [01:46\u003c01:20, 33.7MB/s]\u001b[A\n"," 62%|██████▏   | 4.11G/6.63G [01:46\u003c01:35, 28.3MB/s]\u001b[A\n"," 62%|██████▏   | 4.12G/6.63G [01:47\u003c01:43, 26.1MB/s]\u001b[A\n"," 62%|██████▏   | 4.13G/6.63G [01:47\u003c01:20, 33.3MB/s]\u001b[A\n"," 62%|██████▏   | 4.13G/6.63G [01:47\u003c01:16, 35.0MB/s]\u001b[A\n"," 62%|██████▏   | 4.13G/6.63G [01:47\u003c01:31, 29.4MB/s]\u001b[A\n"," 62%|██████▏   | 4.14G/6.63G [01:47\u003c01:09, 38.6MB/s]\u001b[A\n"," 63%|██████▎   | 4.15G/6.63G [01:47\u003c01:02, 42.3MB/s]\u001b[A\n"," 63%|██████▎   | 4.15G/6.63G [01:48\u003c01:17, 34.3MB/s]\u001b[A\n"," 63%|██████▎   | 4.16G/6.63G [01:48\u003c01:16, 34.6MB/s]\u001b[A\n"," 63%|██████▎   | 4.16G/6.63G [01:48\u003c01:10, 37.8MB/s]\u001b[A\n"," 63%|██████▎   | 4.17G/6.63G [01:48\u003c01:03, 41.7MB/s]\u001b[A\n"," 63%|██████▎   | 4.17G/6.63G [01:48\u003c01:03, 41.6MB/s]\u001b[A\n"," 63%|██████▎   | 4.18G/6.63G [01:48\u003c01:06, 39.3MB/s]\u001b[A\n"," 63%|██████▎   | 4.18G/6.63G [01:48\u003c01:00, 43.8MB/s]\u001b[A\n"," 63%|██████▎   | 4.19G/6.63G [01:48\u003c00:55, 47.4MB/s]\u001b[A\n"," 63%|██████▎   | 4.19G/6.63G [01:49\u003c01:14, 35.2MB/s]\u001b[A\n"," 63%|██████▎   | 4.20G/6.63G [01:49\u003c01:13, 35.7MB/s]\u001b[A\n"," 63%|██████▎   | 4.20G/6.63G [01:49\u003c00:59, 44.0MB/s]\u001b[A\n"," 63%|██████▎   | 4.21G/6.63G [01:49\u003c01:00, 43.0MB/s]\u001b[A\n"," 64%|██████▎   | 4.21G/6.63G [01:49\u003c01:01, 42.2MB/s]\u001b[A\n"," 64%|██████▎   | 4.22G/6.63G [01:49\u003c01:03, 40.7MB/s]\u001b[A\n"," 64%|██████▎   | 4.22G/6.63G [01:49\u003c00:52, 49.1MB/s]\u001b[A\n"," 64%|██████▍   | 4.23G/6.63G [01:49\u003c00:53, 47.8MB/s]\u001b[A\n"," 64%|██████▍   | 4.23G/6.63G [01:50\u003c00:55, 46.2MB/s]\u001b[A\n"," 64%|██████▍   | 4.24G/6.63G [01:50\u003c00:58, 44.0MB/s]\u001b[A\n"," 64%|██████▍   | 4.24G/6.63G [01:50\u003c00:53, 47.6MB/s]\u001b[A\n"," 64%|██████▍   | 4.25G/6.63G [01:50\u003c00:55, 46.2MB/s]\u001b[A\n"," 64%|██████▍   | 4.25G/6.63G [01:50\u003c00:57, 44.3MB/s]\u001b[A\n"," 64%|██████▍   | 4.26G/6.63G [01:50\u003c00:59, 42.8MB/s]\u001b[A\n"," 64%|██████▍   | 4.26G/6.63G [01:50\u003c00:57, 44.4MB/s]\u001b[A\n"," 64%|██████▍   | 4.27G/6.63G [01:50\u003c00:54, 46.4MB/s]\u001b[A\n"," 64%|██████▍   | 4.27G/6.63G [01:50\u003c00:56, 44.6MB/s]\u001b[A\n"," 64%|██████▍   | 4.27G/6.63G [01:51\u003c00:58, 43.2MB/s]\u001b[A\n"," 65%|██████▍   | 4.28G/6.63G [01:51\u003c00:57, 43.7MB/s]\u001b[A\n"," 65%|██████▍   | 4.28G/6.63G [01:51\u003c01:02, 40.3MB/s]\u001b[A\n"," 65%|██████▍   | 4.29G/6.63G [01:51\u003c01:04, 39.1MB/s]\u001b[A\n"," 65%|██████▍   | 4.29G/6.63G [01:51\u003c00:54, 45.8MB/s]\u001b[A\n"," 65%|██████▍   | 4.30G/6.63G [01:51\u003c00:56, 44.3MB/s]\u001b[A\n"," 65%|██████▍   | 4.30G/6.63G [01:51\u003c01:01, 40.8MB/s]\u001b[A\n"," 65%|██████▍   | 4.31G/6.63G [01:51\u003c00:59, 42.3MB/s]\u001b[A\n"," 65%|██████▌   | 4.31G/6.63G [01:51\u003c00:50, 49.1MB/s]\u001b[A\n"," 65%|██████▌   | 4.32G/6.63G [01:52\u003c00:55, 44.9MB/s]\u001b[A\n"," 65%|██████▌   | 4.32G/6.63G [01:52\u003c01:11, 34.6MB/s]\u001b[A\n"," 65%|██████▌   | 4.33G/6.63G [01:52\u003c01:05, 37.6MB/s]\u001b[A\n"," 65%|██████▌   | 4.33G/6.63G [01:52\u003c01:04, 38.0MB/s]\u001b[A\n"," 65%|██████▌   | 4.34G/6.63G [01:52\u003c00:58, 41.8MB/s]\u001b[A\n"," 65%|██████▌   | 4.34G/6.63G [01:52\u003c01:00, 40.8MB/s]\u001b[A\n"," 66%|██████▌   | 4.34G/6.63G [01:52\u003c00:55, 44.5MB/s]\u001b[A\n"," 66%|██████▌   | 4.35G/6.63G [01:52\u003c00:57, 42.8MB/s]\u001b[A\n"," 66%|██████▌   | 4.36G/6.63G [01:53\u003c00:52, 46.9MB/s]\u001b[A\n"," 66%|██████▌   | 4.36G/6.63G [01:53\u003c00:53, 45.4MB/s]\u001b[A\n"," 66%|██████▌   | 4.36G/6.63G [01:53\u003c00:53, 45.8MB/s]\u001b[A\n"," 66%|██████▌   | 4.37G/6.63G [01:53\u003c00:54, 44.3MB/s]\u001b[A\n"," 66%|██████▌   | 4.37G/6.63G [01:53\u003c01:17, 31.5MB/s]\u001b[A\n"," 66%|██████▌   | 4.38G/6.63G [01:53\u003c01:15, 31.9MB/s]\u001b[A\n"," 66%|██████▌   | 4.38G/6.63G [01:53\u003c01:04, 37.3MB/s]\u001b[A\n"," 66%|██████▌   | 4.39G/6.63G [01:54\u003c00:57, 41.9MB/s]\u001b[A\n"," 66%|██████▋   | 4.39G/6.63G [01:54\u003c01:01, 39.2MB/s]\u001b[A\n"," 66%|██████▋   | 4.40G/6.63G [01:54\u003c00:58, 40.9MB/s]\u001b[A\n"," 66%|██████▋   | 4.40G/6.63G [01:54\u003c00:55, 43.5MB/s]\u001b[A\n"," 66%|██████▋   | 4.41G/6.63G [01:54\u003c00:55, 42.8MB/s]\u001b[A\n"," 67%|██████▋   | 4.41G/6.63G [01:54\u003c01:01, 39.0MB/s]\u001b[A\n"," 67%|██████▋   | 4.42G/6.63G [01:54\u003c00:54, 43.3MB/s]\u001b[A\n"," 67%|██████▋   | 4.42G/6.63G [01:54\u003c00:54, 43.3MB/s]\u001b[A\n"," 67%|██████▋   | 4.42G/6.63G [01:54\u003c00:56, 42.0MB/s]\u001b[A\n"," 67%|██████▋   | 4.43G/6.63G [01:55\u003c01:01, 38.5MB/s]\u001b[A\n"," 67%|██████▋   | 4.43G/6.63G [01:55\u003c00:56, 41.8MB/s]\u001b[A\n"," 67%|██████▋   | 4.44G/6.63G [01:55\u003c00:58, 40.3MB/s]\u001b[A\n"," 67%|██████▋   | 4.44G/6.63G [01:55\u003c01:01, 38.3MB/s]\u001b[A\n"," 67%|██████▋   | 4.45G/6.63G [01:55\u003c00:52, 44.4MB/s]\u001b[A\n"," 67%|██████▋   | 4.46G/6.63G [01:55\u003c00:48, 47.9MB/s]\u001b[A\n"," 67%|██████▋   | 4.46G/6.63G [01:55\u003c00:52, 44.3MB/s]\u001b[A\n"," 67%|██████▋   | 4.47G/6.63G [01:55\u003c00:53, 43.3MB/s]\u001b[A\n"," 67%|██████▋   | 4.47G/6.63G [01:56\u003c00:53, 43.4MB/s]\u001b[A\n"," 67%|██████▋   | 4.47G/6.63G [01:56\u003c00:52, 44.5MB/s]\u001b[A\n"," 68%|██████▊   | 4.48G/6.63G [01:56\u003c00:53, 43.1MB/s]\u001b[A\n"," 68%|██████▊   | 4.48G/6.63G [01:56\u003c00:56, 41.2MB/s]\u001b[A\n"," 68%|██████▊   | 4.49G/6.63G [01:56\u003c00:51, 44.7MB/s]\u001b[A\n"," 68%|██████▊   | 4.49G/6.63G [01:56\u003c00:48, 47.5MB/s]\u001b[A\n"," 68%|██████▊   | 4.50G/6.63G [01:56\u003c00:50, 45.0MB/s]\u001b[A\n"," 68%|██████▊   | 4.50G/6.63G [01:56\u003c00:52, 43.7MB/s]\u001b[A\n"," 68%|██████▊   | 4.51G/6.63G [01:57\u003c00:50, 45.1MB/s]\u001b[A\n"," 68%|██████▊   | 4.51G/6.63G [01:57\u003c00:47, 48.2MB/s]\u001b[A\n"," 68%|██████▊   | 4.52G/6.63G [01:57\u003c00:49, 45.6MB/s]\u001b[A\n"," 68%|██████▊   | 4.52G/6.63G [01:57\u003c00:51, 44.3MB/s]\u001b[A\n"," 68%|██████▊   | 4.53G/6.63G [01:57\u003c00:50, 45.0MB/s]\u001b[A\n"," 68%|██████▊   | 4.53G/6.63G [01:57\u003c00:47, 47.6MB/s]\u001b[A\n"," 68%|██████▊   | 4.54G/6.63G [01:58\u003c01:51, 20.2MB/s]\u001b[A\n"," 68%|██████▊   | 4.54G/6.63G [01:58\u003c01:41, 22.2MB/s]\u001b[A\n"," 69%|██████▊   | 4.55G/6.63G [01:58\u003c01:10, 31.8MB/s]\u001b[A\n"," 69%|██████▊   | 4.56G/6.63G [01:58\u003c01:10, 31.7MB/s]\u001b[A\n"," 69%|██████▉   | 4.56G/6.63G [01:58\u003c01:09, 32.1MB/s]\u001b[A\n"," 69%|██████▉   | 4.56G/6.63G [01:58\u003c01:02, 35.3MB/s]\u001b[A\n"," 69%|██████▉   | 4.57G/6.63G [01:58\u003c00:54, 40.5MB/s]\u001b[A\n"," 69%|██████▉   | 4.57G/6.63G [01:59\u003c00:53, 40.9MB/s]\u001b[A\n"," 69%|██████▉   | 4.58G/6.63G [01:59\u003c00:54, 40.7MB/s]\u001b[A\n"," 69%|██████▉   | 4.58G/6.63G [01:59\u003c00:51, 42.6MB/s]\u001b[A\n"," 69%|██████▉   | 4.59G/6.63G [01:59\u003c00:47, 46.3MB/s]\u001b[A\n"," 69%|██████▉   | 4.59G/6.63G [01:59\u003c00:49, 44.3MB/s]\u001b[A\n"," 69%|██████▉   | 4.60G/6.63G [01:59\u003c00:51, 42.5MB/s]\u001b[A\n"," 69%|██████▉   | 4.60G/6.63G [01:59\u003c00:48, 44.5MB/s]\u001b[A\n"," 70%|██████▉   | 4.61G/6.63G [01:59\u003c00:45, 47.3MB/s]\u001b[A\n"," 70%|██████▉   | 4.61G/6.63G [02:00\u003c00:45, 47.1MB/s]\u001b[A\n"," 70%|██████▉   | 4.62G/6.63G [02:00\u003c00:49, 43.4MB/s]\u001b[A\n"," 70%|██████▉   | 4.62G/6.63G [02:00\u003c00:45, 47.3MB/s]\u001b[A\n"," 70%|██████▉   | 4.63G/6.63G [02:00\u003c00:45, 47.5MB/s]\u001b[A\n"," 70%|██████▉   | 4.63G/6.63G [02:00\u003c00:46, 46.3MB/s]\u001b[A\n"," 70%|██████▉   | 4.64G/6.63G [02:00\u003c00:48, 43.7MB/s]\u001b[A\n"," 70%|███████   | 4.64G/6.63G [02:00\u003c00:48, 44.4MB/s]\u001b[A\n"," 70%|███████   | 4.65G/6.63G [02:00\u003c00:44, 47.6MB/s]\u001b[A\n"," 70%|███████   | 4.65G/6.63G [02:00\u003c00:46, 45.6MB/s]\u001b[A\n"," 70%|███████   | 4.66G/6.63G [02:01\u003c00:47, 44.8MB/s]\u001b[A\n"," 70%|███████   | 4.66G/6.63G [02:01\u003c01:41, 20.8MB/s]\u001b[A\n"," 70%|███████   | 4.67G/6.63G [02:01\u003c00:57, 36.4MB/s]\u001b[A\n"," 71%|███████   | 4.68G/6.63G [02:01\u003c00:58, 35.5MB/s]\u001b[A\n"," 71%|███████   | 4.68G/6.63G [02:02\u003c01:05, 31.9MB/s]\u001b[A\n"," 71%|███████   | 4.69G/6.63G [02:02\u003c00:51, 40.1MB/s]\u001b[A\n"," 71%|███████   | 4.70G/6.63G [02:02\u003c00:50, 41.0MB/s]\u001b[A\n"," 71%|███████   | 4.70G/6.63G [02:02\u003c00:50, 41.4MB/s]\u001b[A\n"," 71%|███████   | 4.71G/6.63G [02:02\u003c00:51, 40.4MB/s]\u001b[A\n"," 71%|███████   | 4.71G/6.63G [02:02\u003c00:46, 43.9MB/s]\u001b[A\n"," 71%|███████   | 4.72G/6.63G [02:02\u003c00:46, 44.0MB/s]\u001b[A\n"," 71%|███████   | 4.72G/6.63G [02:02\u003c00:47, 43.0MB/s]\u001b[A\n"," 71%|███████▏  | 4.72G/6.63G [02:03\u003c00:49, 41.3MB/s]\u001b[A\n"," 71%|███████▏  | 4.73G/6.63G [02:03\u003c00:45, 45.0MB/s]\u001b[A\n"," 71%|███████▏  | 4.74G/6.63G [02:03\u003c00:42, 48.2MB/s]\u001b[A\n"," 72%|███████▏  | 4.74G/6.63G [02:03\u003c00:43, 46.2MB/s]\u001b[A\n"," 72%|███████▏  | 4.75G/6.63G [02:03\u003c00:45, 44.6MB/s]\u001b[A\n"," 72%|███████▏  | 4.75G/6.63G [02:03\u003c00:45, 44.5MB/s]\u001b[A\n"," 72%|███████▏  | 4.76G/6.63G [02:03\u003c00:41, 48.1MB/s]\u001b[A\n"," 72%|███████▏  | 4.76G/6.63G [02:03\u003c00:43, 45.9MB/s]\u001b[A\n"," 72%|███████▏  | 4.76G/6.63G [02:03\u003c00:45, 44.3MB/s]\u001b[A\n"," 72%|███████▏  | 4.77G/6.63G [02:04\u003c00:44, 45.4MB/s]\u001b[A\n"," 72%|███████▏  | 4.78G/6.63G [02:04\u003c00:41, 48.2MB/s]\u001b[A\n"," 72%|███████▏  | 4.78G/6.63G [02:04\u003c00:43, 46.0MB/s]\u001b[A\n"," 72%|███████▏  | 4.78G/6.63G [02:04\u003c00:44, 44.4MB/s]\u001b[A\n"," 72%|███████▏  | 4.79G/6.63G [02:04\u003c00:46, 42.3MB/s]\u001b[A\n"," 72%|███████▏  | 4.80G/6.63G [02:04\u003c00:39, 50.4MB/s]\u001b[A\n"," 72%|███████▏  | 4.80G/6.63G [02:04\u003c00:40, 48.4MB/s]\u001b[A\n"," 72%|███████▏  | 4.80G/6.63G [02:04\u003c00:42, 46.5MB/s]\u001b[A\n"," 73%|███████▎  | 4.81G/6.63G [02:04\u003c00:45, 43.1MB/s]\u001b[A\n"," 73%|███████▎  | 4.82G/6.63G [02:05\u003c00:38, 50.3MB/s]\u001b[A\n"," 73%|███████▎  | 4.82G/6.63G [02:05\u003c00:39, 49.0MB/s]\u001b[A\n"," 73%|███████▎  | 4.83G/6.63G [02:05\u003c00:41, 46.9MB/s]\u001b[A\n"," 73%|███████▎  | 4.83G/6.63G [02:05\u003c00:43, 44.2MB/s]\u001b[A\n"," 73%|███████▎  | 4.83G/6.63G [02:05\u003c00:43, 44.2MB/s]\u001b[A\n"," 73%|███████▎  | 4.84G/6.63G [02:05\u003c00:41, 46.1MB/s]\u001b[A\n"," 73%|███████▎  | 4.84G/6.63G [02:05\u003c00:43, 44.1MB/s]\u001b[A\n"," 73%|███████▎  | 4.85G/6.63G [02:05\u003c00:45, 42.4MB/s]\u001b[A\n"," 73%|███████▎  | 4.85G/6.63G [02:05\u003c00:45, 42.2MB/s]\u001b[A\n"," 73%|███████▎  | 4.86G/6.63G [02:06\u003c00:41, 46.2MB/s]\u001b[A\n"," 73%|███████▎  | 4.86G/6.63G [02:06\u003c00:42, 44.2MB/s]\u001b[A\n"," 73%|███████▎  | 4.87G/6.63G [02:06\u003c00:43, 43.3MB/s]\u001b[A\n"," 73%|███████▎  | 4.87G/6.63G [02:06\u003c00:44, 42.1MB/s]\u001b[A\n"," 74%|███████▎  | 4.87G/6.63G [02:06\u003c00:49, 37.8MB/s]\u001b[A\n"," 74%|███████▎  | 4.88G/6.63G [02:06\u003c00:46, 40.0MB/s]\u001b[A\n"," 74%|███████▎  | 4.89G/6.63G [02:06\u003c00:47, 39.5MB/s]\u001b[A\n"," 74%|███████▍  | 4.89G/6.63G [02:07\u003c00:46, 39.8MB/s]\u001b[A\n"," 74%|███████▍  | 4.90G/6.63G [02:07\u003c00:42, 43.4MB/s]\u001b[A\n"," 74%|███████▍  | 4.90G/6.63G [02:07\u003c00:40, 45.8MB/s]\u001b[A\n"," 74%|███████▍  | 4.91G/6.63G [02:07\u003c00:42, 43.8MB/s]\u001b[A\n"," 74%|███████▍  | 4.91G/6.63G [02:07\u003c00:44, 41.4MB/s]\u001b[A\n"," 74%|███████▍  | 4.92G/6.63G [02:07\u003c00:39, 47.2MB/s]\u001b[A\n"," 74%|███████▍  | 4.92G/6.63G [02:07\u003c00:40, 44.9MB/s]\u001b[A\n"," 74%|███████▍  | 4.93G/6.63G [02:07\u003c00:39, 45.8MB/s]\u001b[A\n"," 74%|███████▍  | 4.93G/6.63G [02:07\u003c00:41, 44.2MB/s]\u001b[A\n"," 74%|███████▍  | 4.93G/6.63G [02:08\u003c00:39, 45.7MB/s]\u001b[A\n"," 75%|███████▍  | 4.94G/6.63G [02:08\u003c00:41, 44.0MB/s]\u001b[A\n"," 75%|███████▍  | 4.94G/6.63G [02:08\u003c00:38, 46.5MB/s]\u001b[A\n"," 75%|███████▍  | 4.95G/6.63G [02:08\u003c00:40, 44.2MB/s]\u001b[A\n"," 75%|███████▍  | 4.95G/6.63G [02:08\u003c00:37, 48.1MB/s]\u001b[A\n"," 75%|███████▍  | 4.96G/6.63G [02:08\u003c00:39, 45.9MB/s]\u001b[A\n"," 75%|███████▍  | 4.96G/6.63G [02:08\u003c00:37, 47.5MB/s]\u001b[A\n"," 75%|███████▍  | 4.97G/6.63G [02:08\u003c00:40, 44.0MB/s]\u001b[A\n"," 75%|███████▌  | 4.97G/6.63G [02:08\u003c00:41, 42.7MB/s]\u001b[A\n"," 75%|███████▌  | 4.98G/6.63G [02:09\u003c00:42, 41.8MB/s]\u001b[A\n"," 75%|███████▌  | 4.98G/6.63G [02:09\u003c00:41, 42.9MB/s]\u001b[A\n"," 75%|███████▌  | 4.99G/6.63G [02:09\u003c00:42, 41.6MB/s]\u001b[A\n"," 75%|███████▌  | 4.99G/6.63G [02:09\u003c00:38, 45.5MB/s]\u001b[A\n"," 75%|███████▌  | 5.00G/6.63G [02:09\u003c00:39, 44.0MB/s]\u001b[A\n"," 75%|███████▌  | 5.00G/6.63G [02:09\u003c00:37, 46.1MB/s]\u001b[A\n"," 76%|███████▌  | 5.01G/6.63G [02:09\u003c00:38, 44.8MB/s]\u001b[A\n"," 76%|███████▌  | 5.01G/6.63G [02:09\u003c00:37, 46.4MB/s]\u001b[A\n"," 76%|███████▌  | 5.02G/6.63G [02:09\u003c00:38, 45.0MB/s]\u001b[A\n"," 76%|███████▌  | 5.02G/6.63G [02:10\u003c00:36, 47.1MB/s]\u001b[A\n"," 76%|███████▌  | 5.03G/6.63G [02:10\u003c00:38, 44.4MB/s]\u001b[A\n"," 76%|███████▌  | 5.03G/6.63G [02:10\u003c00:37, 46.4MB/s]\u001b[A\n"," 76%|███████▌  | 5.04G/6.63G [02:10\u003c01:31, 18.7MB/s]\u001b[A\n"," 76%|███████▌  | 5.04G/6.63G [02:11\u003c01:11, 23.8MB/s]\u001b[A\n"," 76%|███████▌  | 5.05G/6.63G [02:11\u003c00:55, 30.5MB/s]\u001b[A\n"," 76%|███████▌  | 5.05G/6.63G [02:11\u003c00:54, 31.2MB/s]\u001b[A\n"," 76%|███████▋  | 5.06G/6.63G [02:11\u003c00:52, 32.1MB/s]\u001b[A\n"," 76%|███████▋  | 5.06G/6.63G [02:11\u003c00:42, 39.2MB/s]\u001b[A\n"," 76%|███████▋  | 5.07G/6.63G [02:11\u003c00:42, 39.2MB/s]\u001b[A\n"," 77%|███████▋  | 5.07G/6.63G [02:11\u003c00:42, 39.3MB/s]\u001b[A\n"," 77%|███████▋  | 5.08G/6.63G [02:11\u003c00:42, 39.0MB/s]\u001b[A\n"," 77%|███████▋  | 5.08G/6.63G [02:12\u003c00:35, 46.8MB/s]\u001b[A\n"," 77%|███████▋  | 5.09G/6.63G [02:12\u003c00:35, 46.0MB/s]\u001b[A\n"," 77%|███████▋  | 5.09G/6.63G [02:12\u003c00:37, 43.6MB/s]\u001b[A\n"," 77%|███████▋  | 5.10G/6.63G [02:12\u003c00:38, 43.2MB/s]\u001b[A\n"," 77%|███████▋  | 5.10G/6.63G [02:12\u003c00:37, 44.2MB/s]\u001b[A\n"," 77%|███████▋  | 5.11G/6.63G [02:12\u003c00:34, 47.3MB/s]\u001b[A\n"," 77%|███████▋  | 5.11G/6.63G [02:12\u003c00:37, 43.5MB/s]\u001b[A\n"," 77%|███████▋  | 5.12G/6.63G [02:12\u003c00:38, 42.2MB/s]\u001b[A\n"," 77%|███████▋  | 5.12G/6.63G [02:13\u003c00:37, 43.6MB/s]\u001b[A\n"," 77%|███████▋  | 5.12G/6.63G [02:13\u003c00:37, 43.3MB/s]\u001b[A\n"," 77%|███████▋  | 5.13G/6.63G [02:13\u003c00:38, 41.9MB/s]\u001b[A\n"," 77%|███████▋  | 5.13G/6.63G [02:13\u003c00:39, 40.4MB/s]\u001b[A\n"," 78%|███████▊  | 5.14G/6.63G [02:13\u003c00:36, 44.1MB/s]\u001b[A\n"," 78%|███████▊  | 5.14G/6.63G [02:13\u003c00:34, 46.9MB/s]\u001b[A\n"," 78%|███████▊  | 5.15G/6.63G [02:13\u003c00:34, 45.5MB/s]\u001b[A\n"," 78%|███████▊  | 5.15G/6.63G [02:13\u003c00:37, 42.2MB/s]\u001b[A\n"," 78%|███████▊  | 5.16G/6.63G [02:13\u003c00:37, 42.1MB/s]\u001b[A\n"," 78%|███████▊  | 5.16G/6.63G [02:14\u003c00:35, 44.5MB/s]\u001b[A\n"," 78%|███████▊  | 5.17G/6.63G [02:14\u003c00:36, 42.9MB/s]\u001b[A\n"," 78%|███████▊  | 5.17G/6.63G [02:14\u003c00:38, 41.2MB/s]\u001b[A\n"," 78%|███████▊  | 5.18G/6.63G [02:14\u003c00:35, 44.4MB/s]\u001b[A\n"," 78%|███████▊  | 5.18G/6.63G [02:14\u003c00:32, 47.8MB/s]\u001b[A\n"," 78%|███████▊  | 5.19G/6.63G [02:14\u003c00:36, 42.6MB/s]\u001b[A\n"," 78%|███████▊  | 5.19G/6.63G [02:14\u003c00:34, 45.2MB/s]\u001b[A\n"," 78%|███████▊  | 5.20G/6.63G [02:14\u003c00:34, 45.0MB/s]\u001b[A\n"," 78%|███████▊  | 5.20G/6.63G [02:14\u003c00:34, 44.1MB/s]\u001b[A\n"," 78%|███████▊  | 5.20G/6.63G [02:15\u003c00:37, 40.6MB/s]\u001b[A\n"," 79%|███████▊  | 5.21G/6.63G [02:15\u003c00:37, 40.7MB/s]\u001b[A\n"," 79%|███████▊  | 5.21G/6.63G [02:15\u003c00:35, 43.4MB/s]\u001b[A\n"," 79%|███████▊  | 5.22G/6.63G [02:15\u003c00:37, 40.8MB/s]\u001b[A\n"," 79%|███████▊  | 5.22G/6.63G [02:15\u003c00:36, 40.9MB/s]\u001b[A\n"," 79%|███████▉  | 5.22G/6.63G [02:15\u003c00:37, 39.8MB/s]\u001b[A\n"," 79%|███████▉  | 5.23G/6.63G [02:15\u003c00:35, 41.8MB/s]\u001b[A\n"," 79%|███████▉  | 5.24G/6.63G [02:15\u003c00:38, 39.1MB/s]\u001b[A\n"," 79%|███████▉  | 5.24G/6.63G [02:16\u003c00:32, 46.3MB/s]\u001b[A\n"," 79%|███████▉  | 5.25G/6.63G [02:16\u003c01:08, 21.5MB/s]\u001b[A\n"," 79%|███████▉  | 5.26G/6.63G [02:16\u003c00:40, 36.2MB/s]\u001b[A\n"," 79%|███████▉  | 5.26G/6.63G [02:16\u003c00:41, 35.4MB/s]\u001b[A\n"," 79%|███████▉  | 5.27G/6.63G [02:17\u003c00:46, 31.6MB/s]\u001b[A\n"," 80%|███████▉  | 5.28G/6.63G [02:17\u003c00:37, 38.9MB/s]\u001b[A\n"," 80%|███████▉  | 5.28G/6.63G [02:17\u003c00:36, 39.9MB/s]\u001b[A\n"," 80%|███████▉  | 5.29G/6.63G [02:17\u003c00:36, 40.0MB/s]\u001b[A\n"," 80%|███████▉  | 5.29G/6.63G [02:17\u003c00:36, 39.5MB/s]\u001b[A\n"," 80%|███████▉  | 5.30G/6.63G [02:17\u003c00:33, 43.3MB/s]\u001b[A\n"," 80%|███████▉  | 5.30G/6.63G [02:17\u003c00:32, 43.9MB/s]\u001b[A\n"," 80%|████████  | 5.31G/6.63G [02:17\u003c00:33, 42.1MB/s]\u001b[A\n"," 80%|████████  | 5.31G/6.63G [02:18\u003c00:33, 42.2MB/s]\u001b[A\n"," 80%|████████  | 5.31G/6.63G [02:18\u003c00:33, 41.8MB/s]\u001b[A\n"," 80%|████████  | 5.32G/6.63G [02:18\u003c00:30, 46.0MB/s]\u001b[A\n"," 80%|████████  | 5.32G/6.63G [02:18\u003c00:31, 44.4MB/s]\u001b[A\n"," 80%|████████  | 5.33G/6.63G [02:18\u003c00:32, 42.8MB/s]\u001b[A\n"," 80%|████████  | 5.33G/6.63G [02:18\u003c00:30, 45.7MB/s]\u001b[A\n"," 81%|████████  | 5.34G/6.63G [02:18\u003c00:28, 48.6MB/s]\u001b[A\n"," 81%|████████  | 5.34G/6.63G [02:18\u003c00:29, 46.5MB/s]\u001b[A\n"," 81%|████████  | 5.35G/6.63G [02:18\u003c00:30, 44.4MB/s]\u001b[A\n"," 81%|████████  | 5.35G/6.63G [02:19\u003c00:31, 43.8MB/s]\u001b[A\n"," 81%|████████  | 5.36G/6.63G [02:19\u003c00:35, 38.7MB/s]\u001b[A\n"," 81%|████████  | 5.36G/6.63G [02:19\u003c00:31, 43.3MB/s]\u001b[A\n"," 81%|████████  | 5.37G/6.63G [02:19\u003c00:31, 43.2MB/s]\u001b[A\n"," 81%|████████  | 5.37G/6.63G [02:19\u003c00:32, 41.5MB/s]\u001b[A\n"," 81%|████████  | 5.37G/6.63G [02:19\u003c00:33, 40.5MB/s]\u001b[A\n"," 81%|████████  | 5.38G/6.63G [02:19\u003c00:33, 40.1MB/s]\u001b[A\n"," 81%|████████  | 5.38G/6.63G [02:19\u003c00:30, 44.6MB/s]\u001b[A\n"," 81%|████████▏ | 5.39G/6.63G [02:19\u003c00:31, 42.6MB/s]\u001b[A\n"," 81%|████████▏ | 5.39G/6.63G [02:20\u003c00:41, 31.8MB/s]\u001b[A\n"," 81%|████████▏ | 5.40G/6.63G [02:20\u003c00:35, 36.8MB/s]\u001b[A\n"," 82%|████████▏ | 5.40G/6.63G [02:20\u003c00:31, 41.8MB/s]\u001b[A\n"," 82%|████████▏ | 5.41G/6.63G [02:20\u003c00:31, 41.3MB/s]\u001b[A\n"," 82%|████████▏ | 5.41G/6.63G [02:20\u003c00:33, 38.7MB/s]\u001b[A\n"," 82%|████████▏ | 5.42G/6.63G [02:20\u003c00:34, 37.9MB/s]\u001b[A\n"," 82%|████████▏ | 5.42G/6.63G [02:20\u003c00:27, 47.6MB/s]\u001b[A\n"," 82%|████████▏ | 5.43G/6.63G [02:21\u003c00:35, 36.4MB/s]\u001b[A\n"," 82%|████████▏ | 5.43G/6.63G [02:21\u003c00:35, 36.1MB/s]\u001b[A\n"," 82%|████████▏ | 5.44G/6.63G [02:21\u003c00:30, 41.3MB/s]\u001b[A\n"," 82%|████████▏ | 5.44G/6.63G [02:21\u003c00:28, 44.2MB/s]\u001b[A\n"," 82%|████████▏ | 5.45G/6.63G [02:21\u003c00:29, 43.4MB/s]\u001b[A\n"," 82%|████████▏ | 5.45G/6.63G [02:21\u003c00:31, 40.6MB/s]\u001b[A\n"," 82%|████████▏ | 5.46G/6.63G [02:21\u003c00:29, 42.5MB/s]\u001b[A\n"," 82%|████████▏ | 5.46G/6.63G [02:22\u003c00:30, 41.0MB/s]\u001b[A\n"," 82%|████████▏ | 5.47G/6.63G [02:22\u003c00:31, 39.2MB/s]\u001b[A\n"," 83%|████████▎ | 5.47G/6.63G [02:22\u003c00:30, 40.2MB/s]\u001b[A\n"," 83%|████████▎ | 5.48G/6.63G [02:22\u003c00:27, 44.2MB/s]\u001b[A\n"," 83%|████████▎ | 5.48G/6.63G [02:22\u003c00:28, 43.1MB/s]\u001b[A\n"," 83%|████████▎ | 5.49G/6.63G [02:22\u003c00:31, 39.4MB/s]\u001b[A\n"," 83%|████████▎ | 5.49G/6.63G [02:22\u003c00:31, 38.6MB/s]\u001b[A\n"," 83%|████████▎ | 5.50G/6.63G [02:22\u003c00:26, 45.8MB/s]\u001b[A\n"," 83%|████████▎ | 5.50G/6.63G [02:22\u003c00:27, 44.7MB/s]\u001b[A\n"," 83%|████████▎ | 5.51G/6.63G [02:23\u003c00:35, 33.6MB/s]\u001b[A\n"," 83%|████████▎ | 5.51G/6.63G [02:23\u003c00:31, 38.4MB/s]\u001b[A\n"," 83%|████████▎ | 5.52G/6.63G [02:23\u003c00:26, 44.3MB/s]\u001b[A\n"," 83%|████████▎ | 5.52G/6.63G [02:23\u003c00:28, 41.5MB/s]\u001b[A\n"," 83%|████████▎ | 5.53G/6.63G [02:23\u003c00:27, 43.8MB/s]\u001b[A\n"," 83%|████████▎ | 5.53G/6.63G [02:23\u003c00:25, 45.6MB/s]\u001b[A\n"," 84%|████████▎ | 5.54G/6.63G [02:23\u003c00:26, 44.2MB/s]\u001b[A\n"," 84%|████████▎ | 5.54G/6.63G [02:24\u003c01:02, 18.7MB/s]\u001b[A\n"," 84%|████████▎ | 5.54G/6.63G [02:24\u003c00:52, 22.2MB/s]\u001b[A\n"," 84%|████████▍ | 5.55G/6.63G [02:24\u003c00:39, 28.9MB/s]\u001b[A\n"," 84%|████████▍ | 5.56G/6.63G [02:24\u003c00:39, 29.3MB/s]\u001b[A\n"," 84%|████████▍ | 5.56G/6.63G [02:25\u003c00:39, 28.7MB/s]\u001b[A\n"," 84%|████████▍ | 5.57G/6.63G [02:25\u003c00:25, 44.7MB/s]\u001b[A\n"," 84%|████████▍ | 5.58G/6.63G [02:25\u003c00:31, 36.0MB/s]\u001b[A\n"," 84%|████████▍ | 5.58G/6.63G [02:25\u003c00:30, 37.2MB/s]\u001b[A\n"," 84%|████████▍ | 5.59G/6.63G [02:25\u003c00:25, 43.2MB/s]\u001b[A\n"," 84%|████████▍ | 5.59G/6.63G [02:25\u003c00:25, 43.6MB/s]\u001b[A\n"," 84%|████████▍ | 5.60G/6.63G [02:25\u003c00:26, 42.3MB/s]\u001b[A\n"," 84%|████████▍ | 5.60G/6.63G [02:25\u003c00:26, 42.4MB/s]\u001b[A\n"," 85%|████████▍ | 5.60G/6.63G [02:26\u003c00:25, 42.8MB/s]\u001b[A\n"," 85%|████████▍ | 5.61G/6.63G [02:26\u003c00:25, 42.8MB/s]\u001b[A\n"," 85%|████████▍ | 5.61G/6.63G [02:26\u003c00:28, 37.7MB/s]\u001b[A\n"," 85%|████████▍ | 5.62G/6.63G [02:26\u003c00:30, 36.1MB/s]\u001b[A\n"," 85%|████████▍ | 5.62G/6.63G [02:26\u003c00:28, 37.4MB/s]\u001b[A\n"," 85%|████████▍ | 5.63G/6.63G [02:26\u003c00:25, 42.0MB/s]\u001b[A\n"," 85%|████████▍ | 5.63G/6.63G [02:26\u003c00:26, 40.8MB/s]\u001b[A\n"," 85%|████████▍ | 5.63G/6.63G [02:26\u003c00:26, 39.7MB/s]\u001b[A\n"," 85%|████████▌ | 5.64G/6.63G [02:27\u003c00:23, 44.8MB/s]\u001b[A\n"," 85%|████████▌ | 5.65G/6.63G [02:27\u003c00:22, 47.3MB/s]\u001b[A\n"," 85%|████████▌ | 5.65G/6.63G [02:27\u003c00:23, 44.5MB/s]\u001b[A\n"," 85%|████████▌ | 5.65G/6.63G [02:27\u003c00:24, 42.9MB/s]\u001b[A\n"," 85%|████████▌ | 5.66G/6.63G [02:27\u003c00:22, 45.7MB/s]\u001b[A\n"," 85%|████████▌ | 5.67G/6.63G [02:27\u003c00:21, 48.4MB/s]\u001b[A\n"," 86%|████████▌ | 5.67G/6.63G [02:27\u003c00:22, 46.5MB/s]\u001b[A\n"," 86%|████████▌ | 5.67G/6.63G [02:27\u003c00:22, 45.4MB/s]\u001b[A\n"," 86%|████████▌ | 5.68G/6.63G [02:27\u003c00:23, 43.9MB/s]\u001b[A\n"," 86%|████████▌ | 5.69G/6.63G [02:28\u003c00:21, 47.4MB/s]\u001b[A\n"," 86%|████████▌ | 5.69G/6.63G [02:28\u003c00:22, 45.3MB/s]\u001b[A\n"," 86%|████████▌ | 5.69G/6.63G [02:28\u003c00:23, 43.4MB/s]\u001b[A\n"," 86%|████████▌ | 5.70G/6.63G [02:28\u003c00:21, 45.9MB/s]\u001b[A\n"," 86%|████████▌ | 5.70G/6.63G [02:28\u003c00:20, 47.9MB/s]\u001b[A\n"," 86%|████████▌ | 5.71G/6.63G [02:28\u003c00:21, 46.3MB/s]\u001b[A\n"," 86%|████████▌ | 5.71G/6.63G [02:28\u003c00:22, 43.8MB/s]\u001b[A\n"," 86%|████████▌ | 5.72G/6.63G [02:28\u003c00:22, 43.5MB/s]\u001b[A\n"," 86%|████████▋ | 5.72G/6.63G [02:28\u003c00:19, 49.5MB/s]\u001b[A\n"," 86%|████████▋ | 5.73G/6.63G [02:29\u003c00:20, 47.5MB/s]\u001b[A\n"," 86%|████████▋ | 5.73G/6.63G [02:29\u003c00:21, 45.3MB/s]\u001b[A\n"," 87%|████████▋ | 5.74G/6.63G [02:29\u003c00:21, 44.7MB/s]\u001b[A\n"," 87%|████████▋ | 5.74G/6.63G [02:29\u003c00:20, 47.3MB/s]\u001b[A\n"," 87%|████████▋ | 5.75G/6.63G [02:29\u003c00:21, 44.9MB/s]\u001b[A\n"," 87%|████████▋ | 5.75G/6.63G [02:29\u003c00:21, 43.3MB/s]\u001b[A\n"," 87%|████████▋ | 5.76G/6.63G [02:29\u003c00:20, 45.9MB/s]\u001b[A\n"," 87%|████████▋ | 5.76G/6.63G [02:29\u003c00:19, 48.3MB/s]\u001b[A\n"," 87%|████████▋ | 5.77G/6.63G [02:30\u003c00:19, 47.0MB/s]\u001b[A\n"," 87%|████████▋ | 5.77G/6.63G [02:30\u003c00:20, 44.7MB/s]\u001b[A\n"," 87%|████████▋ | 5.78G/6.63G [02:30\u003c00:20, 45.3MB/s]\u001b[A\n"," 87%|████████▋ | 5.78G/6.63G [02:30\u003c00:18, 48.0MB/s]\u001b[A\n"," 87%|████████▋ | 5.79G/6.63G [02:30\u003c00:19, 45.8MB/s]\u001b[A\n"," 87%|████████▋ | 5.79G/6.63G [02:30\u003c00:20, 43.3MB/s]\u001b[A\n"," 87%|████████▋ | 5.80G/6.63G [02:30\u003c00:20, 44.2MB/s]\u001b[A\n"," 88%|████████▊ | 5.80G/6.63G [02:30\u003c00:18, 48.1MB/s]\u001b[A\n"," 88%|████████▊ | 5.81G/6.63G [02:30\u003c00:18, 46.5MB/s]\u001b[A\n"," 88%|████████▊ | 5.81G/6.63G [02:31\u003c00:19, 44.3MB/s]\u001b[A\n"," 88%|████████▊ | 5.82G/6.63G [02:31\u003c00:19, 45.8MB/s]\u001b[A\n"," 88%|████████▊ | 5.82G/6.63G [02:31\u003c00:37, 23.2MB/s]\u001b[A\n"," 88%|████████▊ | 5.83G/6.63G [02:31\u003c00:22, 38.6MB/s]\u001b[A\n"," 88%|████████▊ | 5.84G/6.63G [02:31\u003c00:22, 37.4MB/s]\u001b[A\n"," 88%|████████▊ | 5.84G/6.63G [02:32\u003c00:22, 36.8MB/s]\u001b[A\n"," 88%|████████▊ | 5.85G/6.63G [02:32\u003c00:21, 38.4MB/s]\u001b[A\n"," 88%|████████▊ | 5.85G/6.63G [02:32\u003c00:20, 40.1MB/s]\u001b[A\n"," 88%|████████▊ | 5.86G/6.63G [02:32\u003c00:25, 32.5MB/s]\u001b[A\n"," 88%|████████▊ | 5.86G/6.63G [02:32\u003c00:22, 37.1MB/s]\u001b[A\n"," 88%|████████▊ | 5.87G/6.63G [02:32\u003c00:19, 41.1MB/s]\u001b[A\n"," 89%|████████▊ | 5.87G/6.63G [02:32\u003c00:19, 41.1MB/s]\u001b[A\n"," 89%|████████▊ | 5.87G/6.63G [02:32\u003c00:20, 40.1MB/s]\u001b[A\n"," 89%|████████▊ | 5.88G/6.63G [02:33\u003c00:20, 40.1MB/s]\u001b[A\n"," 89%|████████▊ | 5.88G/6.63G [02:33\u003c00:21, 36.8MB/s]\u001b[A\n"," 89%|████████▉ | 5.89G/6.63G [02:33\u003c00:18, 42.3MB/s]\u001b[A\n"," 89%|████████▉ | 5.89G/6.63G [02:33\u003c00:18, 42.4MB/s]\u001b[A\n"," 89%|████████▉ | 5.90G/6.63G [02:33\u003c00:19, 40.4MB/s]\u001b[A\n"," 89%|████████▉ | 5.90G/6.63G [02:33\u003c00:19, 39.7MB/s]\u001b[A\n"," 89%|████████▉ | 5.90G/6.63G [02:33\u003c00:19, 39.0MB/s]\u001b[A\n"," 89%|████████▉ | 5.91G/6.63G [02:33\u003c00:15, 48.8MB/s]\u001b[A\n"," 89%|████████▉ | 5.92G/6.63G [02:33\u003c00:17, 44.2MB/s]\u001b[A\n"," 89%|████████▉ | 5.92G/6.63G [02:34\u003c00:17, 43.2MB/s]\u001b[A\n"," 89%|████████▉ | 5.92G/6.63G [02:34\u003c00:17, 42.1MB/s]\u001b[A\n"," 89%|████████▉ | 5.93G/6.63G [02:34\u003c00:17, 42.2MB/s]\u001b[A\n"," 89%|████████▉ | 5.93G/6.63G [02:34\u003c00:18, 40.8MB/s]\u001b[A\n"," 90%|████████▉ | 5.94G/6.63G [02:34\u003c00:19, 39.0MB/s]\u001b[A\n"," 90%|████████▉ | 5.94G/6.63G [02:34\u003c00:19, 37.9MB/s]\u001b[A\n"," 90%|████████▉ | 5.95G/6.63G [02:34\u003c00:18, 40.7MB/s]\u001b[A\n"," 90%|████████▉ | 5.95G/6.63G [02:34\u003c00:18, 40.4MB/s]\u001b[A\n"," 90%|████████▉ | 5.95G/6.63G [02:35\u003c00:19, 37.1MB/s]\u001b[A\n"," 90%|████████▉ | 5.96G/6.63G [02:35\u003c00:19, 36.9MB/s]\u001b[A\n"," 90%|████████▉ | 5.96G/6.63G [02:35\u003c00:18, 38.8MB/s]\u001b[A\n"," 90%|████████▉ | 5.96G/6.63G [02:35\u003c00:18, 38.5MB/s]\u001b[A\n"," 90%|█████████ | 5.97G/6.63G [02:35\u003c00:18, 38.2MB/s]\u001b[A\n"," 90%|█████████ | 5.97G/6.63G [02:35\u003c00:18, 37.2MB/s]\u001b[A\n"," 90%|█████████ | 5.98G/6.63G [02:35\u003c00:16, 42.0MB/s]\u001b[A\n"," 90%|█████████ | 5.98G/6.63G [02:35\u003c00:14, 46.5MB/s]\u001b[A\n"," 90%|█████████ | 5.99G/6.63G [02:35\u003c00:15, 44.9MB/s]\u001b[A\n"," 90%|█████████ | 5.99G/6.63G [02:36\u003c00:15, 43.0MB/s]\u001b[A\n"," 90%|█████████ | 6.00G/6.63G [02:36\u003c00:14, 45.2MB/s]\u001b[A\n"," 91%|█████████ | 6.00G/6.63G [02:36\u003c00:16, 41.8MB/s]\u001b[A\n"," 91%|█████████ | 6.01G/6.63G [02:36\u003c00:15, 43.7MB/s]\u001b[A\n"," 91%|█████████ | 6.01G/6.63G [02:36\u003c00:18, 36.8MB/s]\u001b[A\n"," 91%|█████████ | 6.02G/6.63G [02:36\u003c00:17, 37.1MB/s]\u001b[A\n"," 91%|█████████ | 6.02G/6.63G [02:36\u003c00:17, 37.9MB/s]\u001b[A\n"," 91%|█████████ | 6.03G/6.63G [02:36\u003c00:13, 46.3MB/s]\u001b[A\n"," 91%|█████████ | 6.03G/6.63G [02:37\u003c00:14, 45.6MB/s]\u001b[A\n"," 91%|█████████ | 6.04G/6.63G [02:37\u003c00:34, 18.2MB/s]\u001b[A\n"," 91%|█████████ | 6.04G/6.63G [02:37\u003c00:29, 21.3MB/s]\u001b[A\n"," 91%|█████████▏| 6.05G/6.63G [02:38\u003c00:22, 28.0MB/s]\u001b[A\n"," 91%|█████████▏| 6.05G/6.63G [02:38\u003c00:21, 28.9MB/s]\u001b[A\n"," 91%|█████████▏| 6.06G/6.63G [02:38\u003c00:18, 32.5MB/s]\u001b[A\n"," 91%|█████████▏| 6.06G/6.63G [02:38\u003c00:16, 37.4MB/s]\u001b[A\n"," 92%|█████████▏| 6.07G/6.63G [02:38\u003c00:15, 38.1MB/s]\u001b[A\n"," 92%|█████████▏| 6.07G/6.63G [02:38\u003c00:15, 38.1MB/s]\u001b[A\n"," 92%|█████████▏| 6.08G/6.63G [02:38\u003c00:14, 41.9MB/s]\u001b[A\n"," 92%|█████████▏| 6.08G/6.63G [02:38\u003c00:12, 45.5MB/s]\u001b[A\n"," 92%|█████████▏| 6.09G/6.63G [02:38\u003c00:13, 43.6MB/s]\u001b[A\n"," 92%|█████████▏| 6.09G/6.63G [02:39\u003c00:13, 42.7MB/s]\u001b[A\n"," 92%|█████████▏| 6.10G/6.63G [02:39\u003c00:12, 44.0MB/s]\u001b[A\n"," 92%|█████████▏| 6.10G/6.63G [02:39\u003c00:11, 47.2MB/s]\u001b[A\n"," 92%|█████████▏| 6.11G/6.63G [02:39\u003c00:13, 42.3MB/s]\u001b[A\n"," 92%|█████████▏| 6.11G/6.63G [02:39\u003c00:14, 39.2MB/s]\u001b[A\n"," 92%|█████████▏| 6.12G/6.63G [02:39\u003c00:13, 41.8MB/s]\u001b[A\n"," 92%|█████████▏| 6.12G/6.63G [02:39\u003c00:12, 42.7MB/s]\u001b[A\n"," 92%|█████████▏| 6.13G/6.63G [02:39\u003c00:13, 41.3MB/s]\u001b[A\n"," 92%|█████████▏| 6.13G/6.63G [02:39\u003c00:13, 39.9MB/s]\u001b[A\n"," 93%|█████████▎| 6.13G/6.63G [02:40\u003c00:12, 43.6MB/s]\u001b[A\n"," 93%|█████████▎| 6.14G/6.63G [02:40\u003c00:11, 47.6MB/s]\u001b[A\n"," 93%|█████████▎| 6.15G/6.63G [02:40\u003c00:11, 45.6MB/s]\u001b[A\n"," 93%|█████████▎| 6.15G/6.63G [02:40\u003c00:11, 44.5MB/s]\u001b[A\n"," 93%|█████████▎| 6.15G/6.63G [02:40\u003c00:11, 45.2MB/s]\u001b[A\n"," 93%|█████████▎| 6.16G/6.63G [02:40\u003c00:10, 47.1MB/s]\u001b[A\n"," 93%|█████████▎| 6.16G/6.63G [02:40\u003c00:10, 45.6MB/s]\u001b[A\n"," 93%|█████████▎| 6.17G/6.63G [02:40\u003c00:11, 44.0MB/s]\u001b[A\n"," 93%|█████████▎| 6.17G/6.63G [02:41\u003c00:10, 45.8MB/s]\u001b[A\n"," 93%|█████████▎| 6.18G/6.63G [02:41\u003c00:09, 48.6MB/s]\u001b[A\n"," 93%|█████████▎| 6.18G/6.63G [02:41\u003c00:10, 45.9MB/s]\u001b[A\n"," 93%|█████████▎| 6.19G/6.63G [02:41\u003c00:10, 44.3MB/s]\u001b[A\n"," 93%|█████████▎| 6.19G/6.63G [02:41\u003c00:10, 46.0MB/s]\u001b[A\n"," 93%|█████████▎| 6.20G/6.63G [02:41\u003c00:10, 44.5MB/s]\u001b[A\n"," 94%|█████████▎| 6.20G/6.63G [02:41\u003c00:11, 41.2MB/s]\u001b[A\n"," 94%|█████████▎| 6.21G/6.63G [02:41\u003c00:10, 42.9MB/s]\u001b[A\n"," 94%|█████████▎| 6.21G/6.63G [02:41\u003c00:09, 48.7MB/s]\u001b[A\n"," 94%|█████████▍| 6.22G/6.63G [02:42\u003c00:10, 44.0MB/s]\u001b[A\n"," 94%|█████████▍| 6.22G/6.63G [02:42\u003c00:10, 42.8MB/s]\u001b[A\n"," 94%|█████████▍| 6.23G/6.63G [02:42\u003c00:09, 44.1MB/s]\u001b[A\n"," 94%|█████████▍| 6.23G/6.63G [02:42\u003c00:09, 44.3MB/s]\u001b[A\n"," 94%|█████████▍| 6.23G/6.63G [02:42\u003c00:09, 43.0MB/s]\u001b[A\n"," 94%|█████████▍| 6.24G/6.63G [02:42\u003c00:10, 39.3MB/s]\u001b[A\n"," 94%|█████████▍| 6.25G/6.63G [02:42\u003c00:08, 47.5MB/s]\u001b[A\n"," 94%|█████████▍| 6.25G/6.63G [02:42\u003c00:10, 39.8MB/s]\u001b[A\n"," 94%|█████████▍| 6.25G/6.63G [02:43\u003c00:10, 39.1MB/s]\u001b[A\n"," 94%|█████████▍| 6.26G/6.63G [02:43\u003c00:09, 42.4MB/s]\u001b[A\n"," 95%|█████████▍| 6.27G/6.63G [02:43\u003c00:08, 46.5MB/s]\u001b[A\n"," 95%|█████████▍| 6.27G/6.63G [02:43\u003c00:08, 44.5MB/s]\u001b[A\n"," 95%|█████████▍| 6.28G/6.63G [02:43\u003c00:08, 43.4MB/s]\u001b[A\n"," 95%|█████████▍| 6.28G/6.63G [02:43\u003c00:08, 46.6MB/s]\u001b[A\n"," 95%|█████████▍| 6.29G/6.63G [02:43\u003c00:07, 49.5MB/s]\u001b[A\n"," 95%|█████████▍| 6.29G/6.63G [02:43\u003c00:07, 48.0MB/s]\u001b[A\n"," 95%|█████████▍| 6.30G/6.63G [02:44\u003c00:08, 42.6MB/s]\u001b[A\n"," 95%|█████████▌| 6.30G/6.63G [02:44\u003c00:07, 49.9MB/s]\u001b[A\n"," 95%|█████████▌| 6.31G/6.63G [02:44\u003c00:08, 40.7MB/s]\u001b[A\n"," 95%|█████████▌| 6.31G/6.63G [02:44\u003c00:08, 39.7MB/s]\u001b[A\n"," 95%|█████████▌| 6.32G/6.63G [02:44\u003c00:07, 43.3MB/s]\u001b[A\n"," 95%|█████████▌| 6.32G/6.63G [02:44\u003c00:07, 46.5MB/s]\u001b[A\n"," 95%|█████████▌| 6.33G/6.63G [02:44\u003c00:07, 44.8MB/s]\u001b[A\n"," 96%|█████████▌| 6.33G/6.63G [02:44\u003c00:07, 43.4MB/s]\u001b[A\n"," 96%|█████████▌| 6.34G/6.63G [02:45\u003c00:06, 44.9MB/s]\u001b[A\n"," 96%|█████████▌| 6.34G/6.63G [02:45\u003c00:07, 43.7MB/s]\u001b[A\n"," 96%|█████████▌| 6.35G/6.63G [02:45\u003c00:07, 41.3MB/s]\u001b[A\n"," 96%|█████████▌| 6.35G/6.63G [02:45\u003c00:06, 45.4MB/s]\u001b[A\n"," 96%|█████████▌| 6.36G/6.63G [02:45\u003c00:06, 47.7MB/s]\u001b[A\n"," 96%|█████████▌| 6.36G/6.63G [02:45\u003c00:06, 45.6MB/s]\u001b[A\n"," 96%|█████████▌| 6.37G/6.63G [02:45\u003c00:06, 44.3MB/s]\u001b[A\n"," 96%|█████████▌| 6.37G/6.63G [02:45\u003c00:06, 44.2MB/s]\u001b[A\n"," 96%|█████████▌| 6.38G/6.63G [02:45\u003c00:05, 47.1MB/s]\u001b[A\n"," 96%|█████████▌| 6.38G/6.63G [02:46\u003c00:05, 45.3MB/s]\u001b[A\n"," 96%|█████████▋| 6.39G/6.63G [02:46\u003c00:06, 43.7MB/s]\u001b[A\n"," 96%|█████████▋| 6.39G/6.63G [02:46\u003c00:13, 19.8MB/s]\u001b[A\n"," 97%|█████████▋| 6.40G/6.63G [02:46\u003c00:07, 34.7MB/s]\u001b[A\n"," 97%|█████████▋| 6.41G/6.63G [02:46\u003c00:06, 35.0MB/s]\u001b[A\n"," 97%|█████████▋| 6.41G/6.63G [02:47\u003c00:06, 35.7MB/s]\u001b[A\n"," 97%|█████████▋| 6.42G/6.63G [02:47\u003c00:06, 38.2MB/s]\u001b[A\n"," 97%|█████████▋| 6.42G/6.63G [02:47\u003c00:05, 39.7MB/s]\u001b[A\n"," 97%|█████████▋| 6.42G/6.63G [02:47\u003c00:05, 40.0MB/s]\u001b[A\n"," 97%|█████████▋| 6.43G/6.63G [02:47\u003c00:05, 37.6MB/s]\u001b[A\n"," 97%|█████████▋| 6.43G/6.63G [02:47\u003c00:05, 41.2MB/s]\u001b[A\n"," 97%|█████████▋| 6.44G/6.63G [02:47\u003c00:05, 40.1MB/s]\u001b[A\n"," 97%|█████████▋| 6.45G/6.63G [02:48\u003c00:04, 41.0MB/s]\u001b[A\n"," 97%|█████████▋| 6.45G/6.63G [02:48\u003c00:04, 40.0MB/s]\u001b[A\n"," 97%|█████████▋| 6.46G/6.63G [02:48\u003c00:04, 43.6MB/s]\u001b[A\n"," 97%|█████████▋| 6.46G/6.63G [02:48\u003c00:03, 46.3MB/s]\u001b[A\n"," 98%|█████████▊| 6.47G/6.63G [02:48\u003c00:03, 44.7MB/s]\u001b[A\n"," 98%|█████████▊| 6.47G/6.63G [02:48\u003c00:03, 43.4MB/s]\u001b[A\n"," 98%|█████████▊| 6.48G/6.63G [02:48\u003c00:03, 45.9MB/s]\u001b[A\n"," 98%|█████████▊| 6.48G/6.63G [02:48\u003c00:03, 48.4MB/s]\u001b[A\n"," 98%|█████████▊| 6.49G/6.63G [02:48\u003c00:03, 44.8MB/s]\u001b[A\n"," 98%|█████████▊| 6.49G/6.63G [02:49\u003c00:03, 44.9MB/s]\u001b[A\n"," 98%|█████████▊| 6.50G/6.63G [02:49\u003c00:03, 44.8MB/s]\u001b[A\n"," 98%|█████████▊| 6.50G/6.63G [02:49\u003c00:03, 43.4MB/s]\u001b[A\n"," 98%|█████████▊| 6.50G/6.63G [02:49\u003c00:03, 39.4MB/s]\u001b[A\n"," 98%|█████████▊| 6.51G/6.63G [02:49\u003c00:03, 43.3MB/s]\u001b[A\n"," 98%|█████████▊| 6.51G/6.63G [02:49\u003c00:02, 43.5MB/s]\u001b[A\n"," 98%|█████████▊| 6.52G/6.63G [02:49\u003c00:02, 42.1MB/s]\u001b[A\n"," 98%|█████████▊| 6.52G/6.63G [02:49\u003c00:02, 39.2MB/s]\u001b[A\n"," 98%|█████████▊| 6.53G/6.63G [02:49\u003c00:02, 40.9MB/s]\u001b[A\n"," 99%|█████████▊| 6.53G/6.63G [02:50\u003c00:02, 48.2MB/s]\u001b[A\n"," 99%|█████████▊| 6.54G/6.63G [02:50\u003c00:02, 43.8MB/s]\u001b[A\n"," 99%|█████████▊| 6.54G/6.63G [02:50\u003c00:05, 17.7MB/s]\u001b[A\n"," 99%|█████████▊| 6.54G/6.63G [02:50\u003c00:04, 20.4MB/s]\u001b[A\n"," 99%|█████████▉| 6.55G/6.63G [02:51\u003c00:02, 28.2MB/s]\u001b[A\n"," 99%|█████████▉| 6.56G/6.63G [02:51\u003c00:02, 29.1MB/s]\u001b[A\n"," 99%|█████████▉| 6.56G/6.63G [02:51\u003c00:02, 32.2MB/s]\u001b[A\n"," 99%|█████████▉| 6.57G/6.63G [02:51\u003c00:01, 36.9MB/s]\u001b[A\n"," 99%|█████████▉| 6.57G/6.63G [02:51\u003c00:01, 37.7MB/s]\u001b[A\n"," 99%|█████████▉| 6.58G/6.63G [02:51\u003c00:01, 38.3MB/s]\u001b[A\n"," 99%|█████████▉| 6.58G/6.63G [02:51\u003c00:01, 42.4MB/s]\u001b[A\n"," 99%|█████████▉| 6.59G/6.63G [02:51\u003c00:00, 46.1MB/s]\u001b[A\n"," 99%|█████████▉| 6.59G/6.63G [02:52\u003c00:00, 44.4MB/s]\u001b[A\n"," 99%|█████████▉| 6.60G/6.63G [02:52\u003c00:00, 43.2MB/s]\u001b[A\n","100%|█████████▉| 6.60G/6.63G [02:52\u003c00:00, 45.9MB/s]\u001b[A\n","100%|█████████▉| 6.61G/6.63G [02:52\u003c00:00, 45.8MB/s]\u001b[A\n","100%|█████████▉| 6.61G/6.63G [02:52\u003c00:00, 43.7MB/s]\u001b[A\n","100%|█████████▉| 6.61G/6.63G [02:52\u003c00:00, 42.3MB/s]\u001b[A\n","100%|█████████▉| 6.62G/6.63G [02:52\u003c00:00, 45.1MB/s]\u001b[A\n","100%|██████████| 6.63G/6.63G [02:54\u003c00:00, 40.8MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: model.tar (7GB)\n","Starting upload for file Experiment.log\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 333/333 [00:02\u003c00:00, 115B/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: Experiment.log (333B)\n","Starting upload for file pretrain_tokenizer.tar\n"]},{"name":"stderr","output_type":"stream","text":["\n","  0%|          | 0.00/3.22M [00:00\u003c?, ?B/s]\u001b[A\n","100%|██████████| 3.22M/3.22M [00:03\u003c00:00, 1.12MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: pretrain_tokenizer.tar (3MB)\n","Starting upload for file model_config.pth\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 2.23k/2.23k [00:01\u003c00:00, 1.15kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: model_config.pth (2KB)\n","Starting upload for file exp030-fold-0.npy\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 30.4k/30.4k [00:01\u003c00:00, 16.7kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp030-fold-0.npy (30KB)\n","Starting upload for file exp030-fold-1.npy\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 27.8k/27.8k [00:01\u003c00:00, 15.5kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp030-fold-1.npy (28KB)\n","Starting upload for file exp030-fold-2.npy\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 28.8k/28.8k [00:01\u003c00:00, 16.7kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp030-fold-2.npy (29KB)\n","Starting upload for file exp030-fold-3.npy\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 28.8k/28.8k [00:02\u003c00:00, 11.2kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp030-fold-3.npy (29KB)\n","Starting upload for file exp030-fold-4.npy\n"]},{"name":"stderr","output_type":"stream","text":["\n","100%|██████████| 27.4k/27.4k [00:02\u003c00:00, 10.1kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp030-fold-4.npy (27KB)\n"]}],"source":["def main(Config):\n","    # setup\n","    Config = setup(Config)\n","    Config.logger = Logger(Config.exp_output_dir)\n","    \n","    # load dataset\n","    train_df = get_input_data(Config, input_type=\"train\")\n","    test_df = get_input_data(Config, input_type=\"test\")\n","    submission_df = pd.read_csv(Config.sample_submission)\n","\n","    # split\n","    train_df = get_split(Config, train_df)\n","\n","    # tokenizer\n","    tokenizer = get_tokenizer(Config)\n","\n","    if not Config.inference_only:\n","        # training\n","        oof_df = train_cv(\n","            cfg=Config,\n","            tokenizer=tokenizer,\n","            input_df=train_df,\n","        )\n","\n","    # predict\n","    raw_pred_df = predict_cv(\n","        cfg=Config,\n","        input_df=test_df,\n","        tokenizer=tokenizer,\n","    )\n","\n","    # upload output to kaggle dataset\n","    if Config.upload_from_colab:\n","        from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","        def dataset_create_new(dataset_name, upload_dir):\n","            dataset_metadata = {}\n","            dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n","            dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","            dataset_metadata['title'] = dataset_name\n","            with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n","                json.dump(dataset_metadata, f, indent=4)\n","            api = KaggleApi()\n","            api.authenticate()\n","            api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n","\n","        dataset_create_new(dataset_name=f\"{Config.competition_name}-exp{Config.exp_id}\", upload_dir=Config.exp_output_dir)\n","\n","    # make submission\n","    if not Config.on_colab:\n","        raw_pred_df[[\"id\", \"score\"]].to_csv(os.path.join(Config.submission, \"submission.csv\"), index=False)\n","\n","\n","if __name__ == \"__main__\":\n","    main(Config)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"aborted","timestamp":1654419447363,"user":{"displayName":"永友遥","userId":"11743586908271963047"},"user_tz":-540},"id":"r2gKJxtNhuzR"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNc1vzWyQQSNngn+lfIRl3j","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1Rw78Ms8jm_3FwAynU58EQc57406MT1dT","name":"exp030.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0326c0660b854a9ebff1dc21d070d5eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0842996910014e0f8f084a07932b7099":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"085d099066f447528880b331b70320b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"091e8973159c43ddb6d92d650e51af9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83295e2c5f3a432ea5483caa27f68bed","IPY_MODEL_ae25bb01a4a244898a05d4d5b01ea27f","IPY_MODEL_c79ae0c16adb4e0792e08597f92afd86"],"layout":"IPY_MODEL_36c4c426d60b4dbdabeacc4df1fc31d7"}},"1a8456642b3548c39dec3185e6ec2c2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"277067020c494245b352ffa3ea283b6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1c609beb2594ca78a7f2f747fb5aad1","placeholder":"​","style":"IPY_MODEL_0326c0660b854a9ebff1dc21d070d5eb","value":" 200/3833 [00:31\u0026lt;09:39,  6.27it/s, loss=0.659, v_num=s3bi, train/loss_step=0.626]"}},"36c4c426d60b4dbdabeacc4df1fc31d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"3884ae1dee0244659e896b2fc474e472":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c8c214846f3443bb03e7c60483db3ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"602fcf0f46284fb3a5012c08ad22535e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_085d099066f447528880b331b70320b3","placeholder":"​","style":"IPY_MODEL_0842996910014e0f8f084a07932b7099","value":"Epoch 0:   5%"}},"676b4c5d95204430a646e40c9c430281":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7248f094b4b6480eb85dbe2aca279602":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e4d644bbb114ec2bccf10b8451ab235":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"83295e2c5f3a432ea5483caa27f68bed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0de7207613840f781eb42e616dabc60","placeholder":"​","style":"IPY_MODEL_9ebf98e580b541cba658a65adc30e272","value":"Sanity Checking DataLoader 0: 100%"}},"86f572d6ff674d0f8355cddd6c5b78b4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_602fcf0f46284fb3a5012c08ad22535e","IPY_MODEL_9753e4bebd744d1a886b0a4fb07f8d4e","IPY_MODEL_277067020c494245b352ffa3ea283b6a"],"layout":"IPY_MODEL_7e4d644bbb114ec2bccf10b8451ab235"}},"93a7df67abad489d9234c6518854c0b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9753e4bebd744d1a886b0a4fb07f8d4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_676b4c5d95204430a646e40c9c430281","max":3833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7248f094b4b6480eb85dbe2aca279602","value":220}},"9ebf98e580b541cba658a65adc30e272":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1c609beb2594ca78a7f2f747fb5aad1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae25bb01a4a244898a05d4d5b01ea27f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_93a7df67abad489d9234c6518854c0b6","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c8c214846f3443bb03e7c60483db3ee","value":2}},"c0de7207613840f781eb42e616dabc60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c79ae0c16adb4e0792e08597f92afd86":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a8456642b3548c39dec3185e6ec2c2f","placeholder":"​","style":"IPY_MODEL_3884ae1dee0244659e896b2fc474e472","value":" 2/2 [00:00\u0026lt;00:00,  5.41it/s]"}}}}},"nbformat":4,"nbformat_minor":0}