{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VuO84cFldVgD"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1svhuuf6dbpY"},"outputs":[],"source":["!pip install -q pytorch-lightning wandb sentencepiece\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install -q --upgrade --force-reinstall --no-deps kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2RLP9JmdejV"},"outputs":[],"source":["!mkdir /root/.kaggle\n","!cp /content/drive/MyDrive/Colab/kaggle/kaggle.json /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4205,"status":"ok","timestamp":1653754522012,"user":{"displayName":"永友遥","userId":"11743586908271963047"},"user_tz":-540},"id":"OSFFpBcvdymq","outputId":"571cb63e-ccb5-425d-a404-b3d9c71a91c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["env: TOKENIZERS_PARALLELISM=true\n"]}],"source":["import os\n","import gc\n","import sys\n","import json\n","import itertools\n","from tqdm.auto import tqdm\n","import logging\n","import datetime\n","import ast\n","import numpy as np\n","import pandas as pd\n","import math\n","import re\n","from sklearn import model_selection as sms\n","from sklearn.preprocessing import LabelEncoder\n","import scipy as sp\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning import Trainer, seed_everything\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n","from pytorch_lightning.loggers import WandbLogger\n","\n","from transformers import AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n","from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n","\n","import wandb\n","\n","%env TOKENIZERS_PARALLELISM=true"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUf7MOUFem6z"},"outputs":[],"source":["class Config:\n","    # ==============================\n","    # Globals #\n","    # ==============================\n","    competition_name = \"us-patent-phrase-to-phrase-matching\"\n","    group = \"RoBERTa-large\"\n","    exp_id = \"026\"\n","    debug = False\n","    inference_only = False\n","    upload_from_colab = True\n","    colab_dir = \"/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching\"\n","    kaggle_json_path = \"/root/.kaggle/kaggle.json\"\n","    kaggle_dataset_path = None\n","    gpus = 1\n","    seed = 2434\n","    max_epochs = 5\n","    accumulate_grad_batches = 2\n","    precision = 32\n","    num_fold = 5\n","    train_fold = [0,1,2,3,4] # 実行するfold\n","    pretrained = True\n","    mlm_pretrained = False\n","    mlm_model_dir = \"/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/mlm_exp001/model\"\n","    gradient_clip_val = 1\n","    # ==============================\n","    # Dataloader #\n","    # ==============================\n","    train_batch_size = 8\n","    valid_batch_size = 32\n","    test_batch_size = 32\n","    num_workers = 8\n","    # ==============================\n","    # Split #\n","    # ==============================\n","    split_name = \"StratifiedGroupKFold\"\n","    split_params = {\n","        \"n_splits\": num_fold,\n","        \"shuffle\": True,\n","        \"random_state\": seed,\n","    }\n","    # ==============================\n","    # Model #\n","    # ==============================\n","    model_name = \"roberta-large\"\n","    max_length = 175\n","    hidden_size = 1024\n","    use_backbone_dropout = True\n","    dropout = 0.2\n","    initializer_range = 0.02\n","    # ==============================\n","    # Loss #\n","    # ==============================\n","    loss_name = \"MSELoss\"\n","    loss_params = {\n","        \"reduction\": \"mean\",\n","    }\n","    # ==============================\n","    # Optimizer #\n","    # ==============================\n","    optimizer_name = \"AdamW\"\n","    optimizer_params = {\n","        \"lr\": 2e-5,\n","        \"eps\": 1e-6,\n","        \"betas\": (0.9, 0.999)\n","    }\n","    encoder_lr = 2e-5\n","    decoder_lr = 2e-5\n","    weight_decay = 0.01\n","    # ==============================\n","    # Scheduler #\n","    # ==============================\n","    scheduler_name = \"cosine-warmup\"\n","    scheduler_warmup_ratio = 0.1\n","    scheduler_params = {}\n","    scheduler_interval = \"step\"\n","    scheduler_cycle = \"one-cycle\" # epoch or one-cycle\n","    # ==============================\n","    # Callbacks #\n","    # ==============================\n","    checkpoint_params = {\n","        \"monitor\": \"val/pearson_corr\",\n","        \"save_top_k\": 1,\n","        \"save_weights_only\": True,\n","        \"mode\": \"max\",\n","        \"verbose\": True,\n","    }\n","    early_stopping = False\n","    early_stopping_params = {\n","        \"monitor\": \"val/pearson_corr\",\n","        \"min_delta\": 0.0,\n","        \"patience\": 8,\n","        \"verbose\": False,\n","        \"mode\": \"min\",\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kq2PHavyzv4C"},"outputs":[],"source":["# ====================================\n","# Setup #\n","# ====================================\n","class Logger:\n","    \"\"\" ref) https://github.com/ghmagazine/kagglebook/blob/master/ch04-model-interface/code/util.py\"\"\"\n","    def __init__(self, path):\n","        self.general_logger = logging.getLogger(path)\n","        stream_handler = logging.StreamHandler()\n","        file_general_handler = logging.FileHandler(os.path.join(path, 'Experiment.log'))\n","        if len(self.general_logger.handlers) == 0:\n","            self.general_logger.addHandler(stream_handler)\n","            self.general_logger.addHandler(file_general_handler)\n","            self.general_logger.setLevel(logging.INFO)\n","\n","    def info(self, message):\n","        # display time\n","        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n","\n","    @staticmethod\n","    def now_string():\n","        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n","\n","\n","def setup(cfg):\n","    cfg.on_colab = \"google.colab\" in sys.modules\n","    if cfg.on_colab:\n","        # kaggle api\n","        f = open(Config.kaggle_json_path, 'r')\n","        json_data = json.load(f)\n","        os.environ[\"KAGGLE_USERNAME\"] = json_data[\"username\"]\n","        # set input/output dir\n","        cfg.input_dir = os.path.join(cfg.colab_dir, \"input\")\n","        cfg.train_csv = os.path.join(cfg.input_dir, \"train.csv\")\n","        cfg.test_csv = os.path.join(cfg.input_dir, \"test.csv\")\n","        cfg.cpc_data = os.path.join(cfg.input_dir, \"cpc-data\")\n","        cfg.cpc_codes_csv = os.path.join(cfg.input_dir, \"cpc-codes/cpc_codes.csv\")\n","        cfg.sample_submission = os.path.join(cfg.input_dir, \"sample_submission.csv\")\n","        cfg.output_dir = os.path.join(cfg.colab_dir, \"output\")\n","        cfg.exp_output_dir = os.path.join(cfg.output_dir, f\"exp{cfg.exp_id}\")\n","        cfg.model_dir = os.path.join(cfg.exp_output_dir, \"model\")\n","\n","        for d in [cfg.output_dir, cfg.exp_output_dir, cfg.model_dir]:\n","            os.makedirs(d, exist_ok=True)\n","            \n","        # wandb\n","        wandb.login()\n","    else:\n","        cfg.input_dir = f\"../input/{cfg.competition_name}\"\n","        cfg.train_csv = os.path.join(cfg.input_dir, \"train.csv\")\n","        cfg.test_csv = os.path.join(cfg.input_dir, \"test.csv\")\n","        cfg.cpc_data = \"../input/cpc-data\"\n","        cfg.cpc_codes_csv = \"../input/cpc-codes/cpc_codes.csv\"\n","        cfg.sample_submission = os.path.join(cfg.input_dir, \"sample_submission.csv\")\n","        cfg.submission = \"./\"\n","        cfg.exp_output_dir = f\"exp{cfg.exp_id}\"\n","        cfg.model_dir = os.path.join(cfg.exp_output_dir, \"model\")\n","\n","        if cfg.kaggle_dataset_path is not None:\n","            cfg.model_dir = os.path.join(cfg.kaggle_dataset_path, \"model\")\n","\n","        for d in [cfg.exp_output_dir, cfg.model_dir]:\n","            os.makedirs(d, exist_ok=True)\n","\n","    return cfg\n","\n","\n","# ====================================\n","# Preprocess #\n","# ====================================\n","def get_tokenizer(cfg):\n","    if cfg.kaggle_dataset_path is None:\n","        pretrained_dir = os.path.join(cfg.exp_output_dir, \"pretrain_tokenizer\")\n","    else:\n","        pretrained_dir = os.path.join(cfg.kaggle_dataset_path, \"pretrain_tokenizer\")\n","\n","    if not os.path.isdir(pretrained_dir):\n","        # except for (\"roberta\", \"deberta-v2\", \"deberta-v3\")\n","        if \"roberta\" not in cfg.model_name:\n","            tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n","        # roberta\n","        else:\n","            tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, trim_offsets=False)\n","\n","        tokenizer.save_pretrained(pretrained_dir)\n","\n","    else:\n","        # deberta-v2 or deberta-v3\n","        if (\"deberta-v2\" in cfg.model_name) or (\"deberta-v3\" in cfg.model_name):\n","            tokenizer = DebertaV2TokenizerFast.from_pretrained(pretrained_dir)\n","        # except for (\"roberta\", \"deberta-v2\", \"deberta-v3\")\n","        elif \"roberta\" not in cfg.model_name:\n","            tokenizer = AutoTokenizer.from_pretrained(pretrained_dir)\n","        # roberta\n","        else:\n","            tokenizer = AutoTokenizer.from_pretrained(pretrained_dir, trim_offsets=False)\n","\n","    return tokenizer\n","\n","\n","def get_backbone_config(cfg):\n","    filename = \"model_config\"\n","    filelist = get_filname_listdir(cfg.exp_output_dir if cfg.on_colab else cfg.kaggle_dataset_path)\n","\n","    if not filename in filelist:\n","        model_config = AutoConfig.from_pretrained(cfg.model_name, output_hidden_states=True)\n","        torch.save(model_config, os.path.join(cfg.exp_output_dir if cfg.on_colab else cfg.kaggle_dataset_path, f\"{filename}.pth\"))\n","    else:\n","        cfg_path = os.path.join(cfg.exp_output_dir if cfg.on_colab else cfg.kaggle_dataset_path, f\"{filename}.pth\")\n","        model_config = torch.load(cfg_path)\n","\n","    return model_config\n","\n","\n","def get_cpc_texts(cfg):\n","    contexts = []\n","    pattern = '[A-Z]\\d+'\n","    for file_name in os.listdir(os.path.join(cfg.cpc_data, \"CPCSchemeXML202105\")):\n","        result = re.findall(pattern, file_name)\n","        if result:\n","            contexts.append(result)\n","    contexts = sorted(set(sum(contexts, [])))\n","    results = {}\n","    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n","        with open(os.path.join(cfg.cpc_data, f\"CPCTitleList202202/cpc-section-{cpc}_20220201.txt\")) as f:\n","            s = f.read()\n","        pattern = f'{cpc}\\t\\t.+'\n","        result = re.findall(pattern, s)\n","        cpc_result = result[0].lstrip(pattern)\n","        for context in [c for c in contexts if c[0] == cpc]:\n","            pattern = f'{context}\\t\\t.+'\n","            result = re.findall(pattern, s)\n","            results[context] = cpc_result + \". \" + result[0].lstrip(pattern)\n","\n","    return results\n","\n","\n","def get_input_data(cfg, input_type: str = \"train\"):\n","    input_df = pd.read_csv(cfg.train_csv if input_type == \"train\" else cfg.test_csv, nrows=2000 if cfg.debug else None)\n","\n","    cpc_texts = get_cpc_texts(cfg)\n","    input_df[\"context_text\"] = input_df[\"context\"].map(cpc_texts)\n","    # input_df[\"context_text\"] = input_df[\"context_text\"].str.lower()\n","\n","    # cpc_codes_df = pd.read_csv(cfg.cpc_codes_csv)\n","    # cpc_codes_df[\"subclass_title\"] = cpc_codes_df[\"subclass_title\"].str.lower()\n","    # cpc_codes_df[\"group_title\"] = cpc_codes_df[\"group_title\"].str.lower()\n","    # input_df = input_df.merge(cpc_codes_df, on=[\"context\"], how=\"left\")\n","\n","    # input_df[\"text\"] = input_df[\"anchor\"] + \"[SEP]\" + input_df[\"target\"] + \"[SEP]\" + input_df[\"context_text\"] + \"[SEP]\" + input_df[\"subclass_title\"]\n","    input_df[\"text\"] = input_df[\"anchor\"] + \"[SEP]\" + input_df[\"target\"] + \"[SEP]\" + input_df[\"context_text\"]\n","\n","    if input_type == \"train\":\n","        input_df[\"score_map\"] = input_df[\"score\"].map({0.00: 0, 0.25: 1, 0.50: 2, 0.75: 3, 1.00: 4})\n"," \n","    return input_df\n","\n","\n","def get_split(cfg, train_df):\n","    split_name = cfg.split_name\n","    split_params = cfg.split_params\n","    splitter = sms.__getattribute__(split_name)(**split_params)\n","\n","    groups = train_df[\"anchor\"].to_numpy()\n","    train_df[\"fold\"] = -1\n","\n","    for fold_id, (train_idx, valid_idx) in enumerate(splitter.split(\n","        train_df,\n","        train_df[\"score_map\"],\n","        groups\n","        )):\n","        train_df.loc[valid_idx, \"fold\"] = int(fold_id)\n","\n","    return train_df\n","\n","\n","def get_filname_listdir(directory):\n","    listdir = os.listdir(directory)\n","    out_lst = [os.path.splitext(d)[0] for d in listdir]\n","    \n","    return out_lst\n","\n","\n","# ====================================\n","# Dataset #\n","# ====================================\n","def get_inputs(cfg, tokenizer, text: str):\n","    encoding = tokenizer(\n","        text,\n","        max_length=cfg.max_length,\n","        padding=\"max_length\",\n","        return_offsets_mapping=False,\n","        add_special_tokens=True,\n","        truncation=True\n","    )\n","\n","    for k, v in encoding.items():\n","        encoding[k] = torch.tensor(v, dtype=torch.long)\n","\n","    return encoding\n","\n","\n","class PPPMDataset(Dataset):\n","    def __init__(self, cfg, tokenizer, input_df: pd.DataFrame, phase: str = \"train\"):\n","        self.cfg = cfg\n","        self.tokenizer = tokenizer\n","        self.text = input_df[\"text\"].to_numpy()\n","        self.label = input_df[\"score\"].to_numpy() if phase == \"train\" else None\n","        self.phase = phase\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, idx):\n","        if self.phase == \"train\":\n","            inputs = get_inputs(\n","                self.cfg,\n","                self.tokenizer,\n","                self.text[idx]\n","            )\n","\n","            return {\n","                \"input_ids\": inputs[\"input_ids\"],\n","                \"attention_mask\": inputs[\"attention_mask\"],\n","                \"label\": torch.tensor(self.label[idx], dtype=torch.float),\n","            }\n","        \n","        elif self.phase == \"test\":\n","            inputs = get_inputs(\n","                self.cfg,\n","                self.tokenizer,\n","                self.text[idx]\n","            )\n","\n","            return {\n","                \"input_ids\": inputs[\"input_ids\"],\n","                \"attention_mask\": inputs[\"attention_mask\"],\n","            }\n","        else:\n","            raise NotImplementedError\n","\n","\n","class PPPMDataModule(pl.LightningDataModule):\n","    def __init__(self, cfg, tokenizer, train_df: pd.DataFrame = None, valid_df: pd.DataFrame = None, test_df: pd.DataFrame = None):\n","        super(PPPMDataModule, self).__init__()\n","\n","        self.cfg = cfg\n","        self.tokenizer = tokenizer\n","        self.train_df = train_df\n","        self.valid_df = valid_df\n","        self.test_df = test_df\n","\n","    def prepare_data(self):\n","        if self.test_df is None:\n","            self.train_dataset = PPPMDataset(\n","                cfg=self.cfg,\n","                tokenizer=self.tokenizer,\n","                input_df=self.train_df,\n","                phase=\"train\"\n","            )\n","            self.val_dataset = PPPMDataset(\n","                cfg=self.cfg,\n","                tokenizer=self.tokenizer,\n","                input_df=self.valid_df,\n","                phase=\"train\"\n","            )\n","        else:\n","            self.test_dataset = PPPMDataset(\n","                cfg=self.cfg,\n","                tokenizer=self.tokenizer,\n","                input_df=self.test_df,\n","                phase=\"test\"\n","            )\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.cfg.train_batch_size,\n","            num_workers=self.cfg.num_workers,\n","            shuffle=True,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","    \n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.val_dataset,\n","            batch_size=self.cfg.valid_batch_size,\n","            num_workers=self.cfg.num_workers,\n","            shuffle=False,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","\n","    def predict_dataloader(self):\n","        return DataLoader(\n","            self.test_dataset,\n","            batch_size=self.cfg.test_batch_size,\n","            num_workers=self.cfg.num_workers,\n","            shuffle=False,\n","            pin_memory=True,\n","            drop_last=False,\n","        )\n","\n","\n","# ====================================\n","# Model #\n","# ====================================\n","class PPPMModel(nn.Module):\n","    def __init__(self, cfg):\n","        super(PPPMModel, self).__init__()\n","        \n","        self.cfg = cfg\n","        self.model_config = get_backbone_config(self.cfg)\n","        if self.cfg.mlm_pretrained:\n","            self.backbone = AutoModel.from_pretrained(self.cfg.mlm_model_dir, config=self.model_config)\n","        elif self.cfg.pretrained:\n","            self.backbone = AutoModel.from_pretrained(self.cfg.model_name, config=self.model_config)\n","        else:\n","            self.backbone = AutoModel.from_config(self.model_config)\n","        self.dropout = nn.Dropout(self.cfg.dropout)\n","        self.fc = nn.Linear(self.cfg.hidden_size, 1)\n","        self._init_weights(self.fc)\n","        self.attention = nn.Sequential(\n","            nn.Linear(self.cfg.hidden_size, 512),\n","            nn.Tanh(),\n","            nn.Linear(512, 1),\n","            nn.Softmax(dim=1)\n","        )\n","        self._init_weights(self.attention)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=self.cfg.initializer_range)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=self.cfg.initializer_range)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n","        x = outputs[0] # (batch_size, seq_len, hidden_size)\n","        \n","        weights = self.attention(x)\n","        x = torch.sum(weights * x, dim=1) # (batch_size, hidden_size)\n","\n","        x = self.dropout(x)\n","        x = self.fc(x) # (batch_size, 1)\n","\n","        return x.squeeze(-1)\n","\n","\n","class PPPMLightningModule(pl.LightningModule):\n","    def __init__(self, cfg):\n","        super(PPPMLightningModule, self).__init__()\n","\n","        self.cfg = cfg\n","        self.model = PPPMModel(self.cfg)\n","        self.criterion = get_criterion(self.cfg)\n","\n","    def setup(self, stage=None):\n","        # calculate training total steps\n","        if stage == \"fit\":\n","            if self.cfg.scheduler_cycle == \"one-cycle\":\n","                self.training_steps = math.ceil(len(self.trainer.datamodule.train_dataloader()) / self.trainer.accumulate_grad_batches) * self.trainer.max_epochs\n","            elif self.cfg.scheduler_cycle == \"epoch\":\n","                self.training_steps = math.ceil(len(self.trainer.datamodule.train_dataloader()) / self.trainer.accumulate_grad_batches) * 1\n","            else:\n","                raise NotImplementedError\n","            self.warmup_steps = int(self.training_steps * self.cfg.scheduler_warmup_ratio)\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        return self.model(input_ids, attention_mask)\n","\n","    def training_step(self, batch, batch_idx):\n","        input_ids, attention_mask, label = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"label\"]\n","        output = self.forward(input_ids, attention_mask)\n","        loss = self.criterion(output.view(-1, 1), label.view(-1, 1))\n","        self.log(\"train/loss\", loss, on_epoch=True, logger=True, prog_bar=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        input_ids, attention_mask, label = batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"label\"]\n","        output = self.forward(input_ids, attention_mask)\n","        loss = self.criterion(output.view(-1, 1), label.view(-1, 1))\n","        self.log(\"val/loss\", loss, on_epoch=True, logger=True, prog_bar=True)\n","\n","        return {\n","            \"preds\": output.detach(),\n","            \"labels\": label.detach(),\n","            \"loss\": loss,\n","        }\n","\n","    def validation_epoch_end(self, outputs):\n","        preds = torch.cat([output[\"preds\"] for output in outputs]).sigmoid().cpu().numpy()\n","        labels = torch.cat([output[\"labels\"] for output in outputs]).cpu().numpy()\n","        score = get_score(labels, preds)\n","        self.log(\"val/pearson_corr\", score, logger=True, prog_bar=True)\n","\n","    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n","        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n","        output = self.forward(input_ids, attention_mask)\n","        output = output.sigmoid()\n","\n","        return output.squeeze()\n","\n","    def configure_optimizers(self):\n","        optimizer_params = get_optimizer_params(self.model, self.cfg.encoder_lr, self.cfg.decoder_lr, self.cfg.weight_decay)\n","        optimizer = get_optimizer(self.cfg, optimizer_params)\n","\n","        if self.cfg.scheduler_name is None:\n","            return [optimizer]\n","        else:\n","            scheduler = get_scheduler(self.cfg, optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=self.training_steps)\n","            scheduler = {\"scheduler\": scheduler, \"interval\": self.cfg.scheduler_interval}\n","\n","            return [optimizer], [scheduler]\n","\n","\n","# ====================================\n","# Criterion, Optimizer, Scheduler #\n","# ====================================\n","def get_criterion(cfg):\n","    loss_name = cfg.loss_name\n","    loss_params = cfg.loss_params\n","\n","    if loss_name != \"SmoothFocalLoss\":\n","        return nn.__getattribute__(loss_name)(**loss_params)\n","    else:\n","        return SmoothFocalLoss(**loss_params)\n","\n","\n","def get_optimizer(cfg, parameters):\n","    optimizer_name = cfg.optimizer_name\n","    optimizer_params = cfg.optimizer_params\n","\n","    return optim.__getattribute__(optimizer_name)(parameters, **optimizer_params)\n","\n","\n","def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n","    # param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)],\n","            'lr': encoder_lr, 'weight_decay': weight_decay},\n","        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)],\n","            'lr': encoder_lr, 'weight_decay': 0.0},\n","        {'params': [p for n, p in model.named_parameters() if \"backbone\" not in n],\n","            'lr': decoder_lr, 'weight_decay': 0.0}\n","    ]\n","\n","    return optimizer_parameters\n","\n","\n","def get_scheduler(cfg, optimizer, num_warmup_steps=None, num_training_steps=None):\n","    scheduler_name = cfg.scheduler_name\n","    scheduler_params = cfg.scheduler_params\n","\n","    if scheduler_name == \"cosine-warmup\":\n","        return get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","            **scheduler_params\n","        )\n","    elif scheduler_name == \"linear-warmup\":\n","        return get_linear_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=num_warmup_steps,\n","            num_training_steps=num_training_steps,\n","            **scheduler_params\n","        )\n","    else:\n","        return optim.lr_scheduler.__getattribute__(scheduler_name)(optimizer, **scheduler_params)\n","\n","\n","class FocalLoss(nn.Module):\n","    \"\"\"\n","    reference: https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes/discussion/322799\n","    \"\"\"\n","    def __init__(self, reduction='none', alpha=1, gamma=2):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.alpha = alpha\n","        self.gamma = gamma\n","\n","    def forward(self, inputs, targets):\n","        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n","        pt = torch.exp(-bce_loss)\n","        loss = self.alpha * (1. - pt)**self.gamma * bce_loss\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","\n","class SmoothFocalLoss(nn.Module):\n","    def __init__(self, reduction='none', alpha=1, gamma=2, smoothing=0.0):\n","        super().__init__()\n","        self.reduction = reduction\n","        self.focal_loss = FocalLoss(reduction='none', alpha=alpha, gamma=gamma)\n","        self.smoothing = smoothing\n","\n","    @staticmethod\n","    def _smooth(targets:torch.Tensor, smoothing=0.0):\n","        assert 0 \u003c= smoothing \u003c 1\n","        with torch.no_grad():\n","            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n","        return targets\n","\n","    def forward(self, inputs, targets):\n","        targets = SmoothFocalLoss._smooth(targets, self.smoothing)\n","        loss = self.focal_loss(inputs, targets)\n","        if self.reduction == 'none':\n","            loss = loss\n","        elif self.reduction == 'sum':\n","            loss = loss.sum()\n","        elif self.reduction == 'mean':\n","            loss = loss.mean()\n","        return loss\n","\n","\n","# ====================================\n","# Train \u0026 Predict #\n","# ====================================\n","def train_fold(cfg, tokenizer, train_df, valid_df, fold):\n","    # Seed\n","    seed_everything(cfg.seed)\n","\n","    # Wandb\n","    wandb_logger = WandbLogger(\n","        project=cfg.competition_name,\n","        group=cfg.group,\n","        name=f\"exp{cfg.exp_id}-fold-{fold}\",\n","        job_type=f\"exp{cfg.exp_id}\",\n","        reinit=True,\n","        anonymous=\"must\",\n","    )\n","\n","    # Model Checkpoint\n","    checkpoint = ModelCheckpoint(\n","        dirpath=cfg.model_dir,\n","        # filename=f\"exp{cfg.exp_id}-fold-{fold}\" + \"-{epoch}\",\n","        filename=f\"exp{cfg.exp_id}-fold-{fold}\",\n","        **cfg.checkpoint_params,\n","    )\n","\n","    # Learning Rate\n","    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n","    callbacks = [checkpoint, lr_monitor]\n","\n","    # Early Stopping\n","    if cfg.early_stopping:\n","        early_stopping = EarlyStopping(**cfg.early_stopping_params)\n","        callbacks += [early_stopping]\n","    \n","    # DataModule\n","    lightning_datamodule = PPPMDataModule(\n","        cfg=cfg,\n","        tokenizer=tokenizer,\n","        train_df=train_df,\n","        valid_df=valid_df,\n","    )\n","\n","    # Model\n","    lightning_model = PPPMLightningModule(\n","        cfg=cfg,\n","    )\n","\n","    # Trainer\n","    trainer = Trainer(\n","        gpus=cfg.gpus,\n","        max_epochs=cfg.max_epochs,\n","        callbacks=callbacks,\n","        logger=[wandb_logger],\n","        accumulate_grad_batches=cfg.accumulate_grad_batches,\n","        precision=cfg.precision,\n","        # deterministic=True,\n","        benchmark=False,\n","        gradient_clip_val=cfg.gradient_clip_val\n","    )\n","\n","    trainer.fit(lightning_model, datamodule=lightning_datamodule)\n","    wandb.finish(quiet=True)\n","\n","    del lightning_datamodule, lightning_model, trainer\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","\n","def train_cv(cfg, tokenizer, input_df: pd.DataFrame):\n","    oof_df = []\n","\n","    for fold_id in range(cfg.num_fold):\n","        if fold_id in cfg.train_fold:\n","            filename = f\"exp{cfg.exp_id}-fold-{fold_id}\"\n","            filelist = get_filname_listdir(cfg.model_dir)\n","\n","            train_df = input_df[input_df[\"fold\"] != fold_id].reset_index(drop=True)\n","            valid_df = input_df[input_df[\"fold\"] == fold_id].reset_index(drop=True)\n","\n","            # training\n","            if not filename in filelist:\n","                train_fold(\n","                    cfg=cfg,\n","                    tokenizer=tokenizer,\n","                    train_df=train_df,\n","                    valid_df=valid_df,\n","                    fold=fold_id,\n","                )\n","\n","            # oof\n","            oof_pred = predict(\n","                cfg=cfg,\n","                tokenizer=tokenizer,\n","                input_df=valid_df,\n","                filename=filename,\n","            )\n","            valid_df[\"oof\"] = oof_pred\n","            oof_df.append(valid_df)\n","\n","            oof_score = get_score(valid_df[\"score\"].to_numpy(), oof_pred)\n","            cfg.logger.info(f\"fold{fold_id}-score: {oof_score}\")\n","\n","    oof_df = pd.concat(oof_df, axis=0).reset_index(drop=True)        \n","    oof_score = get_score(oof_df[\"score\"].to_numpy(), oof_df[\"oof\"].to_numpy())\n","    cfg.logger.info(f\"cv-score: {oof_score}\")\n","\n","    return oof_df\n","\n","\n","def predict_raw_prediction(cfg, tokenizer, input_df: pd.DataFrame, filename: str):\n","    checkpoint_path = os.path.join(cfg.model_dir, filename + \".ckpt\")\n","\n","    lightning_model = PPPMLightningModule(\n","        cfg=cfg,\n","    )\n","\n","    lightning_model = lightning_model.load_from_checkpoint(\n","        checkpoint_path=checkpoint_path,\n","        cfg=cfg,\n","    )\n","\n","    lightning_datamodule = PPPMDataModule(\n","        cfg,\n","        tokenizer=tokenizer,\n","        test_df=input_df\n","    )\n","\n","    trainer = Trainer(\n","        gpus=cfg.gpus,\n","    )\n","\n","    preds = trainer.predict(\n","        lightning_model,\n","        datamodule=lightning_datamodule,\n","        return_predictions=True\n","    )\n","\n","    preds = torch.cat(preds).cpu().numpy() # (samples, 1)\n","\n","    del lightning_datamodule, lightning_model, trainer\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    \n","    return preds\n","\n","\n","def predict(cfg, tokenizer, input_df: pd.DataFrame, filename: str):\n","    file_path = os.path.join(cfg.exp_output_dir, f\"{filename}.npy\")\n","    \n","    if os.path.isfile(file_path):\n","        preds = np.load(file_path)\n","    else:\n","        preds = predict_raw_prediction(\n","            cfg=cfg,\n","            tokenizer=tokenizer,\n","            input_df=input_df,\n","            filename=filename\n","        )\n","        np.save(os.path.join(cfg.exp_output_dir, filename), preds)\n","\n","    return preds\n","\n","\n","def predict_cv(cfg, tokenizer, input_df: pd.DataFrame):\n","    \"\"\"\n","    CVモデルで予測\n","    \"\"\"\n","    output_df = input_df.copy()\n","    fold_preds = []\n","\n","    for fold_id in range(cfg.num_fold):\n","        if fold_id in cfg.train_fold:\n","            filename = f\"exp{cfg.exp_id}-fold-{fold_id}\"\n","            preds = predict_raw_prediction(\n","                cfg=cfg,\n","                tokenizer=tokenizer,\n","                input_df=input_df,\n","                filename=filename\n","            )\n","            fold_preds.append(preds)\n","\n","    fold_preds = np.mean(fold_preds, axis=0)\n","    output_df[\"score\"] = fold_preds\n","    \n","    return output_df\n","\n","\n","# ====================================\n","# Metrics #\n","# ====================================\n","def get_score(y_true, y_pred):\n","    score = sp.stats.pearsonr(y_true, y_pred)[0]\n","\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":602},"id":"CV7a8JxMeU_o"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mazupero\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","Global seed set to 2434\n"]},{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u003ca href=\"https://wandb.me/wandb-init\" target=\"_blank\"\u003ethe W\u0026B docs\u003c/a\u003e."],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.17"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20220528_161528-c0p0fqlv\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/c0p0fqlv\" target=\"_blank\"\u003eexp026-fold-0\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://wandb.me/run\" target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f764774c492c4fa48058306818b7ef13","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.33G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type      | Params\n","----------------------------------------\n","0 | model     | PPPMModel | 355 M \n","1 | criterion | MSELoss   | 0     \n","----------------------------------------\n","355 M     Trainable params\n","0         Non-trainable params\n","355 M     Total params\n","1,423.544 Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2ea4a0c625648289a70b9295e257c56","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9925d36917d40b5b04e7b0e2e2fb807","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cf4feec60df41eaaa6d00913cb706c4","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 1795: 'val/pearson_corr' reached 0.73701 (best 0.73701), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-0.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca7b5cddf9264656ae10c705b604c59f","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 3590: 'val/pearson_corr' reached 0.76415 (best 0.76415), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-0.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f909d0d54af74abf8b4de0abf65c3ccb","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 5385: 'val/pearson_corr' reached 0.77539 (best 0.77539), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-0.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ceef4c2e7170494584016d96978623da","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 7180: 'val/pearson_corr' reached 0.77805 (best 0.77805), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-0.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49ab4e4704b94e488e5762e281570bd6","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 8975: 'val/pearson_corr' reached 0.77823 (best 0.77823), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-0.ckpt' as top 1\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29fd8251352841418b4ac5e69e43dfb3","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003eexp026-fold-0\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/c0p0fqlv\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/c0p0fqlv\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/loops/utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n","  category=PossibleUserWarning,\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","Missing logger folder: /content/lightning_logs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b82ab58ab02416cb225d5d28793929c","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[2022-05-28 17:49:12] - fold0-score: 0.7782349710624115\n","Global seed set to 2434\n"]},{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u003ca href=\"https://wandb.me/wandb-init\" target=\"_blank\"\u003ethe W\u0026B docs\u003c/a\u003e."],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.17"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20220528_174912-nrrbkmzw\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/nrrbkmzw\" target=\"_blank\"\u003eexp026-fold-1\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://wandb.me/run\" target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type      | Params\n","----------------------------------------\n","0 | model     | PPPMModel | 355 M \n","1 | criterion | MSELoss   | 0     \n","----------------------------------------\n","355 M     Trainable params\n","0         Non-trainable params\n","355 M     Total params\n","1,423.544 Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13ecca2a33c84e20bbf333202410c49b","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"831705ca092d4022b493e60c50337ab2","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7bdf1bcf2d834a548b97bf8d1c497343","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 1838: 'val/pearson_corr' reached 0.75670 (best 0.75670), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-1.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62e728a0a5e4475a8587e4273f42c12f","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 3676: 'val/pearson_corr' was not in top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2ec5db22cb743e9b7df63eba381a969","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 5514: 'val/pearson_corr' reached 0.77837 (best 0.77837), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-1.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02ac40444a844da8b62ac903ac8b0b37","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 7352: 'val/pearson_corr' reached 0.79201 (best 0.79201), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-1.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ca6ca367d2b4ca6ba3f1bbab2d9ece0","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 9190: 'val/pearson_corr' was not in top 1\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa1d8fb779964a0b94a992b90bfcb4e7","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003eexp026-fold-1\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/nrrbkmzw\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/nrrbkmzw\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"744a93cbd6564a57a9c4182294c985fa","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[2022-05-28 19:23:37] - fold1-score: 0.7920090257610506\n","Global seed set to 2434\n"]},{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u003ca href=\"https://wandb.me/wandb-init\" target=\"_blank\"\u003ethe W\u0026B docs\u003c/a\u003e."],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.17"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20220528_192337-1rr9dbuc\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/1rr9dbuc\" target=\"_blank\"\u003eexp026-fold-2\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://wandb.me/run\" target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type      | Params\n","----------------------------------------\n","0 | model     | PPPMModel | 355 M \n","1 | criterion | MSELoss   | 0     \n","----------------------------------------\n","355 M     Trainable params\n","0         Non-trainable params\n","355 M     Total params\n","1,423.544 Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"233cc85ebb1f417d9b5ac33ed0d74bd5","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00b9a2ba8dba46638f009e6fe057198c","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b912f581a8148779edeb6cdfcbe96e2","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 1822: 'val/pearson_corr' reached 0.72483 (best 0.72483), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-2.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2ad3d9f42884851ba401c3969032711","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 3644: 'val/pearson_corr' reached 0.77104 (best 0.77104), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-2.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a8916a5d1bc45ea868dfa16fb909d1d","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 5466: 'val/pearson_corr' reached 0.79072 (best 0.79072), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-2.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84bfc6218a5f44099a353fdf11d8a599","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 7288: 'val/pearson_corr' reached 0.79461 (best 0.79461), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-2.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"739c51fe1a964c3aaa129f154d9e7392","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 9110: 'val/pearson_corr' reached 0.79508 (best 0.79508), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-2.ckpt' as top 1\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7aed38d34113405394792ce0d1f02e9c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003eexp026-fold-2\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/1rr9dbuc\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/1rr9dbuc\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8b1b24ceaaa42a5a14bd882f74cfc54","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[2022-05-28 20:57:54] - fold2-score: 0.7950813661809234\n","Global seed set to 2434\n"]},{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u003ca href=\"https://wandb.me/wandb-init\" target=\"_blank\"\u003ethe W\u0026B docs\u003c/a\u003e."],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.17"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20220528_205754-1gf15c29\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/1gf15c29\" target=\"_blank\"\u003eexp026-fold-3\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://wandb.me/run\" target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type      | Params\n","----------------------------------------\n","0 | model     | PPPMModel | 355 M \n","1 | criterion | MSELoss   | 0     \n","----------------------------------------\n","355 M     Trainable params\n","0         Non-trainable params\n","355 M     Total params\n","1,423.544 Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c151a4f020b4be5abbb87098df6877d","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"385680ebfea94ce3a7b37295d5246f0f","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5284d0f178d64606b4cf8c8a839d95d3","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 1822: 'val/pearson_corr' reached 0.75388 (best 0.75388), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-3.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fd80f514c4d4dada507675f5ad5a29d","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 3644: 'val/pearson_corr' reached 0.77051 (best 0.77051), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-3.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07d442d4ad194897bd509a6fb80dfad5","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 5466: 'val/pearson_corr' reached 0.77985 (best 0.77985), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-3.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d709d53b434046718e5d6a459cfef8a8","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 7288: 'val/pearson_corr' reached 0.78986 (best 0.78986), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-3.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89dec783aadd4d4f97c6fa23464d6447","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 9110: 'val/pearson_corr' was not in top 1\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9d7e65c8badc47d5b4965f1c1f5fb8b2","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003eexp026-fold-3\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/1gf15c29\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/1gf15c29\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6577908ed334fe480fd58c39fb341fa","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[2022-05-28 22:32:14] - fold3-score: 0.7898609401009375\n","Global seed set to 2434\n"]},{"data":{"text/html":["Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to \u003ca href=\"https://wandb.me/wandb-init\" target=\"_blank\"\u003ethe W\u0026B docs\u003c/a\u003e."],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.17"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in \u003ccode\u003e/content/wandb/run-20220528_223214-11vt6mc7\u003c/code\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run \u003cstrong\u003e\u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/11vt6mc7\" target=\"_blank\"\u003eexp026-fold-4\u003c/a\u003e\u003c/strong\u003e to \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching\" target=\"_blank\"\u003eWeights \u0026 Biases\u003c/a\u003e (\u003ca href=\"https://wandb.me/run\" target=\"_blank\"\u003edocs\u003c/a\u003e)\u003cbr/\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name      | Type      | Params\n","----------------------------------------\n","0 | model     | PPPMModel | 355 M \n","1 | criterion | MSELoss   | 0     \n","----------------------------------------\n","355 M     Trainable params\n","0         Non-trainable params\n","355 M     Total params\n","1,423.544 Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b50f356a6ae2489e9a9d67cbd84cf141","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73a1edde83c84c4f88efe41ecba92fb7","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab60b4cd92e8477f9fc9876ef0c429b9","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 1844: 'val/pearson_corr' reached 0.76782 (best 0.76782), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-4.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82513e4e7e654631993cc3dcda7356c8","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 3688: 'val/pearson_corr' was not in top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecf1479439244d1ab0352c0adc183718","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 5532: 'val/pearson_corr' reached 0.77758 (best 0.77758), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-4.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc12b7e29842449cb0bcfc003360fe67","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 7376: 'val/pearson_corr' reached 0.79511 (best 0.79511), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-4.ckpt' as top 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba15243ad43c4a1390e3e634710fd35a","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 9220: 'val/pearson_corr' reached 0.79759 (best 0.79759), saving model to '/content/drive/MyDrive/Colab/kaggle/us-patent-phrase-to-phrase-matching/output/exp026/model/exp026-fold-4.ckpt' as top 1\n"]},{"data":{"text/html":["Waiting for W\u0026B process to finish... \u003cstrong style=\"color:green\"\u003e(success).\u003c/strong\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38b67bb022124131945743d41e75f431","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced \u003cstrong style=\"color:#cdcd00\"\u003eexp026-fold-4\u003c/strong\u003e: \u003ca href=\"https://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/11vt6mc7\" target=\"_blank\"\u003ehttps://wandb.ai/azupero/us-patent-phrase-to-phrase-matching/runs/11vt6mc7\u003c/a\u003e\u003cbr/\u003eSynced 5 W\u0026B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea2f77974a034f90a7bc65906f9329b0","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[2022-05-29 00:07:25] - fold4-score: 0.7975943348346395\n","[2022-05-29 00:07:25] - cv-score: 0.7894683226340837\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78e64c820a844b7ea3b5264ab8e5d93f","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c639d177a0c416b9cf0f565f2fe293d","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc93d211c33a49b2bd526584775b166c","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c57362e3a3884792b1c83b7dfd1d853b","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab56b5ca8066465f8af9dca370ca0b94","version_major":2,"version_minor":0},"text/plain":["Predicting: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Starting upload for file pretrain_tokenizer.tar\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3.22M/3.22M [00:04\u003c00:00, 837kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: pretrain_tokenizer.tar (3MB)\n","Starting upload for file model.tar\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6.63G/6.63G [03:00\u003c00:00, 39.4MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: model.tar (7GB)\n","Starting upload for file Experiment.log\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 333/333 [00:02\u003c00:00, 114B/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: Experiment.log (333B)\n","Starting upload for file model_config.pth\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2.23k/2.23k [00:01\u003c00:00, 1.19kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: model_config.pth (2KB)\n","Starting upload for file exp026-fold-0.npy\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 30.4k/30.4k [00:03\u003c00:00, 10.1kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp026-fold-0.npy (30KB)\n","Starting upload for file exp026-fold-1.npy\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 27.8k/27.8k [00:01\u003c00:00, 14.3kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp026-fold-1.npy (28KB)\n","Starting upload for file exp026-fold-2.npy\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28.8k/28.8k [00:03\u003c00:00, 9.60kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp026-fold-2.npy (29KB)\n","Starting upload for file exp026-fold-3.npy\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 28.8k/28.8k [00:03\u003c00:00, 8.90kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp026-fold-3.npy (29KB)\n","Starting upload for file exp026-fold-4.npy\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 27.4k/27.4k [00:02\u003c00:00, 10.0kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Upload successful: exp026-fold-4.npy (27KB)\n"]}],"source":["def main(Config):\n","    # setup\n","    Config = setup(Config)\n","    Config.logger = Logger(Config.exp_output_dir)\n","    \n","    # load dataset\n","    train_df = get_input_data(Config, input_type=\"train\")\n","    test_df = get_input_data(Config, input_type=\"test\")\n","    submission_df = pd.read_csv(Config.sample_submission)\n","\n","    # split\n","    train_df = get_split(Config, train_df)\n","\n","    # tokenizer\n","    tokenizer = get_tokenizer(Config)\n","\n","    if not Config.inference_only:\n","        # training\n","        oof_df = train_cv(\n","            cfg=Config,\n","            tokenizer=tokenizer,\n","            input_df=train_df,\n","        )\n","\n","    # predict\n","    raw_pred_df = predict_cv(\n","        cfg=Config,\n","        input_df=test_df,\n","        tokenizer=tokenizer,\n","    )\n","\n","    # upload output to kaggle dataset\n","    if Config.upload_from_colab:\n","        from kaggle.api.kaggle_api_extended import KaggleApi\n","\n","        def dataset_create_new(dataset_name, upload_dir):\n","            dataset_metadata = {}\n","            dataset_metadata['id'] = f'{os.environ[\"KAGGLE_USERNAME\"]}/{dataset_name}'\n","            dataset_metadata['licenses'] = [{'name': 'CC0-1.0'}]\n","            dataset_metadata['title'] = dataset_name\n","            with open(os.path.join(upload_dir, 'dataset-metadata.json'), 'w') as f:\n","                json.dump(dataset_metadata, f, indent=4)\n","            api = KaggleApi()\n","            api.authenticate()\n","            api.dataset_create_new(folder=upload_dir, convert_to_csv=False, dir_mode='tar')\n","\n","        dataset_create_new(dataset_name=f\"{Config.competition_name}-exp{Config.exp_id}\", upload_dir=Config.exp_output_dir)\n","\n","    # make submission\n","    if not Config.on_colab:\n","        raw_pred_df[[\"id\", \"score\"]].to_csv(os.path.join(Config.submission, \"submission.csv\"), index=False)\n","\n","\n","if __name__ == \"__main__\":\n","    main(Config)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T87Uci09E4fo"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPYC8u9pmazmHsbvwxp+Qkq","collapsed_sections":[],"machine_shape":"hm","mount_file_id":"14ZZzO33kAOJ6VoFSVPzYCXGmOq1qp9pB","name":"exp026.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1327a4cb82d4481b839b5daf2cfc3fb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3609ed104069410394ddd2440bbc9d00","max":1425941629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ab0dfeedf184987ac224dd6117fadee","value":1425941629}},"1f5508d65efb4501b5123206ad57b1c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20b6e9fd08d140e6a4488b1840d83018":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"211ca432bd9042d1a486bf6a80007eb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e3039a601984ae7b75ebce29b9f7c47","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9c386b4beaf74ae79759a01346e113f9","value":2}},"2793f48242dd43d6acc47746713e2a18":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9cfad4d2c1e411cab1762c652eb539b","placeholder":"​","style":"IPY_MODEL_e33d3936c53240858ac4058181f7af68","value":" 2/2 [00:00\u0026lt;00:00,  9.68it/s]"}},"3609ed104069410394ddd2440bbc9d00":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3755314fa71b4b1fa024758ba8c9ce45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ab0dfeedf184987ac224dd6117fadee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ab4db64d91e4a7bbf89fbbcc242286f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"41788bb5ba6a42d2aa6b0097d5a1c431":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b43abec6e7f6412196f96aa5388ecea9","placeholder":"​","style":"IPY_MODEL_a25cedd6aeb44cefbf21c40cc0c1e6a6","value":" 3780/3833 [17:47\u0026lt;00:14,  3.54it/s, loss=0.03, v_num=fqlv, train/loss_step=0.0112]"}},"50d648be7ffb437586b1c45acecc086b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"5e5815c39fb740a1b319a524253a97d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e3039a601984ae7b75ebce29b9f7c47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"727144d52f544ecdbe1cc5adc61e4649":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f3d34754b914d47a6c5892277cde754":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c386b4beaf74ae79759a01346e113f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a107ddf8b37e40c18839b8e5a8753088":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a37bce345b104ddfbbdc2e93887f6a31","placeholder":"​","style":"IPY_MODEL_3755314fa71b4b1fa024758ba8c9ce45","value":"Epoch 0:  99%"}},"a25cedd6aeb44cefbf21c40cc0c1e6a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a37bce345b104ddfbbdc2e93887f6a31":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adbe675449a04d45a10f307715b38962":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_727144d52f544ecdbe1cc5adc61e4649","placeholder":"​","style":"IPY_MODEL_1f5508d65efb4501b5123206ad57b1c4","value":"Sanity Checking DataLoader 0: 100%"}},"b43abec6e7f6412196f96aa5388ecea9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8abf175388f4530a5bc4ea8c06e4410":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b9cfad4d2c1e411cab1762c652eb539b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5145400751740e3948aa6262774d165":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d543077623854d3f9ef6545ae6af7841","placeholder":"​","style":"IPY_MODEL_e5c9d70fd67e474294f4de418d673bac","value":"Downloading: 100%"}},"d2b0fb991a94466b8f28e06e18b2739b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f3d34754b914d47a6c5892277cde754","placeholder":"​","style":"IPY_MODEL_5e5815c39fb740a1b319a524253a97d9","value":" 1.33G/1.33G [00:23\u0026lt;00:00, 63.6MB/s]"}},"d2ea4a0c625648289a70b9295e257c56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_adbe675449a04d45a10f307715b38962","IPY_MODEL_211ca432bd9042d1a486bf6a80007eb9","IPY_MODEL_2793f48242dd43d6acc47746713e2a18"],"layout":"IPY_MODEL_3ab4db64d91e4a7bbf89fbbcc242286f"}},"d543077623854d3f9ef6545ae6af7841":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e33d3936c53240858ac4058181f7af68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5c9d70fd67e474294f4de418d673bac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f764774c492c4fa48058306818b7ef13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c5145400751740e3948aa6262774d165","IPY_MODEL_1327a4cb82d4481b839b5daf2cfc3fb5","IPY_MODEL_d2b0fb991a94466b8f28e06e18b2739b"],"layout":"IPY_MODEL_20b6e9fd08d140e6a4488b1840d83018"}},"f9925d36917d40b5b04e7b0e2e2fb807":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a107ddf8b37e40c18839b8e5a8753088","IPY_MODEL_fcb6f321e91e4ff6a081a20186a3e923","IPY_MODEL_41788bb5ba6a42d2aa6b0097d5a1c431"],"layout":"IPY_MODEL_50d648be7ffb437586b1c45acecc086b"}},"fcad871cd1c54c12b48c0ac72bb32bee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcb6f321e91e4ff6a081a20186a3e923":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcad871cd1c54c12b48c0ac72bb32bee","max":3833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b8abf175388f4530a5bc4ea8c06e4410","value":3780}}}}},"nbformat":4,"nbformat_minor":0}